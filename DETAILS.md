# DETAILS.md

---


ğŸ” **Powered by [Detailer](https://detailer.ginylil.com)** - Smart agent-compatible documentation

## Project Overview

### Purpose & Domain

This project is **JAX**, a high-performance numerical computing and machine learning framework developed by Google. It provides composable transformations of numerical Python functions, enabling automatic differentiation (autodiff), just-in-time (JIT) compilation, vectorization, parallelization, and hardware acceleration (CPU, GPU, TPU).

**Problem Solved:**
- Enables efficient and scalable numerical computation with automatic differentiation.
- Bridges high-level Python numerical code with low-level optimized execution on accelerators.
- Supports advanced features like shape polymorphism, distributed computation, and custom hardware kernels.

**Target Users & Use Cases:**
- Machine learning researchers and practitioners requiring fast, differentiable computations.
- Developers building scalable, hardware-accelerated scientific computing applications.
- Teams needing composable transformations for gradient-based optimization, probabilistic programming, and neural network training.
- Users targeting multi-device and multi-host distributed environments (e.g., TPU pods, GPU clusters).

**Core Business Logic & Domain Models:**
- **JAXPR:** Intermediate representation of JAX computations enabling transformations and compilation.
- **Primitives:** Atomic operations (e.g., add, matmul) with registered evaluation, differentiation, and lowering rules.
- **Tracers & Traces:** Mechanisms to intercept and transform computations for autodiff, batching, and JIT.
- **Arrays:** Abstract and concrete array types supporting sharding, device placement, and distributed execution.
- **Effects:** Explicit modeling of side-effects (state, IO, synchronization) for safe transformations.
- **Sharding & Mesh:** Models for partitioning data and computation across devices.
- **Pallas & Mosaic:** Experimental frameworks for custom kernel programming on accelerators (GPU, TPU).
- **Export & Serialization:** Facilities for serializing compiled functions for cross-platform execution.

---

## Architecture and Structure

### High-Level Architecture

- **Layered Design:**
  - **User API Layer:** Python functions, transformations (`jax.jit`, `jax.grad`, `jax.vmap`, `pjit`, etc.).
  - **Transformation Layer:** Tracers, interpreters, JAXPR construction, partial evaluation.
  - **Compilation Layer:** Lowering JAXPRs to MLIR, XLA compilation, caching.
  - **Execution Layer:** Device buffer management, sharding, distributed execution.
  - **Backend Extensions:** Platform-specific kernels and primitives (Pallas, Mosaic GPU/TPU, cuDNN fused ops).
  - **Debugging & Profiling:** Debugger backends, profiling tools, error handling.
  - **Testing & Benchmarking:** Extensive test harnesses, benchmarks, and regression tests.
  - **CI/CD & Build:** GitHub workflows, build scripts, containerized environments.

### Complete Repository Structure

```
.
â”œâ”€â”€ .github/
â”‚   â”œâ”€â”€ ISSUE_TEMPLATE/
â”‚   â”œâ”€â”€ workflows/
â”‚   â”‚   â”œâ”€â”€ self_hosted_runner_utils/
â”‚   â”‚   â”œâ”€â”€ asan.yaml
â”‚   â”‚   â”œâ”€â”€ bazel_cpu_py_import_rbe.yml
â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”œâ”€â”€ actionlint.yaml
â”‚   â””â”€â”€ dependabot.yml
â”œâ”€â”€ benchmarks/
â”‚   â”œâ”€â”€ mosaic/
â”‚   â”œâ”€â”€ api_benchmark.py
â”‚   â”œâ”€â”€ linalg_benchmark.py
â”‚   â”œâ”€â”€ ...
â”œâ”€â”€ ci/
â”‚   â”œâ”€â”€ envs/
â”‚   â”œâ”€â”€ k8s/
â”‚   â”œâ”€â”€ utilities/
â”‚   â”œâ”€â”€ build_artifacts.sh
â”‚   â”œâ”€â”€ run_bazel_test_cpu_py_import_rbe.sh
â”‚   â”œâ”€â”€ ...
â”œâ”€â”€ cloud_tpu_colabs/
â”‚   â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ JAX_NeurIPS_2020_demo.ipynb
â”‚   â”œâ”€â”€ ...
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ _static/
â”‚   â”‚   â”œâ”€â”€ distributed_data_loading/
â”‚   â”‚   â”œâ”€â”€ multi_process/
â”‚   â”‚   â”œâ”€â”€ pallas/
â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”œâ”€â”€ _templates/
â”‚   â”œâ”€â”€ _tutorials/
â”‚   â”œâ”€â”€ debugging/
â”‚   â”œâ”€â”€ export/
â”‚   â”œâ”€â”€ jep/
â”‚   â”œâ”€â”€ pallas/
â”‚   â”œâ”€â”€ ...
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ about.md
â”‚   â”œâ”€â”€ advanced-autodiff.md
â”‚   â”œâ”€â”€ ...
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ ffi/
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â””â”€â”€ jax_ffi_example/
â”‚   â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â”œâ”€â”€ CMakeLists.txt
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â””â”€â”€ pyproject.toml
â”‚   â”œâ”€â”€ jax_cpp/
â”‚   â”œâ”€â”€ k8s/
â”‚   â”œâ”€â”€ advi.py
â”‚   â”œâ”€â”€ datasets.py
â”‚   â”œâ”€â”€ differentially_private_sgd.py
â”‚   â”œâ”€â”€ gaussian_process_regression.py
â”‚   â”œâ”€â”€ kernel_lsq.py
â”‚   â”œâ”€â”€ mnist_classifier.py
â”‚   â”œâ”€â”€ mnist_classifier_fromscratch.py
â”‚   â”œâ”€â”€ mnist_vae.py
â”‚   â”œâ”€â”€ onnx2xla.py
â”‚   â”œâ”€â”€ spmd_mnist_classifier_fromscratch.py
â”‚   â””â”€â”€ examples_test.py
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ jax_logo.png
â”‚   â”œâ”€â”€ jax_logo.svg
â”‚   â”œâ”€â”€ ...
â”œâ”€â”€ jax/
â”‚   â”œâ”€â”€ _src/
â”‚   â”‚   â”œâ”€â”€ clusters/
â”‚   â”‚   â”œâ”€â”€ cudnn/
â”‚   â”‚   â”œâ”€â”€ debugger/
â”‚   â”‚   â”œâ”€â”€ export/
â”‚   â”‚   â”œâ”€â”€ interpreters/
â”‚   â”‚   â”œâ”€â”€ internal_test_util/
â”‚   â”‚   â”œâ”€â”€ lax/
â”‚   â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â”œâ”€â”€ nn/
â”‚   â”‚   â”œâ”€â”€ pallas/
â”‚   â”‚   â”œâ”€â”€ pjit.py
â”‚   â”‚   â”œâ”€â”€ profiling.py
â”‚   â”‚   â”œâ”€â”€ random.py
â”‚   â”‚   â”œâ”€â”€ sharding.py
â”‚   â”‚   â”œâ”€â”€ stages.py
â”‚   â”‚   â”œâ”€â”€ test_util.py
â”‚   â”‚   â”œâ”€â”€ tree.py
â”‚   â”‚   â”œâ”€â”€ tree_util.py
â”‚   â”‚   â”œâ”€â”€ typing.py
â”‚   â”‚   â”œâ”€â”€ util.py
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ custom_batching.py
â”‚   â”œâ”€â”€ custom_derivatives.py
â”‚   â”œâ”€â”€ custom_transpose.py
â”‚   â”œâ”€â”€ debug.py
â”‚   â”œâ”€â”€ distributed.py
â”‚   â”œâ”€â”€ dlpack.py
â”‚   â”œâ”€â”€ dtypes.py
â”‚   â”œâ”€â”€ errors.py
â”‚   â”œâ”€â”€ ffi.py
â”‚   â”œâ”€â”€ flatten_util.py
â”‚   â”œâ”€â”€ monitoring.py
â”‚   â”œâ”€â”€ profiler.py
â”‚   â”œâ”€â”€ random.py
â”‚   â”œâ”€â”€ sharding.py
â”‚   â”œâ”€â”€ stages.py
â”‚   â”œâ”€â”€ test_util.py
â”‚   â”œâ”€â”€ tree.py
â”‚   â”œâ”€â”€ tree_util.py
â”‚   â”œâ”€â”€ typing.py
â”‚   â”œâ”€â”€ util.py
â”‚   â”œâ”€â”€ version.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ jax_plugins/
â”‚   â”œâ”€â”€ cuda/
â”‚   â”œâ”€â”€ rocm/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ jaxlib/
â”‚   â”œâ”€â”€ _jax/
â”‚   â”œâ”€â”€ cpu/
â”‚   â”œâ”€â”€ cuda/
â”‚   â”œâ”€â”€ gpu/
â”‚   â”œâ”€â”€ mlir/
â”‚   â”œâ”€â”€ ...
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ filecheck/
â”‚   â”œâ”€â”€ mosaic/
â”‚   â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ pallas/
â”‚   â”œâ”€â”€ testdata/
â”‚   â”œâ”€â”€ ...
â”œâ”€â”€ third_party/
â”‚   â”œâ”€â”€ flatbuffers/
â”‚   â”œâ”€â”€ xla/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ .bazelrc
â”œâ”€â”€ .bazelversion
â”œâ”€â”€ .editorconfig
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .pre-commit-config.yaml
â”œâ”€â”€ .readthedocs.yml
â”œâ”€â”€ AUTHORS
â”œâ”€â”€ BUILD.bazel
â”œâ”€â”€ CHANGELOG.md
â”œâ”€â”€ CITATION.bib
â”œâ”€â”€ CONTRIBUTING.md
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ WORKSPACE
â”œâ”€â”€ build_wheel.py
â”œâ”€â”€ conftest.py
â”œâ”€â”€ platform_mappings
â”œâ”€â”€ pyproject.toml
â””â”€â”€ setup.py
```

---

## Technical Implementation Details

### Core Concepts

- **JAXPR (JAX Program Representation):**  
  Internal IR representing staged computations, enabling transformations like autodiff, JIT, and batching.

- **Primitives & Effects:**  
  Atomic operations with registered implementations, abstract evaluations, differentiation, batching, and lowering rules. Effects model side-effects explicitly for safe transformations.

- **Tracers & Interpreters:**  
  Mechanisms to intercept function execution, enabling transformations by tracing computations.

- **Arrays & Sharding:**  
  Abstract array types (`Array`, `EArray`) support local and distributed data, with sharding specifications (`NamedSharding`, `PartitionSpec`) defining device layouts.

- **Compilation & Lowering:**  
  JAXPRs are lowered to MLIR modules (`jax/_src/interpreters/mlir.py`), then compiled via XLA backends (`jax/_src/interpreters/xla.py`), with caching and versioning support (`compilation_cache.py`).

- **Custom Extensions:**  
  - **Pallas:** Framework for custom kernels on accelerators, with primitives for memory, synchronization, and kernel dispatch.  
  - **Mosaic GPU/TPU:** Backend-specific implementations for GPU and TPU, including pipeline management and fused kernels.  
  - **cuDNN fused attention:** Specialized primitives for efficient attention on GPUs.

- **Debugging & Profiling:**  
  Debugger backends (CLI, Colab, Web), profiling tools (`profiler.py`, `collect_profile.py`), and error handling utilities improve developer experience.

- **Testing & Benchmarking:**  
  Extensive test harnesses (`internal_test_util`), benchmarks (`benchmarks/`), and example scripts (`examples/`) validate correctness and performance.

---

## Development Patterns and Standards

- **Modular Design:**  
  Clear separation between core APIs (`jax/`), internal implementations (`jax/_src/`), experimental features (`jax/experimental/`), and platform-specific backends (`jax/_src/pallas`, `jax/_src/cudnn`).

- **Decorator & Registration Patterns:**  
  Use of decorators for primitive registration, lowering rules, batching, and differentiation.

- **Effect System:**  
  Explicit effect tracking for side-effectful operations, enabling safe transformations and optimizations.

- **Caching & Memoization:**  
  Use of LRU caches, weakref caches, and persistent compilation caches to optimize performance.

- **Testing Strategy:**  
  - Parameterized test harnesses with factories for generating diverse test cases.  
  - Regression tests using serialized JAXPRs and MLIR modules.  
  - Hardware-aware tests that conditionally run based on device availability.

- **Configuration Management:**  
  Runtime flags (`jax.config`), environment variables, and context managers control behavior (e.g., enabling x64, transfer guards, debugging).

- **Code Organization:**  
  - Internal modules use underscore prefix (`_src`) to denote private implementation.  
  - Public API modules re-export internal implementations with deprecation management.  
  - Documentation and examples are separated from core code.

- **Error Handling:**  
  Custom exceptions, traceback filtering, and error propagation mechanisms improve usability and debugging.

---

## Integration and Dependencies

### External Dependencies

- **Core Libraries:**  
  - `numpy`: Numerical arrays and constants.  
  - `flatbuffers`: Serialization of exported functions.  
  - `mlir`: MLIR IR construction and dialects.  
  - `ctypes`: For FFI and native code integration.  
  - `threading`, `asyncio`: For concurrency and parallelism support.  
  - `requests`, `kubernetes`, `mpi4py`: For cluster environment detection and distributed execution.

- **Build & CI Tools:**  
  - Bazel build system.  
  - Docker containers for consistent build/test environments.  
  - GitHub Actions workflows for CI/CD.

- **Profiling & Debugging Tools:**  
  - Perfetto, Nsight, TensorBoard for profiling.  
  - `web_pdb` for web-based debugging.

### Internal Dependencies

- **JAX Internal Modules:**  
  - Core abstractions (`core.py`, `effects.py`, `tree_util.py`).  
  - Compilation and lowering (`mlir.py`, `xla.py`, `compiler.py`).  
  - Distributed and sharding (`sharding.py`, `mesh.py`, `clusters/`).  
  - Experimental features (`pallas/`, `experimental/`).  
  - Debugging and error handling (`debugger/`, `traceback_util.py`).  
  - Utilities (`util.py`, `monitoring.py`).

- **Third-Party Plugins:**  
  - GPU and ROCm plugins under `jax_plugins/`.

---

## Usage and Operational Guidance

### Getting Started

- Use the **public JAX API** (`jax`, `jax.numpy`, `jax.nn`) for numerical computing and machine learning.
- Leverage **transformations** like `jax.jit`, `jax.grad`, `jax.vmap`, `jax.pmap`, and `pjit` for performance and parallelism.
- Use **sharding APIs** (`PartitionSpec`, `NamedSharding`) to distribute data and computation across devices.
- For custom kernels and hardware-specific optimizations, explore **Pallas** and **Mosaic GPU/TPU** extensions.

### Development Workflow

- Follow **modular code organization**: add new primitives in `jax/_src/core.py` or relevant submodules.
- Register **lowering rules** in `jax/_src/interpreters/mlir.py` or backend-specific files.
- Use **effect system** to model side effects in new primitives.
- Write **tests** using the `internal_test_util` harness, defining `Harness` instances and parameterized tests.
- Use **benchmarks** under `benchmarks/` to measure performance impacts.
- Use **examples/** for reference implementations and usage patterns.

### Debugging and Profiling

- Use `jax.debug.print`, `jax.debug.breakpoint`, and `jax.experimental.checkify` for runtime debugging.
- Use `jax.profiler` and `collect_profile.py` to collect and analyze performance traces.
- For interactive debugging, use CLI or web debugger backends (`jax._src.debugger`).

### Build and CI

- Build with Bazel or provided scripts (`ci/build_artifacts.sh`).
- Use GitHub Actions workflows under `.github/workflows/` for automated testing and deployment.
- Use Docker containers for consistent build/test environments.

### Export and Serialization

- Use `jax.export` APIs to serialize compiled functions for cross-platform execution.
- Manage **shape polymorphism** with symbolic shapes (`symbolic_shape()`) and constraints.
- Use versioning and compatibility guarantees documented in `docs/export/export.md`.

### Distributed Execution

- Use cluster environment detection (`jax/_src/clusters/`) for multi-host setups.
- Use `jax.distributed.initialize()` to set up distributed environments.
- Use `pjit` and `pmap` for multi-device parallelism.
- Use sharding abstractions and mesh contexts (`jax.sharding`, `jax.mesh`) to control data placement.

---

## Summary

This codebase is a **comprehensive, modular, and extensible numerical computing framework** that bridges high-level Python numerical programming with low-level optimized execution on accelerators (CPU, GPU, TPU). It features:

- A **rich transformation system** enabling autodiff, JIT, vectorization, and parallelism.
- A **powerful intermediate representation (JAXPR)** enabling staged compilation and optimization.
- **Extensible primitive and effect systems** for custom operations and side-effect management.
- **Advanced backend integrations** with MLIR, XLA, and hardware-specific extensions (Pallas, Mosaic).
- **Robust debugging, profiling, and testing infrastructure** supporting development and maintenance.
- **Comprehensive documentation, examples, and CI/CD pipelines** facilitating onboarding, usage, and continuous quality assurance.

The repository structure, modular design, and extensive tooling make it suitable for both research and production use in high-performance numerical computing and machine learning.

---

# End of DETAILS.md