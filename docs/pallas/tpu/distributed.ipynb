{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zSNjLhGQJMgq"
   },
   "source": [
    "# Distributed Computing in Pallas for TPUs\n",
    "\n",
    "In this tutorial, we will cover the basics of distributed computing in Pallas on TPUs. We will learn about TPU topologies, communication using the remote DMA primitive, and calling a distributed kernel from JAX using `shard_map`. We will also cover some more advanced kernel writing techniques, such as double-buffering, bi-directional bandwidth optimization, and nested pipelining. As educational examples, we will learn how to implement various collective primitives from JAX, such as `lax.ppermute`, `lax.all_gather`, `lax.psum`, and `lax.psum_scatter`.\n",
    "\n",
    "Some recommended readings beforehand:\n",
    " - [Pallas Pipelining on TPU](pallas_tpu_pipelining)\n",
    " - [Collectives with `shard_map`](shard_map_collectives_tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 1978,
     "status": "ok",
     "timestamp": 1722904801801,
     "user": {
      "displayName": "Justin Fu",
      "userId": "17543197034567316452"
     },
     "user_tz": 420
    },
    "id": "PyAGnWc9yI8T",
    "outputId": "1d8229bd-cab5-495f-93e9-fff2e41db480"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with 4 TPU v5 lite devices.\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "from jax import lax\n",
    "from jax import numpy as jnp\n",
    "from jax.experimental import pallas as pl\n",
    "from jax.experimental import shard_map\n",
    "from jax.experimental.pallas import tpu as pltpu\n",
    "\n",
    "P = jax.sharding.PartitionSpec\n",
    "\n",
    "num_devices = jax.local_device_count()\n",
    "assert num_devices > 1, \"Please run this notebook with more than one device.\"\n",
    "assert \"TPU\" in jax.devices()[0].device_kind, \"Please run this notebook with TPU devices.\"\n",
    "print(f\"Running with {num_devices} {jax.devices()[0].device_kind} devices.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DySMGNByclMi"
   },
   "source": [
    "## TPU Topologies\n",
    "\n",
    "TPUs are typically deployed in pods of multiple devices connected via a high-bandwidth interchip interconnect (ICI) for communication within the pod that is much faster than a typical network connection. For example, the specifications sheet for a [TPU v5p](https://cloud.google.com/tpu/docs/v5p) states an ICI bandwidth of 4.8Tb/s per chip (for reference, TPU v5p also has 21Tb/s of *local* HBM bandwidth). The ICI allows us to implement fast and performant distributed kernels that require high-bandwidth communication within a pod, and use the datacenter network for parallelization over less bandwidth-intensive operations, such as data-parallelism over a batch dimension.\n",
    "\n",
    "TPUs pods are typically arranged in an ND torus topology. The following graphic gives several examples of configurations of different sizes.\n",
    "\n",
    "![tpu_topologies](https://cloud.google.com/static/tpu/docs/images/v4-topologies.png)\n",
    "\n",
    "Flattened as a graph, the torus can be visualized as follows. Each edge (orange or black) is a bidirectional connection between two devices. You will commonly hear about rings in conjunction with discussion about device toplogies â€” a key feature of a torus is that when taking a slice along an axis of the pod, such as the nodes `[(0,1), (1, 1), (2, 1), (3, 1)]` or `[(0, 1), (1, 1)]`, we have a ring of devices. This is a feature we can use to simplify communication patterns within the pod.\n",
    "\n",
    "![tpu_torus](https://cloud.google.com/static/tpu/docs/images/untwisted-tori.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Oc_WD1hChfN"
   },
   "source": [
    "## Remote Direct Memory Access (RDMA) Model\n",
    "\n",
    "TPUs communicate via a push-only model known as a remote direct memory access (RDMA). A TPU is allowed to issue copy instruction to push from a local buffer to any buffer on another device within the same pod that executes asynchronously from the main program thread. However, a TPU can only read data that is stored locally. This is in contrast to more traditional multi-core programming where it is possible to both read from and write to values to a shared memory.\n",
    "\n",
    "### Async Remote Copy Operation\n",
    "The `pltpu.make_async_remote_copy` function is used to create a remote DMA descriptor object which parameterizes both a \"send\" operation and a \"receive\" operation. Here's its signature:\n",
    "\n",
    "```python\n",
    " def make_async_remote_copy(\n",
    "     src_ref: Ref,\n",
    "     dst_ref: Ref,\n",
    "     send_sem: Ref[SemaphoreType],\n",
    "     recv_sem: Ref[SemaphoreType],\n",
    "     device_id: int | tuple[int, ...],\n",
    "     device_id_type: DeviceIdType\n",
    " ) -> AsyncCopyDescriptor:\n",
    "```\n",
    "\n",
    "- `src_ref` is the local `Ref` (in any memory space) containing the data you wish to send to `dst_ref` on another device.\n",
    "- `dst_ref` is the remote `Ref` (in any memory space) at which data will be copied to on the target device.\n",
    "- `send_sem` is a DMA semaphore used to block until all data has been sent from `src_ref`.\n",
    "- `recv_sem` is a DMA semaphore used to block until the expected number of bytes have been received at `dst_ref`. The sender of the DMA will write to the receiver's `recv_sem`.\n",
    "- `device_id` is the device ID of the target device to send to.\n",
    "- `device_id_type` specifies the format of `device_id`, which can either be in LOGICAL format (integer device ID), or in MESH format (an ND-tuple index into the logical device mesh). The default mode is MESH.\n",
    "\n",
    "`make_async_remote_copy` returns a descriptor object on which you use the `.start()` method to initiate the DMA, and the `.wait_send()` to block on `send_sem` and `.wait_recv()` to block on `recv_sem` (or `.wait()` to block on both). If a device is only expected to send data, it is sufficient to only call `.start()` and `.wait_send()`, and likewise if a device is only receiving it is sufficient to only call `.wait_recv()`. If using a SPMD pattern where all devices execute the DMA, each device will generally call both `.start()` and `.wait()`.\n",
    "```python\n",
    "dma_descriptor = make_async_remote_copy(src_ref, dst_ref, send_sem, recv_sem, device_id)\n",
    "dma_descriptor.start() # Initiate the DMA (non-blocking).\n",
    "# ... do other work\n",
    "dma_descriptor.wait_send() # Block until all data has been sent.\n",
    "dma_descriptor.wait_recv() # Block until all data has been received.\n",
    "```\n",
    "\n",
    "As an example, let's visualize a DMA where we consider 4 devices (indexed 0, 1, 2, 3). We consider a scheme where device 0 copies to device 1, and device 2 & 3 copy to each other. In practice, we can create such an asymmetric communication pattern by using `@pl.when` to branch on the device ID.\n",
    "\n",
    "(1) Each device creates the DMA descriptor. Devices 0, 2, and 3 call `.start()` to initiate the DMA from `src_ref`. Device 1 is skips the `.start()` and does nothing, e.g. by using `pl.when`.\n",
    "\n",
    "![rdma_start](../../_static/pallas/distributed/rdma_start.svg)\n",
    "\n",
    "(2) As `.start()` is non-blocking, each device is free to do other computation while the DMA is in flight. Devices 0, 2, and 3 call `.wait_send()` to wait on `send_sem` which blocks until all data has been sent.\n",
    "\n",
    "![rdma_send](../../_static/pallas/distributed/rdma_send.svg)\n",
    "\n",
    "(3) Finally, devices 1, 2, and 3 will call `.wait_recv()` to wait on `recv_sem` until all data has arrived at `dst_ref`.\n",
    "\n",
    "![rdma_recv](../../_static/pallas/distributed/rdma_recv.svg)\n",
    "\n",
    "The above communication pattern can be written as follows:\n",
    "```python\n",
    "def example_kernel(input_ref, output_ref, send_sem, recv_sem):\n",
    "    device_id = lax.axis_index('x')\n",
    "    copy_0_to_1 = pltpu.make_async_remote_copy(\n",
    "        src_ref=input_ref,\n",
    "        dst_ref=output_ref,\n",
    "        send_sem=send_sem,\n",
    "        recv_sem=recv_sem,\n",
    "        device_id=1,\n",
    "    )\n",
    "    copy_2_to_3 = pltpu.make_async_remote_copy(\n",
    "        src_ref=input_ref,\n",
    "        dst_ref=output_ref,\n",
    "        send_sem=send_sem,\n",
    "        recv_sem=recv_sem,\n",
    "        device_id=3,\n",
    "    )\n",
    "    copy_3_to_2 = pltpu.make_async_remote_copy(\n",
    "        src_ref=input_ref,\n",
    "        dst_ref=output_ref,\n",
    "        send_sem=send_sem,\n",
    "        recv_sem=recv_sem,\n",
    "        device_id=2,\n",
    "    )\n",
    "    @pl.when(device_id == 0)\n",
    "    def _():\n",
    "      copy_0_to_1.start()\n",
    "      copy_0_to_1.wait_send()\n",
    "    @pl.when(device_id == 1)\n",
    "    def _():\n",
    "      copy_0_to_1.wait_recv()\n",
    "    @pl.when(device_id == 2)\n",
    "    def _():\n",
    "      copy_2_to_3.start()\n",
    "      copy_2_to_3.wait_send()\n",
    "      copy_3_to_2.wait_recv()\n",
    "    @pl.when(device_id == 3)\n",
    "    def _():\n",
    "      copy_3_to_2.start()\n",
    "      copy_3_to_2.wait_send()\n",
    "      copy_2_to_3.wait_recv()\n",
    "```\n",
    "\n",
    "### DMA Semaphores\n",
    "\n",
    "`send_sem` and `recv_sem` are instances of a special type of semaphore reserved exclusively for use with DMAs. They must be allocated with the `tpu.SemaphoreType.DMA` type when specifying input specs to `pallas_call`.\n",
    "\n",
    "Internally, DMA semaphores can be thought of as integer-valued progress trackers. On DMA start, the local device will begin to increment the value of `send_sem` and the receiver's `recv_sem` asynchronously. Waiting on a semaphore will block until the value of the semaphore reaches the total bytes of data sent/received; when the value is reached, waiting threads are released and the sempahore's value is decremented by the same amount. This means that either all data has been sent (for `send_sem`) or all data has been received (for `dst_sem`). The value of the semaphore can be read with `pl.semaphore_read`, but note that the underlying semantics of the value could change between hardware generations (e.g. the value may not represent exactly the number of bytes sent, although this is a useful mental model to have when reasoning about the behavior of the semaphore).\n",
    "\n",
    "### Routing\n",
    "\n",
    "A sender is allowed to send data to any receiver within the same pod, even if they do not share a direct connection (the exception to this rule is for TPU v5e, where devices can only route to a power of 2 offset from themselves). TPUs have an internal routing mechanism which can pass data along to the next device on the path to the destination. However, communicating in this way is not recommended as you have no control over network contention as a kernel writer. The examples we will cover in this tutorial minimize inefficient communication by only transferring data to neighboring devices.\n",
    "\n",
    "### Failure modes\n",
    "\n",
    "If using remote DMAs incorrectly, you may encounter several failure modes which can be difficult to debug. The general symptoms of buggy DMA usage are crashes, hanging, or silent data corruption:\n",
    "- If semaphores exit the program with an invalid non-zero value, Pallas will crash and exit the program.\n",
    "- If semaphores are waited on but an insufficient number of bytes are received (i.e. there is no sender, or if the sent data is less than the size of `dst_ref` on the receiving device), the program may hang indefinitely waiting for bytes that are never sent. In this case the program would need to be restarted.\n",
    "- If encountering a race condition, there could be silent data corruption if two simultaneous writes or a simultaneous read and write occur.\n",
    "\n",
    "Some common causes of the above include:\n",
    "- If a device calls `.wait_recv()` but no other device sends to it, the kernel may hang.\n",
    "- If a device is sent a more bytes than it expected to receive, it may also crash due to non-zero semaphore states. If sent less, it may hang indefinitely.\n",
    "- If DMAs are started but the semaphores are not waited on, the program may crash due to non-zero semaphore states.\n",
    "- If two devices copy to the same destination, you may encounter non-deterministic results due to a race condition, or crashing due to  non-zero semaphore states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpGSN1Sui0Bu"
   },
   "source": [
    "### Example: Right Permute (`lax.ppermute`)\n",
    "\n",
    "Let's dive into a very basic example. We will implement a kernel that performs a right permutation, where each device sends its slice of the data to its right neighbor.\n",
    "\n",
    "Suppose we had an array with 512 elements, which we shard into slices of size 128 across 4 devices. Each device will pass its slice to the next device, and the output will consist of the same data, but with the slices rotated by 1. This is identical to the `lax.ppermute` operation where the permutation is set to `(n, (n+1) % 4)`.\n",
    "\n",
    "In order to call the kernel in distributed mode, we wrap the `pallas_call` in a `shard_map` transformation. From there, we can write the kernel the same way as you would write a normal single-device Pallas kernel, except we now have access to remote DMA instructions. JAX collective primitives such as `lax.axis_index` can be used to obtain a `device_id` that can be used to compute which target devices to copy to, by referencing the same named axes names passed into `shard_map`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 1606,
     "status": "ok",
     "timestamp": 1722904803566,
     "user": {
      "displayName": "Justin Fu",
      "userId": "17543197034567316452"
     },
     "user_tz": 420
    },
    "id": "YkyIKN2thZ-V",
    "outputId": "9b7ed142-d161-4237-fed8-cbce41adc5f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input =  [0.9858954  0.11763906 0.9955574  0.775211  ]\n",
      "Pallas Result =  [0.775211   0.9858954  0.11763906 0.9955574 ]\n",
      "lax.ppermute Result =  [0.775211   0.9858954  0.11763906 0.9955574 ]\n",
      "Difference |Pallas - lax.ppermute| =  0.0\n"
     ]
    }
   ],
   "source": [
    "partition = P(None, 'x')\n",
    "mesh = jax.make_mesh((num_devices,), ('x',))\n",
    "sharding = jax.sharding.NamedSharding(mesh, partition)\n",
    "\n",
    "# Create an input array that shards the last dimension across\n",
    "# all devices.\n",
    "input_arr = jax.random.uniform(jax.random.key(0), (8, 128 * num_devices))\n",
    "input_arr = jax.device_put(input_arr, sharding)\n",
    "\n",
    "\n",
    "def right_permute_kernel(input_ref, output_ref, send_sem, recv_sem):\n",
    "  my_id = lax.axis_index('x')\n",
    "  right_neighbor = lax.rem(my_id + 1, num_devices)\n",
    "  remote_copy_op = pltpu.make_async_remote_copy(\n",
    "      src_ref=input_ref,\n",
    "      dst_ref=output_ref,\n",
    "      send_sem=send_sem,\n",
    "      recv_sem=recv_sem,\n",
    "      device_id=(right_neighbor,),\n",
    "      device_id_type=pltpu.DeviceIdType.MESH,\n",
    "  )\n",
    "  remote_copy_op.start()\n",
    "  remote_copy_op.wait()\n",
    "\n",
    "\n",
    "out_shape = jax.ShapeDtypeStruct((8, 128), jnp.float32)\n",
    "grid_spec = pltpu.PrefetchScalarGridSpec(\n",
    "    num_scalar_prefetch=0,\n",
    "    # TPUMemorySpace.ANY will (usually) place the tensor in HBM.\n",
    "    in_specs=[\n",
    "        pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY),\n",
    "    ],\n",
    "    out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY),\n",
    "    scratch_shapes=(\n",
    "        # We allocate DMA semaphores in scratch memory.\n",
    "        [pltpu.SemaphoreType.DMA] * 2\n",
    "    ),\n",
    ")\n",
    "right_permute = pl.pallas_call(\n",
    "    right_permute_kernel,\n",
    "    out_shape=out_shape,\n",
    "    grid_spec=grid_spec,\n",
    ")\n",
    "# Wrap the kernel within a shard_map to call.\n",
    "pallas_result = jax.jit(\n",
    "    shard_map.shard_map(\n",
    "        right_permute,\n",
    "        mesh=mesh,\n",
    "        in_specs=partition,\n",
    "        out_specs=partition,\n",
    "        check_rep=False,\n",
    "    )\n",
    ")(input_arr)\n",
    "\n",
    "# Compare Pallas result to XLA shard_map result.\n",
    "perm = tuple((src, (src + 1) % num_devices) for src in range(num_devices))\n",
    "\n",
    "xla_result = jax.jit(\n",
    "    shard_map.shard_map(\n",
    "        lambda x: lax.ppermute(x, 'x', perm),\n",
    "        mesh=mesh, in_specs=partition, out_specs=partition)\n",
    ")(input_arr)\n",
    "\n",
    "print('Input = ', input_arr[0, ::128])\n",
    "print('Pallas Result = ', pallas_result[0, ::128])\n",
    "print('lax.ppermute Result = ', xla_result[0, ::128])\n",
    "print(\n",
    "    'Difference |Pallas - lax.ppermute| = ',\n",
    "    jnp.mean(jnp.abs(pallas_result - xla_result)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iyfhdGXuUnq2"
   },
   "source": [
    "### Example: All-gather (`lax.all_gather`)\n",
    "\n",
    "In this next example we will implement the all-gather collective operation, which has a JAX equivalent in `lax.all_gather`. In contrast with the right-permute example from above which only involves a pair of source and destination neighbors, an all-gather operation requires communication between all devices and therefore we must think about how data is routed between them. The specifics of how we implement this are dictated by the device topology, for which we assume is a ring.\n",
    "\n",
    "#### Ring Communication Pattern\n",
    "\n",
    "We will write our kernel assuming a ring topology. Rings are a natural fit for TPUs as slicing along any dimension of a torus produces a ring. When writing collectives, we often only need to think about 1D slices of our torus at a time because the different dimensions of the torus are reserved for different types of parallelism (data vs. model, for example).\n",
    "\n",
    "The strategy we will use is to write a looped kernel, where on each iteration a device receives one slice of the sharded array from its left neighbor, and copies the previously received slice to its right neighbor. After `num_devices` iterations, each device will have a copy of the entire array in its local HBM.\n",
    "\n",
    "![all_gather](../../_static/pallas/distributed/all_gather.svg)\n",
    "\n",
    "We can re-purpose Pallas's `grid` argument to implement the loop. Rather than iterating over tiles of an array as we have done in previous tutorials, we instead set the grid to `(num_devices,)` to indicate that we want to loop over the number of devices and use `pl.program_id` to obtain the loop iteration inside of the Pallas kernel. The following code snippet demonstrates how to implement this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 812,
     "status": "ok",
     "timestamp": 1722904804531,
     "user": {
      "displayName": "Justin Fu",
      "userId": "17543197034567316452"
     },
     "user_tz": 420
    },
    "id": "ojQEZB5mBRqM",
    "outputId": "e1648f54-737c-4921-ca3b-b4c639a38d2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  (32, 128) [0.9858954  0.54248166 0.9547038  0.954962  ]\n",
      "Pallas Result:  (16, 8, 128) [0.9858954  0.54248166 0.9547038  0.954962   0.9858954  0.54248166\n",
      " 0.9547038  0.954962   0.9858954  0.54248166 0.9547038  0.954962\n",
      " 0.9858954  0.54248166 0.9547038  0.954962  ]\n",
      "lax.all_gather Result:  (16, 8, 128) [0.9858954  0.54248166 0.9547038  0.954962   0.9858954  0.54248166\n",
      " 0.9547038  0.954962   0.9858954  0.54248166 0.9547038  0.954962\n",
      " 0.9858954  0.54248166 0.9547038  0.954962  ]\n",
      "Difference |Pallas - lax.all_gather| =  0.0\n"
     ]
    }
   ],
   "source": [
    "partition = P('x', None)\n",
    "mesh = jax.make_mesh((num_devices,), ('x',))\n",
    "sharding = jax.sharding.NamedSharding(mesh, partition)\n",
    "\n",
    "# Create an input array that shards the first dimension across\n",
    "# all devices.\n",
    "input_arr = jax.random.uniform(jax.random.key(0), (8 * num_devices, 128))\n",
    "input_arr = jax.device_put(input_arr, sharding)\n",
    "\n",
    "\n",
    "def all_gather_kernel(input_ref,\n",
    "                      output_ref,\n",
    "                      local_copy_sem,\n",
    "                      send_sem,\n",
    "                      recv_sems):\n",
    "  outer_step = pl.program_id(0)\n",
    "  my_id = lax.axis_index('x')\n",
    "  right_neighbor = lax.rem(my_id + 1, num_devices)\n",
    "  copy_slot = my_id - outer_step\n",
    "  copy_slot = lax.rem(copy_slot + num_devices, num_devices)\n",
    "\n",
    "  @pl.when(outer_step == 0)\n",
    "  def _():\n",
    "    local_copy_op = pltpu.make_async_copy(\n",
    "      src_ref=input_ref,\n",
    "      dst_ref=output_ref.at[my_id],\n",
    "      sem=local_copy_sem,\n",
    "    )\n",
    "    local_copy_op.start()\n",
    "    local_copy_op.wait()\n",
    "\n",
    "  # Copy to our right neighbor.\n",
    "  # Note that we will also be receiving data from our left neighbor,\n",
    "  # but at `copy_slot-1` rather than `copy_slot`! This makes use of the fact\n",
    "  # that the indices do not need to be symmetric between remote DMAs.\n",
    "  remote_copy_op = pltpu.make_async_remote_copy(\n",
    "      src_ref=output_ref.at[copy_slot],\n",
    "      dst_ref=output_ref.at[copy_slot],\n",
    "      send_sem=send_sem,\n",
    "      recv_sem=recv_sems.at[outer_step],\n",
    "      device_id=(right_neighbor,),\n",
    "      device_id_type=pltpu.DeviceIdType.MESH,\n",
    "  )\n",
    "  remote_copy_op.start()\n",
    "  remote_copy_op.wait()\n",
    "\n",
    "out_shape = jax.ShapeDtypeStruct((num_devices, 8, 128), jnp.float32)\n",
    "grid_spec = pltpu.PrefetchScalarGridSpec(\n",
    "            num_scalar_prefetch=0,\n",
    "            in_specs=[\n",
    "                # TPUMemorySpace.ANY will (usually) place the tensor in HBM.\n",
    "                pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY),\n",
    "            ],\n",
    "            out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY),\n",
    "            scratch_shapes=(\n",
    "              # DMA semaphores are allocated in scratch memory.\n",
    "              # We allocated one semaphore for a local HBM-VMEM copy,\n",
    "              # and one for the remote send semaphore.\n",
    "              [pltpu.SemaphoreType.DMA] * 2\n",
    "              # We additionally allocate one receive semaphore per device.\n",
    "              # This is to avoid situations where we have multiple\n",
    "              # DMAs in flight, as we do not want to share a receive\n",
    "              # semaphore between the DMAs.\n",
    "              + [pltpu.SemaphoreType.DMA((num_devices-1,))]\n",
    "\n",
    "            ),\n",
    "            grid=(num_devices-1,)\n",
    "        )\n",
    "\n",
    "all_gather = pl.pallas_call(\n",
    "      all_gather_kernel,\n",
    "      out_shape=out_shape,\n",
    "      grid_spec=grid_spec,\n",
    "  )\n",
    "\n",
    "# Wrap the kernel within a shard_map to call.\n",
    "pallas_result = jax.jit(\n",
    "      shard_map.shard_map(\n",
    "          all_gather,\n",
    "          mesh=mesh,\n",
    "          in_specs=partition,\n",
    "          out_specs=partition,\n",
    "          check_rep=False\n",
    "      )\n",
    ")(input_arr)\n",
    "\n",
    "# Compare Pallas result to XLA shard_map result.\n",
    "xla_result = jax.jit(\n",
    "    shard_map.shard_map(\n",
    "        lambda x: lax.all_gather(x, 'x'),\n",
    "        mesh=mesh, in_specs=partition, out_specs=partition\n",
    "    )\n",
    ")(input_arr)\n",
    "\n",
    "print('Input: ', input_arr.shape, input_arr[::8, 0])\n",
    "print('Pallas Result: ', pallas_result.shape, pallas_result[:, 0, 0])\n",
    "print('lax.all_gather Result: ', xla_result.shape, xla_result[:, 0, 0])\n",
    "print('Difference |Pallas - lax.all_gather| = ',\n",
    "      jnp.mean(jnp.abs(pallas_result - xla_result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KgU7HI2pS4om"
   },
   "source": [
    "A detail worth mentioning here is the use of multiple receive semaphores. Because we only block on the receiving device, it is still possible for a sender to have sent multiple DMAs in flight before the receiver has finished processing the first one (see the next section and reduce-sum example which discusses race conditions in more detail). In this situation we may hit a situation where the same semaphore is being used for multiple DMAs occurring simultaneously. To avoid this, we allocate `num_devices-1` semaphores so there is no risk of re-use. While this race condition is unlikely to happen on such a small kernel, on larger kernels there is more chance for devices to fall out of sync and potentially cause a silent failure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KgU7HI2pS4om"
   },
   "source": [
    "## Advanced Techniques\n",
    "\n",
    "Now that we have seen how to write several basic kernels using remote DMA operations, we will go over more advanced techniques for synchronization and writing efficient kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8M_kdl0FCtrL"
   },
   "source": [
    "### Synchronization: Regular and Barrier Semaphores\n",
    "\n",
    "The examples we implemented in the basic tutorial do not require special handling of synchronization as all necessary communication writes to disjoint buffers. However, other operations may require more complex communication patterns that need additional synchronization primitives to avoid race conditions. Pallas provides two additional primitives to help with this: regular and barrier semaphores.\n",
    "\n",
    "#### Regular Semaphores\n",
    "\n",
    "Regular semaphores are the standard tool used to synchronize across multiple devices. Semaphores are fundamentally counters - they can be incremented by any device after which a device can block until the value of the semaphore reaches a specific value (and then decrement the value).\n",
    "\n",
    "The three main operations that can be used on regular semaphores are signal, wait, and read:\n",
    "```python\n",
    "def semaphore_signal(\n",
    "    sem: Ref[SemaphoreType],\n",
    "    inc: int,\n",
    "    device_id: int | tuple[int, ...],\n",
    "    device_id_type: DeviceIdType\n",
    ") -> None:\n",
    "  ... # Increments the semaphore `sem` on the target device `device_id` by `inc`.\n",
    "  \n",
    "def semaphore_wait(\n",
    "    semaphore: Ref[SemaphoreType],\n",
    "    value: int,\n",
    ") -> None:\n",
    "  ... # Blocks until the locally allocated copy of `sem` reaches `value`, then decrement by `value` and proceed.\n",
    "    \n",
    "def semaphore_read(\n",
    "    sem: Ref[SemaphoreType],\n",
    ") -> jax.Array:\n",
    "  ...  # Returns the current value of `sem` as an `int32[]`.\n",
    "```\n",
    "\n",
    "In order to use regular semaphores, they can be allocated in the same way as a DMA semaphore, but by specifying `pltpu.SemaphoreType.REGULAR` rather than `pltpu.SemaphoreType.DMA`.\n",
    "\n",
    "Semaphores must be zero at the end of a Pallas program to complete succesfully. There are two error cases where this may happen:\n",
    " - If a semaphore is over-signaled, the program will end with non-zero (>0) semaphores. In this case, the program will crash upon completion. This is useful for debugging as non-zero semaphores typically means there is a bug somewhere inside of the program.\n",
    " - If a semaphore is over-waited, the program will hang on the blocking `semaphore_wait` call while it waits for the sempahore to be incremented. In this case the device or program will need to be restarted.\n",
    "\n",
    "#### Barrier Semaphores\n",
    "\n",
    "Barrier semaphores are globally-allocated semaphores used to synchronize devices across an entire program and ensure that all devices have entered the Pallas kernel.\n",
    "\n",
    "If a Pallas kernel is executed within the context of a larger XLA program, we need to ensure that all devices that communicate have entered the kernel. However, DMA and regular semaphores are both locally scoped - they are only understood by other devices that have entered the kernel. Barrier semaphores serve as a globally understood semaphore that can be used for synchronization no matter where in the XLA program the device is currently executing.\n",
    "\n",
    "By default, if you do not specify a barrier semaphore, Pallas will automatically insert a barrier semaphore at the beginning of your program. However, it can be more efficient to write your own. Barrier semaphores are similar to regular semaphores in that they are counters that can be incremented via `semaphore_signal` and can be decremented via `semaphore_wait`. They are created by calling `get_barrier_semaphore()` within a kernel. Typically, we use barriers once at the beginning of a kernel to synchronize with all devices we are communicating with.\n",
    "\n",
    "```python\n",
    "from jax.experimental.pallas import tpu as pltpu\n",
    "\n",
    "def example_kernel(...):\n",
    "  # Use barrier semaphores at the beginning of a kernel.\n",
    "  # is_start_of_kernel = ...\n",
    "  # right_neighbor = ...\n",
    "  # ...\n",
    "  @pl.when(is_start_of_kernel)\n",
    "  def _():\n",
    "    barrier_sem = pltpu.get_barrier_semaphore()\n",
    "    # Increment the semaphore of your right neighbor.\n",
    "    pltpu.semaphore_signal(\n",
    "          barrier_sem,\n",
    "          device_id=right_neighbor,\n",
    "          device_id_type=pltpu.DeviceIdType.LOGICAL,\n",
    "    )\n",
    "    # Wait until your left neighbor has incremented your semaphore\n",
    "    pltpu.semaphore_wait(barrier_sem, 1)\n",
    "  # ...\n",
    "```\n",
    "\n",
    "When using barrier semaphores, the `collective_id` compiler parameter must be passed to `pallas_call` to specify which barrier semaphore is being used. A TPU has a small, fixed number of barrier semaphores available (typically on the order of 20-30) and therefore they should be used sparingly. In order to ensure correctness, only kernels that share the same communication pattern should use the same `collective_id`. For example, if two kernels synchronize only with neighbors on the same mesh axis, they are allowed to share the same `collective_id`. However, if two kernels synchronize along different axes, they must have different `collective_id`s. Failure to do so may result in race conditions that are difficult to debug.\n",
    "\n",
    "```python\n",
    "kernel = pl.pallas_call(\n",
    "      example_kernel,\n",
    "      ...,\n",
    "      compiler_params=pltpu.TPUCompilerParams(collective_id=0),\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zy20AxN5TSLA"
   },
   "source": [
    "### Double-buffering\n",
    "\n",
    "In order to avoid reading from a local `Ref` that is also being written into by another device and creating a race condition, a useful technique is the \"double-buffered\" strategy where we allocate a two `Ref`s for each destination value. On each iteration, one `Ref` will be designated as a \"working\" slot, and the other will be designated as a \"receiving\" slot. The device is free to use the working slot for computation, but will only copy data into its neighbor's receiving slot. The working and receiving slots alternate every iteration, so that once a copy is finished, the old receiving slot becomes the new working slot, and vice versa. Using this scheme properly, data is never read from and written to the same buffer.\n",
    "\n",
    "The following code skeleton demonstrates how double-buffering can be used. We keep a running iteration counter in the variable `iteration`, and the `working_slot` and `receiving_slot` alternate between 0 and 1 every iteration. `dst_ref` is allocated as a double-buffer and has the size `[2, ...]`. On each iteration, we read from the working slot using `dst_ref.at[working_slot, ...]` and use the value to perform computation. Simultaneously, we copy to our neighbor's `dst_ref.at[receiving_slot]` to avoid overwriting their `working_slot` value. By structuring our communication in this fashion it is possible to overlap the communication latency of the remote DMA with local computation while minimizing the risk of race conditions.\n",
    "```python\n",
    "def kernel(...):\n",
    "  # ...\n",
    "  iteration = pl.program_id(0)\n",
    "  working_slot = lax.rem(iteration, 2)\n",
    "  receiving_slot = 1 - working_slot\n",
    "  # ...\n",
    "\n",
    "  local_copy_op = pltpu.make_async_copy(\n",
    "    src_ref=dst_ref.at[working_slot, ...],\n",
    "    dst_ref=local_scratch_ref,\n",
    "    sem=local_copy_sem,\n",
    "  )\n",
    "  local_copy_op.start()\n",
    "  remote_copy_op = pltpu.make_async_remote_copy(\n",
    "    src_ref=src_ref,\n",
    "    dst_ref=dst_ref.at[receiving_slot, ...],\n",
    "    send_sem=send_sem,\n",
    "    recv_sem=recv_sem,\n",
    "    device_id=target_device,\n",
    "    device_id_type=pltpu.DeviceIdType.MESH,\n",
    "  )\n",
    "  remote_copy_op.start()\n",
    "  \n",
    "  local_copy_op.wait()\n",
    "  # ... do work on local_scratch while waiting for async_copy_op to finish.\n",
    "  remote_copy_op.wait()\n",
    "\n",
    "```\n",
    "\n",
    "In terms of synchronization, the double-buffered construction works if all devices are executing on the same iteration. If a sender manages to get one iteration ahead of its receiver, it's `working_slot` and `receiving_slot` indices will be flipped compared to the receiver, meaning that it could be writing into the `working_slot` at the same time the receiver is reading from it. In order to avoid this, it may be necessary to use a semaphore to synchronize the sender with the receiver, or add additional buffering slots (\"triple\", \"quadruple\", or N-buffered) to allow additional run-ahead at the cost of more memory. In our previous `all_gather` example, note that the kernel contained a receiving buffer with N slots, which avoids race conditions altogether. In our next kernel, we will instead go through an example which uses a double-buffer with explicit synchronization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Or0Itv72No5d"
   },
   "source": [
    "### Example: All-Reduce Sum (`lax.psum`)\n",
    "\n",
    "We will now implement an all-reduce sum kernel using double-buffering and semaphores for synchronization. For those familiar with collective operations in JAX, the equivalent operation is `lax.psum`. All-reduce is a standard collective operation where the objective is to reduce along an axis of an array, but the array is sharded across multiple devices.\n",
    "\n",
    "![reduce_sum_1](../../_static/pallas/distributed/reduce_sum_1.svg)\n",
    "\n",
    "In the above example, we have the array [5, 2, 1, 3] sharded across 4 devices. An all-reduce sum operation would sum all values and replicate the result on each device, leading to the result [11, 11, 11, 11] sharded across all 4 devices.\n",
    "\n",
    "The naive implementation of all-reduce would be to gather all required values onto each device, and then reduce. However, we can improve the performance of this implementation by interleaving communication with computation. An interleaved, single-direction all-reduce can be visualized as follows. On each iteration, we receive an input value from our left neighbor, and concurrently pass input along to our next neighbor while incrementing it with our local accumulator. After N-1 iterations, each device will have a copy of the full sum in it's memory.\n",
    "\n",
    "![reduce_sum_2](../../_static/pallas/distributed/reduce_sum_2.svg)\n",
    "\n",
    "#### Putting it all together\n",
    "\n",
    "The following kernel demonstrates how to combine these principles into a functional kernel.\n",
    "\n",
    "The prologue (executed when `outer_step==0`) first initiates a barrier with both neighbors to ensure that they have also entered the kernel. It also handles initialization for all `Ref`s and handles the first remote copy to the right neighbor's \"working\" slot.\n",
    "\n",
    "The main body assumes that a value has already been copied into our local working slot, either from the previous iteration or from the prologue. A complicating factor is that our destination buffers live in HBM, but we need to load values to VMEM before we perform arithmetic. Therefore, we simultaneously copy the working slot value into our VMEM (`receive_scratch`) and pass the value on to our right neighbor's receiving slot. Once the value has been copied into our VMEM, we can accumulate it into our result (contained in `o_ref`).\n",
    "\n",
    "A subtle race condition can occur if one device runs one loop ahead of it's right neighbor. In this case, it could copy into the receiver's `working_slot` at the same time the receiver is reading from it. In order to avoid this, each device will block on a `REGULAR` semaphore before copying into the right neighbor's `dst_ref` until it has signaled that it is done reading from its `working_slot`. This race condition is rarely triggered for a small kernel such as this example, but can it can be explicitly triggered if for example using a `pltpu.delay` instruction to artifically hang a device.\n",
    "\n",
    "Note that this is not an optimal or fully general kernel, as the block sizes must entirely fit in VMEM and we could better interleave communication and accumulation. We will discuss these optimizations in later sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 254,
     "status": "ok",
     "timestamp": 1722904804952,
     "user": {
      "displayName": "Justin Fu",
      "userId": "17543197034567316452"
     },
     "user_tz": 420
    },
    "id": "XrY5bMlvBroQ",
    "outputId": "77497000-4496-462e-cc3c-73fb640cc14c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input =  [0.9858954  0.11763906 0.9955574  0.775211  ]\n",
      "Pallas result =  [2.8743029 2.8743029 2.8743029 2.8743029]\n",
      "lax.psum result =  [2.8743029 2.8743029 2.8743029 2.8743029]\n",
      "Difference |Pallas - lax.psum| =  1.4959369e-08\n"
     ]
    }
   ],
   "source": [
    "partition = P(None, 'x')\n",
    "mesh = jax.make_mesh((num_devices,), ('x',))\n",
    "sharding = jax.sharding.NamedSharding(mesh, partition)\n",
    "\n",
    "input_arr = jax.random.uniform(jax.random.key(0), shape=(8, 128 * num_devices))\n",
    "input_arr = jax.device_put(input_arr, sharding)\n",
    "\n",
    "\n",
    "def all_reduce_kernel(\n",
    "    x_ref,\n",
    "    o_ref,\n",
    "    hbm_scratch,\n",
    "    copy_sem,\n",
    "    remote_recv_sem,\n",
    "    remote_send_sem,\n",
    "    capacity_sem,\n",
    "    receive_scratch,\n",
    "):\n",
    "  outer_step = pl.program_id(0)\n",
    "  working_slot = lax.rem(outer_step, 2)\n",
    "  receiving_slot = 1 - working_slot\n",
    "\n",
    "  my_id = lax.axis_index('x')\n",
    "  right_neighbor = lax.rem(my_id + 1, num_devices)\n",
    "  left_neighbor = lax.rem(my_id - 1 + num_devices, num_devices)\n",
    "\n",
    "  @pl.when(outer_step == 0)\n",
    "  def _():\n",
    "    # Barrier with both neighbors at the start, since we will be\n",
    "    # communicating with both.\n",
    "    barrier_sem = pltpu.get_barrier_semaphore()\n",
    "    pltpu.semaphore_signal(\n",
    "        barrier_sem,\n",
    "        inc=1,\n",
    "        device_id=(left_neighbor,),\n",
    "        device_id_type=pltpu.DeviceIdType.MESH,\n",
    "    )\n",
    "    pltpu.semaphore_signal(\n",
    "        barrier_sem,\n",
    "        inc=1,\n",
    "        device_id=(right_neighbor,),\n",
    "        device_id_type=pltpu.DeviceIdType.MESH,\n",
    "    )\n",
    "    pltpu.semaphore_wait(barrier_sem, 2)\n",
    "\n",
    "    # Initialize o_ref, acc_scratch, and hbm_scratch.\n",
    "    o_ref[...] = jnp.zeros_like(o_ref)\n",
    "    receive_scratch[...] = jnp.zeros_like(receive_scratch)\n",
    "    initial_copy = pltpu.make_async_remote_copy(\n",
    "        src_ref=x_ref,\n",
    "        dst_ref=hbm_scratch.at[working_slot],\n",
    "        send_sem=remote_send_sem,\n",
    "        recv_sem=remote_recv_sem,\n",
    "        device_id=(right_neighbor,),\n",
    "        device_id_type=pltpu.DeviceIdType.MESH,\n",
    "    )\n",
    "    initial_copy.start()\n",
    "    initial_copy.wait()\n",
    "\n",
    "  # Signal to our left neighbor that we are ready to receive.\n",
    "  # Without this signal, our left neighbor can be >=1 iteration ahead,\n",
    "  # meaning it could write into our working slot.\n",
    "  pltpu.semaphore_signal(\n",
    "      capacity_sem,\n",
    "      inc=1,\n",
    "      device_id=(left_neighbor,),\n",
    "      device_id_type=pltpu.DeviceIdType.MESH,\n",
    "  )\n",
    "\n",
    "  # Copy the partial result our left neighbor sent to us into VMEM for\n",
    "  # computation.\n",
    "  local_copy = pltpu.make_async_copy(\n",
    "      src_ref=hbm_scratch.at[working_slot],\n",
    "      dst_ref=receive_scratch,\n",
    "      sem=copy_sem,\n",
    "  )\n",
    "  local_copy.start()\n",
    "\n",
    "  # Block until our right neighbor is ready to receive.\n",
    "  pltpu.semaphore_wait(capacity_sem, 1)\n",
    "  # Pass the value to our right neighbor.\n",
    "  remote_copy = pltpu.make_async_remote_copy(\n",
    "      src_ref=hbm_scratch.at[working_slot],\n",
    "      dst_ref=hbm_scratch.at[receiving_slot],\n",
    "      send_sem=remote_send_sem,\n",
    "      recv_sem=remote_recv_sem,\n",
    "      device_id=(right_neighbor,),\n",
    "      device_id_type=pltpu.DeviceIdType.MESH,\n",
    "  )\n",
    "  remote_copy.start()\n",
    "  # Finish local copy and accumulate while remote_copy is happening.\n",
    "  local_copy.wait()\n",
    "  o_ref[...] += receive_scratch[...]\n",
    "  # Block until remote copy finishes.\n",
    "  remote_copy.wait()\n",
    "\n",
    "\n",
    "out_shape = (\n",
    "    jax.ShapeDtypeStruct((8, 128), jnp.float32),\n",
    "    # We allocate the double-buffer as a Pallas output so that it is\n",
    "    # resident in HBM.\n",
    "    jax.ShapeDtypeStruct((2, 8, 128), jnp.float32),  # hbm_scratch\n",
    ")\n",
    "\n",
    "grid_spec = pltpu.PrefetchScalarGridSpec(\n",
    "    num_scalar_prefetch=0,\n",
    "    in_specs=[\n",
    "        # Our input lives in VMEM\n",
    "        pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.VMEM),\n",
    "    ],\n",
    "    out_specs=[\n",
    "        # Our output lives in VMEM\n",
    "        pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.VMEM),\n",
    "        # Our double-buffer lives in HBM\n",
    "        pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY),\n",
    "    ],\n",
    "    grid=(num_devices,),\n",
    "    scratch_shapes=(\n",
    "        [pltpu.SemaphoreType.DMA] * 3\n",
    "        + [pltpu.SemaphoreType.REGULAR]  # capacity_sem\n",
    "        + [pltpu.VMEM((8, 128), jnp.float32)]  # receive_scratch\n",
    "    ),\n",
    ")\n",
    "\n",
    "kernel = pl.pallas_call(\n",
    "    all_reduce_kernel,\n",
    "    out_shape=out_shape,\n",
    "    grid_spec=grid_spec,\n",
    "    compiler_params=pltpu.TPUCompilerParams(collective_id=0),\n",
    ")\n",
    "\n",
    "pallas_result = jax.jit(\n",
    "    shard_map.shard_map(\n",
    "        kernel,\n",
    "        mesh=mesh,\n",
    "        in_specs=partition,\n",
    "        out_specs=partition,\n",
    "        check_rep=False,\n",
    "    )\n",
    ")(input_arr)\n",
    "pallas_result = jax.block_until_ready(pallas_result)[0]\n",
    "\n",
    "\n",
    "def lax_sum(x):\n",
    "  return lax.psum(x, 'x')\n",
    "\n",
    "\n",
    "xla_result = jax.jit(\n",
    "    shard_map.shard_map(\n",
    "        lax_sum, mesh=mesh, in_specs=P(None, 'x'), out_specs=P(None, 'x')\n",
    "    )\n",
    ")(input_arr)\n",
    "\n",
    "print('Input = ', input_arr[0, ::128])\n",
    "print('Pallas result = ', pallas_result[0, ::128])\n",
    "print('lax.psum result = ', xla_result[0, ::128])\n",
    "difference = jnp.mean(jnp.abs(pallas_result - xla_result))\n",
    "print('Difference |Pallas - lax.psum| = ', difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8bsZAzQreC_"
   },
   "source": [
    "### Run-ahead and Race Conditions\n",
    "\n",
    "As a general rule of thumb, to maximize performance we want to allow a device to run-ahead of other devices without synchronization as much as possible without sacrificing correctness of the program. While we could enforce a barrier across all devices at the beginning of each iteration, this bottlenecks the performance of the program to the slowest device on each loop. By relaxing synchronization and allowing a moderate amount of run-ahead, we can better accommodate variance in latency between iterations and devices because a device that is slow on one iteration could catch up on the next iteration.\n",
    "\n",
    "In the all-reduce kernel we wrote previously, we allow devices to run ahead but by less than one iteration compared to its neighbors (however, non-neighboring devices could be more than 1 iteration apart). To see why the semaphore synchronization is necessary, consider the case when one device (say device 2) hangs and falls behind the other devices. An RDMA has no \"handshake\" â€” only the receiver is blocked while waiting for the data to arrive. Therefore, each device can run up to one iteration ahead before it becomes blocked waiting for the next RDMA to arrive. If we have N devices, this means that the final device can be up to N iterations ahead of the first device.\n",
    "\n",
    "![race_condition](../../_static/pallas/distributed/race_condition.svg)\n",
    "\n",
    "Without adding synchronization in the other direction (forcing senders to block), device 1 could potentially run up to `N` iterations (`N = num_devices`) ahead of device 2, sending multiple writes and overwriting values in the process. To solve this in the `all_reduce` kernel we wrote previously we implemented a \"handshake\" protocol where the receiver signals back to the sender that it is ready to receive, and only then does the sender begin issuing the next RDMA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UD8lNrqsUeXy"
   },
   "source": [
    "### Bi-directional Communication\n",
    "\n",
    "In our previous kernels, we communicated in a single direction around a ring from left-to-right. However, as ICI connections are bi-directional, we are effectively wasting half of the total bandwidth by not sending values in the opposite direction from right-to-left. In this next kernel we will demonstrate an example which communicates in both directions to maximize ICI bandwidth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4KjakLhbBk73"
   },
   "source": [
    "### Example: Bi-directional Reduce-Scatter (`lax.psum_scatter`)\n",
    "\n",
    "A reduce-scatter operation is the combination of an all-reduce followed by a scatter. Or alternatively, an all-reduce is the combination of a reduce-scatter followed by all-gather.\n",
    "\n",
    "The following graphic depicts the semantics of this operation. We assume that each device starts with a collection of partial sums (denoted by a letter + number, such as `A0`). The goal is to reduce along one axis (numbers), while sharding along the other axis (letters).\n",
    "\n",
    "![reduce_scatter_1](../../_static/pallas/distributed/reduce_scatter_1.svg)\n",
    "\n",
    "In order to implement a bi-directional communication strategy, we slice each input block in half, and designate a direction for each half. The top half of each block will be passed from right-to-left, and the bottom half will be passed from left-to-right. A second deviation from the communication patterns of our previous all-reduce and all-gather kernels is that we will also pass around accumulators or partial sums and keep the inputs local to each device. This is in contrast to the previous examples where we passed around inputs but kept the accumulator local to the device. Passing around the accumulator is a more natural fit for this problem as in contrast to all-reduce, most of the data in the inputs are not part of the output that will be stored locally on the device. (e.g. `B0`, `C0`, and `D0` in the above graphic will not be stored on the device holding `A` at the end).\n",
    "\n",
    "The following diagram illustrates this communication pattern, where the colored boxes represent accumulators (not inputs!). Initially, the accumulator is simply the value that was contained in the input. At each iteration of the algorithm, we will receive a partial sum from our neighbors in each direction. We then compute the correct slice of our input to accumulate into the partial buffer, then pass the new partial sum along to our next neighbor. After N iterations, the accumulator will have passed through each device, meaning that it will hold the full sum in the end.\n",
    "\n",
    "![reduce_scatter_2](../../_static/pallas/distributed/reduce_scatter_2.svg)\n",
    "\n",
    "In terms of construction of the kernel, we introduce an additional `phase` dimension to the Pallas grid, which denotes which accumulator (left or right) we are currently computing on. We let `phase=0` denote the accumulator moving to the left, and `phase=1` denote the accumulator moving to the right. We then pipeline the two phases, such that while computing the result for one phase we are transferring our previously computed values in the opposite direction in preparation for the next phase. For example, when we are on `phase=0` (left), we first begin a DMA to transfer results we computed in the previous iteration to our right neighbor (right-DMA). Then, we accumulate into the left-buffer and save the result to HBM. We then wait for the right-DMA to complete so that it is ready for `phase=1` (right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 544,
     "status": "ok",
     "timestamp": 1722904805699,
     "user": {
      "displayName": "Justin Fu",
      "userId": "17543197034567316452"
     },
     "user_tz": 420
    },
    "id": "nRauUAxNHg28"
   },
   "outputs": [],
   "source": [
    "partition = P(None, 'x')\n",
    "mesh = jax.make_mesh((num_devices,), ('x',))\n",
    "sharding = jax.sharding.NamedSharding(mesh, partition)\n",
    "\n",
    "# We need a block size of (16, 128) to ensure that a half-slice is at least\n",
    "# of size (8, 128), which is the size of a VREG. This makes tiling easier\n",
    "# for the compiler.\n",
    "block_size = (16, 128)\n",
    "input_arr = jax.random.uniform(\n",
    "    jax.random.key(0),\n",
    "    shape=(block_size[0] * num_devices, block_size[1] * num_devices),\n",
    ")\n",
    "input_arr = jax.device_put(input_arr, sharding)\n",
    "\n",
    "LEFT = 0\n",
    "RIGHT = 1\n",
    "\n",
    "\n",
    "def mod(x, n):\n",
    "  return lax.rem(x + n, n)\n",
    "\n",
    "\n",
    "def signal(left_or_right, semaphore):\n",
    "  my_id = lax.axis_index('x')\n",
    "  if left_or_right == LEFT:\n",
    "    neighbor = mod(my_id - 1, num_devices)\n",
    "  else:\n",
    "    neighbor = mod(my_id + 1, num_devices)\n",
    "  pltpu.semaphore_signal(\n",
    "      semaphore,\n",
    "      inc=1,\n",
    "      device_id=(neighbor,),\n",
    "      device_id_type=pltpu.DeviceIdType.MESH,\n",
    "  )\n",
    "\n",
    "\n",
    "def reduce_scatter_kernel(\n",
    "    x_ref,\n",
    "    o_ref,\n",
    "    hbm_scratch,\n",
    "    local_copy_sem,\n",
    "    left_recv_sem,\n",
    "    left_send_sem,\n",
    "    right_recv_sem,\n",
    "    right_send_sem,\n",
    "    left_capacity_sem,\n",
    "    right_capacity_sem,\n",
    "    accum_scratch,\n",
    "):\n",
    "  outer_step = pl.program_id(0)\n",
    "  phase = pl.program_id(1)\n",
    "  is_start = jnp.logical_and(outer_step == 0, phase == 0)\n",
    "  last_iteration = outer_step == pl.num_programs(0) - 1\n",
    "\n",
    "  working_slot = lax.rem(outer_step, 2)\n",
    "  receiving_slot = 1 - working_slot\n",
    "  my_id = lax.axis_index('x')\n",
    "  right_neighbor = mod(my_id + 1, num_devices)\n",
    "  left_neighbor = mod(my_id - 1, num_devices)\n",
    "\n",
    "  left_copy_device = mod(my_id + outer_step + 1, num_devices)\n",
    "  right_copy_device = mod(my_id - outer_step - 1, num_devices)\n",
    "  # Slices can be specified using pl.ds(start, size)\n",
    "  left_copy_slice = pl.ds(0, block_size[0] // 2)\n",
    "  right_copy_slice = pl.ds(block_size[0] // 2, block_size[0] // 2)\n",
    "  current_phase_slice = pl.ds(phase * (block_size[0] // 2), block_size[0] // 2)\n",
    "\n",
    "  initial_left_copy = pltpu.make_async_remote_copy(\n",
    "      src_ref=x_ref.at[my_id, left_copy_slice],\n",
    "      dst_ref=hbm_scratch.at[working_slot, left_copy_slice],\n",
    "      send_sem=left_send_sem,\n",
    "      recv_sem=left_recv_sem,\n",
    "      device_id=(left_neighbor,),\n",
    "      device_id_type=pltpu.DeviceIdType.MESH,\n",
    "  )\n",
    "\n",
    "  initial_right_copy = pltpu.make_async_remote_copy(\n",
    "      src_ref=x_ref.at[my_id, right_copy_slice],\n",
    "      dst_ref=hbm_scratch.at[working_slot, right_copy_slice],\n",
    "      send_sem=right_send_sem,\n",
    "      recv_sem=right_recv_sem,\n",
    "      device_id=(right_neighbor,),\n",
    "      device_id_type=pltpu.DeviceIdType.MESH,\n",
    "  )\n",
    "\n",
    "  left_copy = pltpu.make_async_remote_copy(\n",
    "      src_ref=hbm_scratch.at[working_slot, left_copy_slice],\n",
    "      dst_ref=hbm_scratch.at[receiving_slot, left_copy_slice],\n",
    "      send_sem=left_send_sem,\n",
    "      recv_sem=left_recv_sem,\n",
    "      device_id=(left_neighbor,),\n",
    "      device_id_type=pltpu.DeviceIdType.MESH,\n",
    "  )\n",
    "  right_copy = pltpu.make_async_remote_copy(\n",
    "      # Note: Right copy is flipped with regards to slots since we are copying\n",
    "      # to the next outer_step iteration.\n",
    "      src_ref=hbm_scratch.at[receiving_slot, right_copy_slice],\n",
    "      dst_ref=hbm_scratch.at[working_slot, right_copy_slice],\n",
    "      send_sem=right_send_sem,\n",
    "      recv_sem=right_recv_sem,\n",
    "      device_id=(right_neighbor,),\n",
    "      device_id_type=pltpu.DeviceIdType.MESH,\n",
    "  )\n",
    "\n",
    "  # --- Prologue ---\n",
    "  @pl.when(is_start)\n",
    "  def _():\n",
    "    # Barrier with both neighbors at the start, since we will be\n",
    "    # communicating with both.\n",
    "    barrier_sem = pltpu.get_barrier_semaphore()\n",
    "    pltpu.semaphore_signal(\n",
    "        barrier_sem,\n",
    "        inc=1,\n",
    "        device_id=(left_neighbor,),\n",
    "        device_id_type=pltpu.DeviceIdType.MESH,\n",
    "    )\n",
    "    pltpu.semaphore_signal(\n",
    "        barrier_sem,\n",
    "        inc=1,\n",
    "        device_id=(right_neighbor,),\n",
    "        device_id_type=pltpu.DeviceIdType.MESH,\n",
    "    )\n",
    "    pltpu.semaphore_wait(barrier_sem, 2)\n",
    "\n",
    "    # Initialize o_ref, acc_scratch, and hbm_scratch with initial copies.\n",
    "    o_ref[...] = jnp.zeros_like(o_ref[...])\n",
    "    accum_scratch[...] = jnp.zeros_like(accum_scratch[...])\n",
    "\n",
    "    initial_left_copy.start()\n",
    "    initial_left_copy.wait()\n",
    "    initial_right_copy.start()\n",
    "\n",
    "    # We tell our left neighbor that it is allowed to send to the right.\n",
    "    # (and vice versa for right neighbor)\n",
    "    signal(LEFT, right_capacity_sem)\n",
    "    signal(RIGHT, left_capacity_sem)\n",
    "\n",
    "  # --- Body ---\n",
    "  # At the beginning of our kernel body, we start a DMA which copies\n",
    "  # the result we computed in the previous phase to our neighbor.\n",
    "  # This allows us to overlap the communication of sending our previous phase\n",
    "  # with the computation for the current phase.\n",
    "  @pl.when(~is_start)\n",
    "  def _():\n",
    "    @pl.when(phase == LEFT)\n",
    "    def _():\n",
    "      # We block here until our right neighbor tells use we can send to\n",
    "      # the right.\n",
    "      pltpu.semaphore_wait(right_capacity_sem, 1)\n",
    "      right_copy.start()\n",
    "\n",
    "    @pl.when(phase == RIGHT)\n",
    "    def _():\n",
    "      # We block here until our left neighbor tells use we can send to\n",
    "      # the left.\n",
    "      pltpu.semaphore_wait(left_capacity_sem, 1)\n",
    "      left_copy.start()\n",
    "\n",
    "  local_copy = pltpu.make_async_copy(\n",
    "      src_ref=hbm_scratch.at[working_slot, current_phase_slice],\n",
    "      dst_ref=accum_scratch,\n",
    "      sem=local_copy_sem,\n",
    "  )\n",
    "  local_copy.start()\n",
    "  local_copy.wait()\n",
    "\n",
    "  @pl.when(~last_iteration)\n",
    "  def _():\n",
    "    @pl.when(phase == LEFT)\n",
    "    def _():\n",
    "      accum_scratch[...] += x_ref[left_copy_device, left_copy_slice]\n",
    "\n",
    "    @pl.when(phase == RIGHT)\n",
    "    def _():\n",
    "      accum_scratch[...] += x_ref[right_copy_device, right_copy_slice]\n",
    "\n",
    "  local_copy = pltpu.make_async_copy(\n",
    "      src_ref=accum_scratch,\n",
    "      dst_ref=hbm_scratch.at[working_slot, current_phase_slice],\n",
    "      sem=local_copy_sem,\n",
    "  )\n",
    "  local_copy.start()\n",
    "  local_copy.wait()\n",
    "\n",
    "  @pl.when(is_start)\n",
    "  def _():\n",
    "    initial_right_copy.wait()\n",
    "\n",
    "  # At the end of our kernel body, we wait on the DMA of the previous phase\n",
    "  # to make sure the results are ready for the next phase.\n",
    "  @pl.when(~is_start)\n",
    "  def _():\n",
    "    @pl.when(phase == LEFT)\n",
    "    def _():\n",
    "      right_copy.wait()\n",
    "      signal(LEFT, right_capacity_sem)\n",
    "\n",
    "    @pl.when(phase == RIGHT)\n",
    "    def _():\n",
    "      left_copy.wait()\n",
    "      signal(RIGHT, left_capacity_sem)\n",
    "\n",
    "  # --- Epilogue ---\n",
    "  # Store result on last iteration.\n",
    "  @pl.when(last_iteration)\n",
    "  def _():\n",
    "    # Clean up semaphores so that they exit with a value of 0.\n",
    "    @pl.when(phase == LEFT)\n",
    "    def _():\n",
    "      o_ref[left_copy_slice, ...] = accum_scratch[...]\n",
    "      pltpu.semaphore_wait(right_capacity_sem, 1)\n",
    "\n",
    "    @pl.when(phase == RIGHT)\n",
    "    def _():\n",
    "      o_ref[right_copy_slice, ...] = accum_scratch[...]\n",
    "      pltpu.semaphore_wait(left_capacity_sem, 1)\n",
    "\n",
    "\n",
    "out_shape = (\n",
    "    jax.ShapeDtypeStruct((block_size[0], block_size[1]), jnp.float32),  # output\n",
    "    # Shape: [working/recv, block[0], block[1]]\n",
    "    jax.ShapeDtypeStruct(\n",
    "        (2, block_size[0], block_size[1]), jnp.float32\n",
    "    ),  # hbm_scratch\n",
    ")\n",
    "\n",
    "grid_spec = pltpu.PrefetchScalarGridSpec(\n",
    "    num_scalar_prefetch=0,\n",
    "    in_specs=[\n",
    "        pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.VMEM),\n",
    "    ],\n",
    "    out_specs=[\n",
    "        pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.VMEM),\n",
    "        pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY),\n",
    "    ],\n",
    "    grid=(num_devices, 2),\n",
    "    scratch_shapes=(\n",
    "        [pltpu.SemaphoreType.DMA] * 5\n",
    "        + [pltpu.SemaphoreType.REGULAR] * 2  # Capacity semaphores\n",
    "        + [\n",
    "            pltpu.VMEM((block_size[0] // 2, block_size[1]), jnp.float32)\n",
    "        ]  # accum_scratch\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "def pallas_reduce_scatter(input_arr):\n",
    "  input_arr = input_arr.reshape(num_devices, block_size[0], block_size[1])\n",
    "  return pl.pallas_call(\n",
    "      reduce_scatter_kernel,\n",
    "      out_shape=out_shape,\n",
    "      grid_spec=grid_spec,\n",
    "      compiler_params=pltpu.TPUCompilerParams(collective_id=0),\n",
    "  )(input_arr)[0]\n",
    "\n",
    "\n",
    "pallas_result = jax.jit(\n",
    "    shard_map.shard_map(\n",
    "        pallas_reduce_scatter,\n",
    "        mesh=mesh,\n",
    "        in_specs=P(None, 'x'),\n",
    "        out_specs=P('x', None),\n",
    "        check_rep=False,\n",
    "    )\n",
    ")(input_arr)\n",
    "\n",
    "pallas_result = jax.block_until_ready(pallas_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 596,
     "status": "ok",
     "timestamp": 1722904806442,
     "user": {
      "displayName": "Justin Fu",
      "userId": "17543197034567316452"
     },
     "user_tz": 420
    },
    "id": "E-NMh-_teoi4",
    "outputId": "24beb42f-1bdd-4c34-e8d2-681dd7f2e9c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: (64, 512) [0.78051674 0.3524047  0.59993696 0.9714314  0.24692321 0.01347649\n",
      " 0.01857424 0.24841607 0.86097646 0.8261659  0.9753758  0.6902338\n",
      " 0.4431417  0.963323   0.3158517  0.535548  ]\n",
      "Pallas Result: (64, 128) [1.3593563 1.6274805 1.0979297 3.082869  1.4194957 1.4163033 1.2401303\n",
      " 1.1892898 2.6545286 2.221559  2.7995253 2.08431   2.2509837 3.0726733\n",
      " 2.4662397 1.9542246]\n",
      "lax.psum_scatter Result: (64, 128) [1.3593563 1.6274805 1.0979297 3.082869  1.4194957 1.4163033 1.2401303\n",
      " 1.1892898 2.6545286 2.221559  2.7995253 2.08431   2.2509837 3.0726733\n",
      " 2.4662397 1.9542246]\n",
      "Difference |Pallas - lax.psum_scatter|: 2.3841858e-07\n"
     ]
    }
   ],
   "source": [
    "# Compare our result to XLA.\n",
    "def lax_reduce_sum_scatter(x):\n",
    "  x = x.reshape(num_devices, block_size[0], block_size[1])\n",
    "  return lax.psum_scatter(x, 'x')\n",
    "\n",
    "\n",
    "xla_result = jax.jit(\n",
    "    shard_map.shard_map(\n",
    "        lax_reduce_sum_scatter,\n",
    "        mesh=mesh,\n",
    "        in_specs=P(None, 'x'),\n",
    "        out_specs=P('x', None),\n",
    "    )\n",
    ")(input_arr)\n",
    "\n",
    "print('Input:', input_arr.shape, input_arr[::4, 0])\n",
    "print('Pallas Result:', pallas_result.shape, pallas_result[::4, 0])\n",
    "print('lax.psum_scatter Result:', xla_result.shape, xla_result[::4, 0])\n",
    "print(\n",
    "    'Difference |Pallas - lax.psum_scatter|:',\n",
    "    jnp.max(jnp.abs(pallas_result - xla_result)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ThKas40r40Ji"
   },
   "source": [
    "### Nested Remote and Local DMA Pipelines\n",
    "\n",
    "A limitation of the previous all-reduce and reduce-scatter kernels that we wrote is that the blocks we copy via remote DMA must be small enough to fit in our working VMEM that we use for accumulation. For some kernels it may be advantageous to use larger block sizes to better utilize the TPU. For example, a matrix multiplication requires on the order of $O(N^3)$ compute operations, but only $O(N^2)$ memory transfers. Therefore, we want each block of work transferred between devices to be large enough such that the operation becomes compute bound and we can hide the communication cost using pipelining. For reference, the VMEM of a TPU (for generations v4/v5) is typically on the order of 10-100MB, whereas HBM ranges from 10-100GB.\n",
    "\n",
    "To address this problem, we need to be able to write an \"inner kernel\" that handles local HBM-VMEM pipelining inside of the \"outer kernel\" that handles pipelining larger HBM-HBM transfers between devices. Pallas offers an API for constructing nested pipelines using the `emit_pipeline` function. The basic call signature for `emit_pipeline` follows that of a standard `pallas_call` by specifying a `grid` and `BlockSpec`s for the inputs and outputs:\n",
    "\n",
    "```python\n",
    "def emit_pipeline(\n",
    "    kernel: Callable,\n",
    "    grid: tuple[int],\n",
    "    in_specs: PyTree[BlockSpec] = None,\n",
    "    out_specs: PyTree[BlockSpec] = None,\n",
    "    should_accumulate_out: bool = False,\n",
    "    dimension_semantics: tuple[GridDimensionSemantics] = None,\n",
    ") -> Callable:\n",
    "  ... # Returns a custom pipeline given an inner kernel and BlockSpecs.\n",
    "```\n",
    "\n",
    "Indeed, one can view `pallas_call` itself as simply a wrapper around `emit_pipeline`. Because our outer kernel only involves remote HBM-HBM transfers, we are not using any of the built-in pipelining that `pallas_call` provides for HBM-VMEM transfers. The following code skeleton demonstrates what a typical program structure would look like using this pattern:\n",
    "\n",
    "```python\n",
    "\n",
    "def outer_kernel(...):\n",
    "  # ... do work to pipeline remote HBM-HBM transfers (outer kernel)\n",
    "\n",
    "  def inner_kernel(...):\n",
    "    # ... do work (inner kernel)\n",
    "  pltpu.emit_pipeline(\n",
    "          inner_kernel,\n",
    "          grid=inner_grid,\n",
    "          in_specs=...,\n",
    "          out_specs=...,\n",
    "  )(inner_kernel_args)\n",
    "  # ... do more work (outer kernel)\n",
    "\n",
    "pl.pallas_call(\n",
    "  outer_kernel,\n",
    "  grid=outer_grid,\n",
    "  in_specs=...\n",
    "  out_specs=...\n",
    "  scratch=inner_kernel_allocs\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DzFeQjYaasX5"
   },
   "source": [
    "### Example: Reduce-Scatter with large HBM blocks\n",
    "\n",
    "In this next example we will modify our previous reduce-scatter example to utilize a nested inner pipeline. Note that the communication and computation costs of `reduce_scatter` both scale linearly with the size of the input, so we do not necessarily expect to see the operation become compute-bound with larger block sizes. This example is purely for demonstration purposes on how to use the pipeline emitter.\n",
    "\n",
    "We will increase the block sizes of the outer kernel such that they would be undesirable to place inside of VMEM, and allocate all inputs and outputs in HBM (`memory_space=TPUMemorySpace.Any`). The only major change from our previous kernel is the body of the kernel where accumulation is done. Rather than manually copying from HBM to VMEM, accumulating, and copying back to HBM, we use `emit_pipeline` to handle the memory transfers for us. Accumulation is done in an inner kernel with a much smaller, VMEM-friendly block size.\n",
    "\n",
    "In our previous kernel we had the following kernel body to copy data from HBM to the VMEM accumulator, increment, and then copy the results back to HBM:\n",
    "\n",
    "```python\n",
    "local_copy = pltpu.make_async_copy(\n",
    "    src_ref=hbm_scratch.at[working_slot, current_phase_slice],\n",
    "    dst_ref=accum_scratch,\n",
    "    sem=local_copy_sem,\n",
    ")\n",
    "local_copy.start()\n",
    "local_copy.wait()\n",
    "@pl.when(~last_iteration)\n",
    "def _():\n",
    "  @pl.when(phase == LEFT)\n",
    "  def _():\n",
    "    accum_scratch[...] += x_ref[left_copy_device, left_copy_slice]\n",
    "  @pl.when(phase == RIGHT)\n",
    "  def _():\n",
    "    accum_scratch[...] += x_ref[right_copy_device, right_copy_slice]\n",
    "local_copy = pltpu.make_async_copy(\n",
    "    src_ref=accum_scratch,\n",
    "    dst_ref=hbm_scratch.at[working_slot, current_phase_slice],\n",
    "    sem=local_copy_sem,\n",
    ")\n",
    "local_copy.start()\n",
    "local_copy.wait()\n",
    "```\n",
    "\n",
    "Our new kernel replaces it with the following `emit_pipeline` call:\n",
    "\n",
    "```python\n",
    "def inner_kernel(input_ref, accum_ref):\n",
    "  accum_ref[...] = input_ref[...]\n",
    "accum_pipeline = pltpu.emit_pipeline(inner_kernel,\n",
    "                                     in_specs=[inner_block_spec],\n",
    "                                     out_specs=inner_block_spec,\n",
    "                                     should_accumulate_out=True,\n",
    "                                     grid=inner_grid)\n",
    "@pl.when(~last_iteration)\n",
    "def _():\n",
    "  @pl.when(phase == LEFT)\n",
    "  def _():\n",
    "    accum_pipeline(x_ref.at[left_copy_device, left_copy_slice],\n",
    "                   hbm_scratch.at[working_slot, left_copy_slice],\n",
    "    )\n",
    "  @pl.when(phase == RIGHT)\n",
    "  def _():\n",
    "    accum_pipeline(x_ref.at[right_copy_device, right_copy_slice],\n",
    "                   hbm_scratch.at[working_slot, right_copy_slice],\n",
    "    )\n",
    "```\n",
    "\n",
    "The full kernel is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 1341,
     "status": "ok",
     "timestamp": 1722904807930,
     "user": {
      "displayName": "Justin Fu",
      "userId": "17543197034567316452"
     },
     "user_tz": 420
    },
    "id": "27jni-pSartL"
   },
   "outputs": [],
   "source": [
    "partition = P(None, 'x')\n",
    "mesh = jax.make_mesh((num_devices,), ('x',))\n",
    "sharding = jax.sharding.NamedSharding(mesh, partition)\n",
    "\n",
    "# We pick a large outer kernel block size that we do not want to place\n",
    "# in VMEM. For pedagogical purposes we use (4096, 4096), although in\n",
    "# principle this can be much larger.\n",
    "outer_block_size = (4096, 4096)\n",
    "# We pick a smaller VMEM block size for the inner kernel.\n",
    "inner_block_size = (128, 128)\n",
    "input_arr = jax.random.uniform(\n",
    "    jax.random.key(0),\n",
    "    shape=(\n",
    "        outer_block_size[0] * num_devices,\n",
    "        outer_block_size[1] * num_devices,\n",
    "    ),\n",
    ")\n",
    "input_arr = jax.device_put(input_arr, sharding)\n",
    "\n",
    "\n",
    "inner_grid = (\n",
    "    outer_block_size[0] // inner_block_size[0] // 2,\n",
    "    outer_block_size[1] // inner_block_size[1],\n",
    ")\n",
    "inner_block_spec = pl.BlockSpec(\n",
    "    index_map=lambda i, j: (i, j),\n",
    "    block_shape=inner_block_size,\n",
    "    memory_space=pltpu.TPUMemorySpace.ANY,\n",
    ")\n",
    "\n",
    "\n",
    "def reduce_scatter_kernel(\n",
    "    x_ref,\n",
    "    o_ref,\n",
    "    hbm_scratch,\n",
    "    left_recv_sem,\n",
    "    left_send_sem,\n",
    "    copy_sem,\n",
    "    right_recv_sem,\n",
    "    right_send_sem,\n",
    "    left_capacity_sem,\n",
    "    right_capacity_sem,\n",
    "):\n",
    "  outer_step = pl.program_id(0)\n",
    "  phase = pl.program_id(1)\n",
    "  is_start = jnp.logical_and(outer_step == 0, phase == 0)\n",
    "  last_iteration = outer_step == pl.num_programs(0) - 1\n",
    "\n",
    "  working_slot = lax.rem(outer_step, 2)\n",
    "  receiving_slot = 1 - working_slot\n",
    "  my_id = lax.axis_index('x')\n",
    "  right_neighbor = mod(my_id + 1, num_devices)\n",
    "  left_neighbor = mod(my_id - 1, num_devices)\n",
    "\n",
    "  left_copy_device = mod(my_id + outer_step + 1, num_devices)\n",
    "  right_copy_device = mod(my_id - outer_step - 1, num_devices)\n",
    "  left_copy_slice = pl.ds(0, outer_block_size[0] // 2)\n",
    "  right_copy_slice = pl.ds(outer_block_size[0] // 2, outer_block_size[0] // 2)\n",
    "  current_phase_slice = pl.ds(\n",
    "      phase * (outer_block_size[0] // 2), outer_block_size[0] // 2\n",
    "  )\n",
    "\n",
    "  initial_left_copy = pltpu.make_async_remote_copy(\n",
    "      src_ref=x_ref.at[my_id, left_copy_slice],\n",
    "      dst_ref=hbm_scratch.at[working_slot, left_copy_slice],\n",
    "      send_sem=left_send_sem,\n",
    "      recv_sem=left_recv_sem,\n",
    "      device_id=(left_neighbor,),\n",
    "      device_id_type=pltpu.DeviceIdType.MESH,\n",
    "  )\n",
    "\n",
    "  initial_right_copy = pltpu.make_async_remote_copy(\n",
    "      src_ref=x_ref.at[my_id, right_copy_slice],\n",
    "      dst_ref=hbm_scratch.at[working_slot, right_copy_slice],\n",
    "      send_sem=right_send_sem,\n",
    "      recv_sem=right_recv_sem,\n",
    "      device_id=(right_neighbor,),\n",
    "      device_id_type=pltpu.DeviceIdType.MESH,\n",
    "  )\n",
    "\n",
    "  left_copy = pltpu.make_async_remote_copy(\n",
    "      src_ref=hbm_scratch.at[working_slot, left_copy_slice],\n",
    "      dst_ref=hbm_scratch.at[receiving_slot, left_copy_slice],\n",
    "      send_sem=left_send_sem,\n",
    "      recv_sem=left_recv_sem,\n",
    "      device_id=(left_neighbor,),\n",
    "      device_id_type=pltpu.DeviceIdType.MESH,\n",
    "  )\n",
    "  right_copy = pltpu.make_async_remote_copy(\n",
    "      src_ref=hbm_scratch.at[receiving_slot, right_copy_slice],\n",
    "      dst_ref=hbm_scratch.at[working_slot, right_copy_slice],\n",
    "      send_sem=right_send_sem,\n",
    "      recv_sem=right_recv_sem,\n",
    "      device_id=(right_neighbor,),\n",
    "      device_id_type=pltpu.DeviceIdType.MESH,\n",
    "  )\n",
    "\n",
    "  # --- Prologue ---\n",
    "  @pl.when(is_start)\n",
    "  def _():\n",
    "    # Barrier with both neighbors at the start, since we will be\n",
    "    # communicating with both.\n",
    "    barrier_sem = pltpu.get_barrier_semaphore()\n",
    "    pltpu.semaphore_signal(\n",
    "        barrier_sem,\n",
    "        inc=1,\n",
    "        device_id=(left_neighbor,),\n",
    "        device_id_type=pltpu.DeviceIdType.MESH,\n",
    "    )\n",
    "    pltpu.semaphore_signal(\n",
    "        barrier_sem,\n",
    "        inc=1,\n",
    "        device_id=(right_neighbor,),\n",
    "        device_id_type=pltpu.DeviceIdType.MESH,\n",
    "    )\n",
    "    pltpu.semaphore_wait(barrier_sem, 2)\n",
    "\n",
    "    initial_left_copy.start()\n",
    "    initial_left_copy.wait()\n",
    "    initial_right_copy.start()\n",
    "\n",
    "    # We tell our left neighbor that it is allowed to send to the right.\n",
    "    # (and vice versa for right neighbor)\n",
    "    signal(LEFT, right_capacity_sem)\n",
    "    signal(RIGHT, left_capacity_sem)\n",
    "\n",
    "  @pl.when(~is_start)\n",
    "  def _():\n",
    "    @pl.when(phase == LEFT)\n",
    "    def _():\n",
    "      # We block here until our right neighbor tells use we can send to\n",
    "      # the right.\n",
    "      pltpu.semaphore_wait(right_capacity_sem, 1)\n",
    "      right_copy.start()\n",
    "\n",
    "    @pl.when(phase == RIGHT)\n",
    "    def _():\n",
    "      # We block here until our left neighbor tells use we can send to\n",
    "      # the left.\n",
    "      pltpu.semaphore_wait(left_capacity_sem, 1)\n",
    "      left_copy.start()\n",
    "\n",
    "  # --- Body ---\n",
    "  def inner_kernel(input_ref, accum_ref):\n",
    "    # We do not explicitly use += because we set should_accumulate_out=True.\n",
    "    accum_ref[...] = input_ref[...]\n",
    "\n",
    "  accum_pipeline = pltpu.emit_pipeline(\n",
    "      inner_kernel,\n",
    "      in_specs=[inner_block_spec],\n",
    "      out_specs=inner_block_spec,\n",
    "      should_accumulate_out=True,\n",
    "      grid=inner_grid,\n",
    "  )\n",
    "\n",
    "  @pl.when(~last_iteration)\n",
    "  def _():\n",
    "    @pl.when(phase == LEFT)\n",
    "    def _():\n",
    "      accum_pipeline(\n",
    "          x_ref.at[left_copy_device, left_copy_slice],\n",
    "          hbm_scratch.at[working_slot, left_copy_slice],\n",
    "      )\n",
    "\n",
    "    @pl.when(phase == RIGHT)\n",
    "    def _():\n",
    "      accum_pipeline(\n",
    "          x_ref.at[right_copy_device, right_copy_slice],\n",
    "          hbm_scratch.at[working_slot, right_copy_slice],\n",
    "      )\n",
    "\n",
    "  # --- Epilogue ---\n",
    "  @pl.when(is_start)\n",
    "  def _():\n",
    "    initial_right_copy.wait()\n",
    "\n",
    "  @pl.when(~is_start)\n",
    "  def _():\n",
    "    @pl.when(phase == LEFT)\n",
    "    def _():\n",
    "      right_copy.wait()\n",
    "      signal(LEFT, right_capacity_sem)\n",
    "\n",
    "    @pl.when(phase == RIGHT)\n",
    "    def _():\n",
    "      left_copy.wait()\n",
    "      signal(RIGHT, left_capacity_sem)\n",
    "\n",
    "  # Store result on last iteration.\n",
    "  @pl.when(last_iteration)\n",
    "  def _():\n",
    "    output_copy = pltpu.make_async_copy(\n",
    "        src_ref=hbm_scratch.at[working_slot, current_phase_slice],\n",
    "        dst_ref=o_ref.at[current_phase_slice],\n",
    "        sem=copy_sem,\n",
    "    )\n",
    "    output_copy.start()\n",
    "    output_copy.wait()\n",
    "\n",
    "    # Clean up semaphores so that they exit with a value of 0.\n",
    "    @pl.when(phase == LEFT)\n",
    "    def _():\n",
    "      pltpu.semaphore_wait(right_capacity_sem, 1)\n",
    "\n",
    "    @pl.when(phase == RIGHT)\n",
    "    def _():\n",
    "      pltpu.semaphore_wait(left_capacity_sem, 1)\n",
    "\n",
    "\n",
    "out_shape = (\n",
    "    jax.ShapeDtypeStruct(\n",
    "        (outer_block_size[0], outer_block_size[1]), jnp.float32\n",
    "    ),\n",
    "    # Shape: [working/recv, block[0], block[1]]\n",
    "    jax.ShapeDtypeStruct(\n",
    "        (2, outer_block_size[0], outer_block_size[1]), jnp.float32\n",
    "    ),  # hbm_scratch\n",
    ")\n",
    "\n",
    "grid_spec = pltpu.PrefetchScalarGridSpec(\n",
    "    num_scalar_prefetch=0,\n",
    "    in_specs=[\n",
    "        pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY),\n",
    "    ],\n",
    "    out_specs=[\n",
    "        pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY),\n",
    "        pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY),\n",
    "    ],\n",
    "    grid=(num_devices, 2),\n",
    "    scratch_shapes=(\n",
    "        [pltpu.SemaphoreType.DMA] * 5\n",
    "        + [pltpu.SemaphoreType.REGULAR] * 2  # Capacity semaphores\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "def pallas_reduce_scatter(input_arr):\n",
    "  input_arr = input_arr.reshape(\n",
    "      num_devices, outer_block_size[0], outer_block_size[1]\n",
    "  )\n",
    "  return pl.pallas_call(\n",
    "      reduce_scatter_kernel,\n",
    "      out_shape=out_shape,\n",
    "      grid_spec=grid_spec,\n",
    "      compiler_params=pltpu.TPUCompilerParams(collective_id=0),\n",
    "  )(input_arr)[0]\n",
    "\n",
    "\n",
    "pallas_result = jax.jit(\n",
    "    shard_map.shard_map(\n",
    "        pallas_reduce_scatter,\n",
    "        mesh=mesh,\n",
    "        in_specs=P(None, 'x'),\n",
    "        out_specs=P('x', None),\n",
    "        check_rep=False,\n",
    "    )\n",
    ")(input_arr)\n",
    "\n",
    "pallas_result = jax.block_until_ready(pallas_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 768,
     "status": "ok",
     "timestamp": 1722904808851,
     "user": {
      "displayName": "Justin Fu",
      "userId": "17543197034567316452"
     },
     "user_tz": 420
    },
    "id": "cTEyiMDyx9Y0",
    "outputId": "1de26695-3713-430e-9ab4-4ea646691680"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: (16384, 16384) [0.74162567 0.0242182  0.27751946 ... 0.05213022 0.36088037 0.04494429]\n",
      "Pallas Result: (16384, 4096) [2.0648427 1.674587  1.9148926 ... 1.3371865 1.3296283 1.2887063]\n",
      "lax.psum_scatter Result: (16384, 4096) [2.0648427 1.674587  1.9148926 ... 1.3371865 1.3296283 1.2887063]\n",
      "Difference |Pallas - lax.psum_scatter|: 2.3841858e-07\n"
     ]
    }
   ],
   "source": [
    "# Now we compare our result to XLA.\n",
    "def lax_reduce_sum_scatter(x):\n",
    "  x = x.reshape(num_devices, outer_block_size[0], outer_block_size[1])\n",
    "  return lax.psum_scatter(x, 'x')\n",
    "\n",
    "\n",
    "xla_result = jax.jit(\n",
    "    shard_map.shard_map(\n",
    "        lax_reduce_sum_scatter,\n",
    "        mesh=mesh,\n",
    "        in_specs=P(None, 'x'),\n",
    "        out_specs=P('x', None),\n",
    "    )\n",
    ")(input_arr)\n",
    "\n",
    "print('Input:', input_arr.shape, input_arr[::4, 0])\n",
    "print('Pallas Result:', pallas_result.shape, pallas_result[::4, 0])\n",
    "print('lax.psum_scatter Result:', xla_result.shape, xla_result[::4, 0])\n",
    "print(\n",
    "    'Difference |Pallas - lax.psum_scatter|:',\n",
    "    jnp.max(jnp.abs(pallas_result - xla_result)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zz5AFbriliyv"
   },
   "source": [
    "## Final Notes\n",
    "\n",
    "### Megacore\n",
    "\n",
    "Certain TPUs contain multiple cores in a [Megacore](pallas_tpu_megacore) configuration. In this configuration, our general recommendation is to only initiate DMAs from a single core, and only perform HBM-HBM transfers. To do this, set one of the grid axes to the number of cores (can be obtained via `jax.devices()[0].num_cores`) and the dimension_semantics to `\"parallel\"`. Then, you can use `core_index = pl.program_id(axis)` to obtain the core index along that axis, and use `@pl.when(core_index==i)` to execute code specific to that core.\n",
    "\n",
    "### Interaction with XLA\n",
    "\n",
    "In this tutorial we covered several kernel examples which replicate the functionality of collective operations in JAX such as `lax.all_gather`, `lax.psum`, and `lax.psum_scatter`. An important caveat to note is that a Pallas kernel is somewhat opaque to the XLA compiler and may cause it to miss some optimizations it would normally perform. For example, XLA can asynchronously dispatch collective operations in order to interleave communication and computation without writing a custom kernel. This is not guaranteed to happen when Pallas kernels are involved so it is important to profile your program to see if this is an issue. Another example is the fact that the `emit_pipeline` function we used in this tutorial to generate nested pipelines is not visible to the XLA compiler, and therefore cannot be fused with neighboring operations.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Excellent follow-up excercises for the reader could include implementing a distributed matrix multiplication, implementing `lax.all_to_all`, and relaxing synchronization to allow for additional run-ahead."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
