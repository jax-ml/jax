{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [
        {
          "file_id": "1pSGSm6X2iYKI4elY_96BC4Ky1Y6U_S2v",
          "timestamp": 1761945248463
        }
      ],
      "last_runtime": {
        "build_target": "//third_party/py/jax_triton/google/pallas_tpu:notebook",
        "kind": "private"
      }
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pallas Core-specifc Programming"
      ],
      "metadata": {
        "id": "YIt0Za36LYg9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this guide, we explore using `pl.core_map` to write Pallas kernels. Compared with `pallas_call`, `core_map` offers a few key characteristics:\n",
        "\n",
        "* **Per-core level programming**: You write code for an TPU/GPU core, not for a JAX device. This is crucial if you want to specifically control a core, or how cores communicate and distribute work among one another.\n",
        "\n",
        "* **Flexible pipelining**: You have the option to write pipelining communications on your own, instead of relying on Pallas grids and specs. This is helpful if your pipeline diverges from the standard \"copy-in, compute & copy-out\" pattern.\n",
        "\n",
        "* **Collectives**: Since `core_map` allows inter-core communications, it is especially helpful when writing collectives on the core level."
      ],
      "metadata": {
        "id": "khDWSc7aOVts"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This guide focuses on TPU. For how to use `core_map` on GPU to achieve higher thread flexibility, check out our [Pallas GPU `core_map` tutorial](https://docs.jax.dev/en/latest/pallas/gpu/reference.html#using-core-map)."
      ],
      "metadata": {
        "id": "i8pl0CLqTVvL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment setup\n",
        "\n",
        "Modern accelerators often have multiple cores under a device. For TPU chips higher than v4, every JAX device by default contains two TensorCores (aka. a [Megacore](https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#chips)). They also contain a [SparseCore](https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#sparsecore), consisting of many subcores.\n",
        "\n",
        "This guide was written on a v5p chip, which contains 4 devices (2 TensorCores each) and a SparseCore of 16 subcores."
      ],
      "metadata": {
        "id": "bsOPXdJkzC-x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14PNaMVsLUur",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1761945195796,
          "user_tz": 420,
          "elapsed": 2194,
          "user": {
            "displayName": "Ivy Zheng",
            "userId": "15297372265856137303"
          }
        },
        "outputId": "da8bce46-c28b-4cff-a957-0158ee330bee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running with 4 TPU v5 devices.\n"
          ]
        }
      ],
      "source": [
        "from functools import partial\n",
        "\n",
        "import jax\n",
        "from jax.sharding import NamedSharding, PartitionSpec as P\n",
        "from jax.experimental import pallas as pl\n",
        "from jax.experimental.pallas import tpu as pltpu\n",
        "from jax.experimental.pallas import tpu_sc as plsc\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "num_devices = jax.local_device_count()\n",
        "assert num_devices > 1, \"Please run this notebook with more than one device.\"\n",
        "assert \"TPU\" in jax.devices()[0].device_kind, \"Please run this notebook with TPU devices.\"\n",
        "print(f\"Running with {num_devices} {jax.devices()[0].device_kind} devices.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition to the typical TPU device mesh, you need to make a mesh of cores. Consider this as an addition dimension called \"core\", with length 2, in addition to the 4-device mesh you work with. That is 8 cores in total."
      ],
      "metadata": {
        "id": "3f0XEhaYnGyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mesh of devices\n",
        "mesh = jax.make_mesh((jax.device_count(),), ('device',))\n",
        "print(mesh)\n",
        "\n",
        "# Mesh of cores, within a JAX device\n",
        "tc_mesh = pltpu.create_tensorcore_mesh('core')\n",
        "print(tc_mesh)\n",
        "\n",
        "num_devices = mesh.size\n",
        "num_cores = len(tc_mesh.devices)\n",
        "print(f\"There are {num_devices} devices, and {num_cores} cores each.\")"
      ],
      "metadata": {
        "id": "jr5MARD-mIlC",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1761945195973,
          "user_tz": 420,
          "elapsed": 2,
          "user": {
            "displayName": "Ivy Zheng",
            "userId": "15297372265856137303"
          }
        },
        "outputId": "96b6d69e-342a-4d23-cf36-654f2bafc879"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mesh('device': 4, axis_types=(Auto,))\n",
            "TensorCoreMesh(devices=array([TensorCore(id=0), TensorCore(id=1)], dtype=object), axis_names=('core',))\n",
            "There are 4 devices, and 2 cores each.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A simple per-core kernel\n",
        "\n",
        "`pl.core_map` allows you to write per-core local code, just as `jax.shard_map` allows you to write per-device code.\n",
        "\n",
        "In the example kernel below, each core has its own VMEM and semaphore allocations. As with normal kernel, you can initiate copy between HBM and VMEM refs using `async_copy`.\n",
        "\n",
        "**Communication amongst cores**\n",
        "\n",
        "Before making a inter-core communication, you may need to do a global barrier signal (`pltpu.semaphore_signal`), to make sure all the destination semaphores have been properly initialized.\n",
        "\n",
        "After that, use `pltpu.make_async_remote_copy` to send the actual data. The `device_id` allows you to specify the destination using only the axis coordinate(s) that are different from the source.\n"
      ],
      "metadata": {
        "id": "CYxwiULfndlh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This runs on every core\n",
        "def swap_cores_kernel(in_hbm, out_hbm,\n",
        "                      in_vmem, scratch_vmem, out_vmem,\n",
        "                      sem, send_sem, recv_sem):\n",
        "  core_index = jax.lax.axis_index('core')\n",
        "  num_cores = jax.lax.axis_size('core')\n",
        "  slc_size = in_hbm.shape[-1] // num_cores\n",
        "  slc = pl.ds(core_index * slc_size, slc_size)\n",
        "\n",
        "  # A barrier to make sure all cores have entered run_scoped.\n",
        "  # You won't need this if not doing inter-core communications.\n",
        "  sem0 = pltpu.get_barrier_semaphore()\n",
        "  for i in range(num_devices):\n",
        "    for j in range(num_cores):\n",
        "      pltpu.semaphore_signal(sem0, 1, device_id={'device': i, 'core': j})\n",
        "  pltpu.semaphore_wait(sem0, num_devices * num_cores)\n",
        "\n",
        "  # Copy in the input\n",
        "  pltpu.async_copy(in_hbm.at[:, slc], in_vmem, sem).wait()\n",
        "\n",
        "  # Swap data between core 0 and core 1\n",
        "  dst_core = (core_index + 1) % num_cores\n",
        "  the_copy = pltpu.make_async_remote_copy(\n",
        "      in_vmem, scratch_vmem, send_sem, recv_sem, device_id={'core': dst_core},\n",
        "  )\n",
        "  the_copy.start()\n",
        "  the_copy.wait()\n",
        "\n",
        "  # Core-local compute\n",
        "  out_vmem[...] = scratch_vmem[...] * 2\n",
        "\n",
        "  # Copy out the output\n",
        "  pltpu.async_copy(out_vmem, out_hbm.at[:, slc], sem).wait()\n"
      ],
      "metadata": {
        "id": "GkGRT2HRJOUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you have the local kernel:\n",
        "\n",
        " * Wrap it with `pl.run_scoped` and HBM refs, so that the required scratch spaces (VMEM and semaphores) are allocated, one copy per core.\n",
        "\n",
        " * Call it inside a `pl.core_map`, which takes the TensorCore mesh.\n",
        "\n",
        "    * You would need `collective_id` if there exists inter-core communications.\n",
        "\n",
        " * Allocate the output buffer and pass in the references of the input and output."
      ],
      "metadata": {
        "id": "2T0tSkFmoFLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (32, 256)\n",
        "local_vmem_shape = (32 // num_devices, 256 // num_cores)\n",
        "input_pspec = P('device', None)\n",
        "sharding = NamedSharding(mesh, input_pspec)\n",
        "\n",
        "@jax.jit\n",
        "@partial(jax.shard_map, mesh=mesh, in_specs=input_pspec, out_specs=input_pspec,\n",
        "         check_vma=False)\n",
        "def swap_cores(x):\n",
        "  # Get buffers out of the input and output\n",
        "  x_hbm_ref, o_hbm_ref = jax.tree.map(jax.new_ref, (x, jax.lax.empty(x.shape, x.dtype)))\n",
        "\n",
        "  @pl.core_map(tc_mesh, compiler_params=pltpu.CompilerParams(collective_id=0))\n",
        "  def _():\n",
        "    pl.run_scoped(\n",
        "        partial(swap_cores_kernel, x_hbm_ref, o_hbm_ref),\n",
        "        *([pltpu.VMEM(local_vmem_shape, x.dtype)] * 3),  # VMEM allocations\n",
        "        *([pltpu.SemaphoreType.DMA] * 3),          # semaphores\n",
        "    )\n",
        "  return o_hbm_ref[...]\n",
        "\n",
        "\n",
        "x = jax.random.normal(jax.random.key(0), input_shape, jnp.float32)\n",
        "x = jax.device_put(x, sharding)\n",
        "y = swap_cores(x)\n",
        "\n",
        "np.testing.assert_array_equal(y[:, 128:], x[:, :128] * 2)\n",
        "np.testing.assert_array_equal(y[:, :128], x[:, 128:] * 2)"
      ],
      "metadata": {
        "id": "KT6zkEKi1Sbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save the boilerplate\n",
        "\n",
        "You could make a shortcut `kernel()` that wraps all the `shard_map`, `core_map` and `run_scoped` boilerplates.\n",
        "\n",
        "Some similar APIs are currently available in Pallas package, such as `plgpu.kernel` and `plsc.kernel`. A unified API may be released soon."
      ],
      "metadata": {
        "id": "dLV8sKa4HuSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def kernel(kernel_body, core_mesh, scratch_shapes,\n",
        "           in_specs, out_specs, out_shape=None, out_dtype=None,\n",
        "           compiler_params=None):\n",
        "  @jax.jit\n",
        "  @partial(jax.shard_map, mesh=mesh, in_specs=in_specs, out_specs=out_specs, check_vma=False)\n",
        "  def run(x):\n",
        "    y = jax.lax.empty(out_shape if out_shape else x.shape,\n",
        "                      out_dtype if out_dtype else x.dtype)\n",
        "    def pl_kernel(hbm_refs):\n",
        "      @pl.core_map(core_mesh, compiler_params=compiler_params)\n",
        "      def _():\n",
        "        pl.run_scoped(partial(kernel_body, *hbm_refs), *scratch_shapes)\n",
        "    _, y = pl.run_state(pl_kernel)((x, y))\n",
        "    return y\n",
        "  return run\n",
        "\n",
        "scratch_shapes = [pltpu.VMEM(local_vmem_shape, x.dtype)] * 3 + [pltpu.SemaphoreType.DMA] * 3\n",
        "y = kernel(swap_cores_kernel, tc_mesh, scratch_shapes, input_pspec, input_pspec,\n",
        "           compiler_params=pltpu.CompilerParams(collective_id=0))(x)\n",
        "\n",
        "np.testing.assert_array_equal(y[:, 128:], x[:, :128] * 2)\n",
        "np.testing.assert_array_equal(y[:, :128], x[:, 128:] * 2)"
      ],
      "metadata": {
        "id": "7cHnsRHPHyfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipelining with `core_map`\n",
        "\n",
        "Note that the kernel above only does simple copies and computes, without automatic pipelining via Pallas `grid` and `BlockSpec`. To do pipelining inside `core_map`, use `pltpu.emit_pipeline` inside the core-local kernel.\n",
        "\n",
        "**Parallelize work per core**\n",
        "\n",
        "Since you are programming on the core level, you get to customize exactly how the work is splitted amongst cores. To do that, you need to:\n",
        "\n",
        "1. Provide an `index_map` function that, given the iteration indices, return *the slice* of the input data that shall be passed in.\n",
        "\n",
        "1. On `BlockSpec`, wrap the corresponding dimension with `pl.BoundedSlice`, indicating the `index_map` function would return a slice instead of a iteration index on that dimension.\n",
        "\n",
        "This manual approach gives you the full control to work splitting, in contrast to `pl.pallas_call`, [which automatically parallelizes an array axis over cores under the hood](https://docs.jax.dev/en/latest/pallas/tpu/pipelining.html#tpus-in-megacore-configuration).\n",
        "\n",
        "**Scratch shapes allocation**\n",
        "\n",
        "Note that in the example below, the top level `pl.run_scoped` (wrapped inside `kernel`) did not allocate any scratch buffer. Instead, the VMEM scratch shapes were allocated within `pltpu.emit_pipeline`, which called another `pl.run_scoped` within.\n"
      ],
      "metadata": {
        "id": "4-G--Wnysdjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_one_body(in_vmem, out_vmem):\n",
        "  out_vmem[...] = in_vmem[...] + 1\n",
        "\n",
        "input_shape = (1024, 1024)\n",
        "input_pspec = P('device', None)\n",
        "\n",
        "def add_one_kernel(x_hbm_ref, o_hbm_ref):\n",
        "  core_idx = jax.lax.axis_index('core')\n",
        "  in_shape = x_hbm_ref.shape\n",
        "  core_slc_size = in_shape[0] // num_cores  # The slice that this core will work on\n",
        "  index_map = lambda i, j: (\n",
        "      pl.ds(pl.multiple_of(core_idx * core_slc_size + i * 8, 8), 8), j)\n",
        "\n",
        "  pltpu.emit_pipeline(\n",
        "      add_one_body,\n",
        "      grid=(core_slc_size // 8, in_shape[1] // 128),\n",
        "      in_specs=[pl.BlockSpec(\n",
        "          block_shape=(pl.BoundedSlice(8), 128), index_map=index_map,\n",
        "      )],\n",
        "      out_specs=[pl.BlockSpec(\n",
        "          block_shape=(pl.BoundedSlice(8), 128), index_map=index_map,\n",
        "      )]\n",
        "  )(x_hbm_ref, o_hbm_ref)\n",
        "\n",
        "\n",
        "x = jax.random.normal(jax.random.key(0), input_shape, jnp.float32)\n",
        "x = jax.device_put(x, NamedSharding(mesh, input_pspec))\n",
        "y = kernel(add_one_kernel, tc_mesh, [], input_pspec, input_pspec)(x)\n",
        "\n",
        "np.testing.assert_array_equal(y, x + 1)"
      ],
      "metadata": {
        "id": "xUMRPLxb1rEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scalar prefetch\n",
        "\n",
        "The code below extended the kernel above but uses [scalar prefetch and dynamic block indexing](https://docs.jax.dev/en/latest/pallas/tpu/sparse.html) to select a specific sub-slice of the input.\n",
        "\n",
        "This involves pre-allocating an SMEM buffer (via the `pl.run_scoped` call inside `kernel`) and fill the buffer using a `sync_copy` before the pipeline starts. The dynamic index value is then being closed-over inside the `index_map` API."
      ],
      "metadata": {
        "id": "Cq5rYyvL2Tte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (1024, 1024)\n",
        "input_pspec = P('device', None)\n",
        "output_shape = (1024, 512)\n",
        "\n",
        "def indexed_add_one_kernel(in_refs, out_refs, i_smem_ref):\n",
        "  (x_hbm_ref, i_hbm_ref), o_hbm_ref = in_refs, out_refs\n",
        "  in_shape = x_hbm_ref.shape\n",
        "  pltpu.sync_copy(i_hbm_ref, i_smem_ref)\n",
        "\n",
        "  core_idx = jax.lax.axis_index('core')\n",
        "  core_slc_size = in_shape[0] // num_cores\n",
        "  i_map = lambda i: pl.ds(pl.multiple_of(core_idx * core_slc_size + i * 8, 8), 8)\n",
        "  j_map = lambda j: pl.ds(pl.multiple_of((i_smem_ref[0] + j * 128), 128), 128)\n",
        "\n",
        "  pltpu.emit_pipeline(\n",
        "      add_one_body,\n",
        "      grid=(core_slc_size // 8, output_shape[1] // 128),\n",
        "      in_specs=[pl.BlockSpec(\n",
        "          block_shape=(pl.BoundedSlice(8), pl.BoundedSlice(128)),\n",
        "          index_map=lambda i, j: (i_map(i), j_map(j)),\n",
        "      )],\n",
        "      out_specs=[pl.BlockSpec(\n",
        "          block_shape=(pl.BoundedSlice(8), 128),\n",
        "          index_map=lambda i, j: (i_map(i), j),\n",
        "      )]\n",
        "  )(x_hbm_ref, o_hbm_ref)\n",
        "\n",
        "\n",
        "xs = jax.random.normal(jax.random.key(0), input_shape, jnp.float32)\n",
        "xs = jax.device_put(xs, NamedSharding(mesh, input_pspec))\n",
        "idx = 256\n",
        "y = kernel(indexed_add_one_kernel, tc_mesh, [pltpu.SMEM((1,), jnp.int32)],\n",
        "            in_specs=((input_pspec, P()),), out_specs=input_pspec,\n",
        "            out_shape=(input_shape[0] // num_devices, input_shape[1] // 2),\n",
        "            out_dtype=jnp.float32,\n",
        "           )((xs, jnp.array([idx])),)\n",
        "\n",
        "np.testing.assert_array_equal(y, xs[:, idx:(idx+512)] + 1)"
      ],
      "metadata": {
        "id": "SE8pTStHeSWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mapping over SparseCores\n",
        "\n",
        "TPU v4 and above includes a [SparseCore](https://openxla.org/xla/sparsecore), which is specialized in sparse memory access and operations. This guide will not dive into the capabilities of SparseCore, but rather show how to run a program on SparseCore with same semantics and minimal changes from the TensorCore code.\n",
        "\n",
        "Start with knowing the basic SparseCore specs of your chip, and create a `VectorSubcoreMesh` for vector operations. Note that each SparseCore has 16 (or other number) subcores, and `core_map` will map your code on each of them."
      ],
      "metadata": {
        "id": "B8qeo-4A2KRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc_info = pltpu.get_tpu_info().sparse_core\n",
        "assert sc_info is not None\n",
        "print(sc_info)\n",
        "\n",
        "sc_mesh = plsc.VectorSubcoreMesh(\n",
        "    core_axis_name=\"core\", subcore_axis_name=\"subcore\",\n",
        "    num_cores=sc_info.num_cores\n",
        ")\n",
        "sc_num_cores = sc_info.num_cores\n",
        "sc_num_subcores = sc_info.num_subcores"
      ],
      "metadata": {
        "id": "AHurx-yyYVvs",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1761945200759,
          "user_tz": 420,
          "elapsed": 56,
          "user": {
            "displayName": "Ivy Zheng",
            "userId": "15297372265856137303"
          }
        },
        "outputId": "e6ef55e1-89c2-4b47-d2a6-0377344ae35f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SparseCoreInfo(num_cores=4, num_subcores=16, num_lanes=8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below is very similar from the `add_one_kernel` we wrote earlier, except for a few differences:\n",
        "\n",
        "1. You need to split the work amongst all subcores, so a few lines to compute the specific slice for each subcore.\n",
        "\n",
        "1. SparseCore register computation allows smaller slices (`4x16` max for int32), so you need nested loops to iterate the slice during computation phase."
      ],
      "metadata": {
        "id": "n2_dfsUWFgwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (4096, 128)\n",
        "SC_REG_OP_SHAPE = (4, 16)\n",
        "\n",
        "def sc_add_one_body(in_vmem, out_vmem):\n",
        "  @pl.loop(0, in_vmem.shape[0], step=SC_REG_OP_SHAPE[0])\n",
        "  def _reg_loop_0(c0):\n",
        "    @pl.loop(0, in_vmem.shape[1], step=SC_REG_OP_SHAPE[1])\n",
        "    def _reg_loop_1(c1):\n",
        "      slc = (pl.ds(c0, SC_REG_OP_SHAPE[0]), pl.ds(c1, SC_REG_OP_SHAPE[1]))\n",
        "      out_vmem.at[*slc][...] = in_vmem.at[*slc][...] + 1\n",
        "\n",
        "\n",
        "def sc_add_one_kernel(x_hbm_ref, o_hbm_ref):\n",
        "  in_shape = x_hbm_ref.shape\n",
        "  core_idx = jax.lax.axis_index('core')\n",
        "  subcore_idx = jax.lax.axis_index(\"subcore\")\n",
        "  cm_idx = core_idx * sc_num_subcores + subcore_idx  # index on the core_map\n",
        "  slc_size = in_shape[0] // (sc_num_subcores * sc_num_cores)\n",
        "  index_map = lambda i, j: (\n",
        "      pl.ds(pl.multiple_of(cm_idx * slc_size + i * 8, 8), 8), j)\n",
        "\n",
        "  pltpu.emit_pipeline(\n",
        "      sc_add_one_body,\n",
        "      grid=(slc_size // 8, in_shape[1] // 128),\n",
        "      in_specs=[pl.BlockSpec(\n",
        "          block_shape=(pl.BoundedSlice(8), 128), index_map=index_map,\n",
        "      )],\n",
        "      out_specs=[pl.BlockSpec(\n",
        "          block_shape=(pl.BoundedSlice(8), 128), index_map=index_map,\n",
        "      )]\n",
        "  )(x_hbm_ref, o_hbm_ref)\n",
        "\n",
        "\n",
        "x = jax.random.randint(jax.random.key(0), input_shape, 0, 64, jnp.int32)\n",
        "x = jax.device_put(x, NamedSharding(mesh, input_pspec))\n",
        "y = kernel(sc_add_one_kernel, sc_mesh, [], input_pspec, input_pspec)(x)\n",
        "\n",
        "np.testing.assert_array_equal(y, x + 1)"
      ],
      "metadata": {
        "id": "6fNShx6k2kxi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}