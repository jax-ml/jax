{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Custom derivative rules for Python code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqiaKasFjH82",
        "colab_type": "text"
      },
      "source": [
        "# Custom derivative rules for JAX-transformable Python functions\n",
        "\n",
        "*mattjj@ Mar 19 2020*\n",
        "\n",
        "There are two ways to define differentiation rules in JAX:\n",
        "\n",
        "1. using `jax.custom_jvp` and `jax.custom_vjp` to define custom differentiation rules for Python functions that are already JAX-transformable; and\n",
        "2. defining new `core.Primitive` instances along with all their transformation rules, for example to call into functions from other systems like solvers, simulators, or general numerical computing systems.\n",
        "\n",
        "This notebook is about #1. To read instead about #2, see the [notebook on adding primitives](https://jax.readthedocs.io/en/latest/notebooks/How_JAX_primitives_work.html).\n",
        "\n",
        "For an introduction to JAX's automatic differentiation API, see [The Autodiff Cookbook](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html). This notebook assumes some familiarity with [`jax.jvp`](https://jax.readthedocs.io/en/latest/jax.html#jax.jvp) and [`jax.grad`](https://jax.readthedocs.io/en/latest/jax.html#jax.vjp), and the mathematical meaning of JVPs and VJPs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Fg3NFNY-2RY",
        "colab_type": "text"
      },
      "source": [
        "## TL;DR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXic8tr--1PK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import jax.numpy as np\n",
        "from jax import custom_jvp\n",
        "\n",
        "@custom_jvp\n",
        "def f(x, y):\n",
        "  return np.sin(x) * y\n",
        "\n",
        "@f.defjvp\n",
        "def f_jvp(primals, tangents):\n",
        "  x, y = primals\n",
        "  x_dot, y_dot = tangents\n",
        "  primal_out = f(x, y)\n",
        "  tangent_out = np.cos(x) * x_dot * y - np.sin(x) * y_dot\n",
        "  return primal_out, tangent_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrNf588X_kJF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "246fe70d-2348-4e3e-f58d-766ef16304bc"
      },
      "source": [
        "from jax import jvp, grad\n",
        "\n",
        "print(f(2., 3.))\n",
        "y, y_dot = jvp(f, (2., 3.), (1., 0.))\n",
        "print(y)\n",
        "print(y_dot)\n",
        "print(grad(f)(2., 3.))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.7278922\n",
            "2.7278922\n",
            "-1.2484405\n",
            "-1.2484405\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35ScHqhrBwPh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from jax import custom_vjp\n",
        "\n",
        "@custom_vjp\n",
        "def f(x, y):\n",
        "  return np.sin(x) * y\n",
        "\n",
        "def f_fwd(x, y):\n",
        "  return f(x, y), (np.cos(x), np.sin(x), y)\n",
        "\n",
        "def f_bwd(res, g):\n",
        "  cos_x, sin_x, y = res\n",
        "  return (cos_x * g * y, -sin_x * g)\n",
        "\n",
        "f.defvjp(f_fwd, f_bwd)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpSozxKUCXgp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "889eb046-19bf-49c7-d8a9-c037b8325fe9"
      },
      "source": [
        "print(grad(f)(2., 3.))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-1.2484405\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5ypWA7XlZpu",
        "colab_type": "text"
      },
      "source": [
        "## Example problems\n",
        "\n",
        "To get an idea of what problems `jax.custom_jvp` and `jax.custom_vjp` are meant to solve, let's go over a few examples. A more thorough introduction to the `jax.custom_jvp` and `jax.custom_vjp` APIs is in [the next section](#scrollTo=Dr0aNkBslfQf).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AR02eyd1GQhC",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "### Numerical stability\n",
        "\n",
        "One application of `jax.custom_jvp` is to improve the numerical stability of differentiation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GksPXslaGPaW",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Say we want to write a function called `log1pexp`, which computes $x \\mapsto \\log ( 1 + e^x )$. We can write that using `jax.numpy`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lWbTvs40ET-",
        "colab_type": "code",
        "outputId": "b7b9d021-5a34-42ee-cba6-90d3a3e1ee55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import jax.numpy as np\n",
        "\n",
        "def log1pexp(x):\n",
        "  return np.log(1. + np.exp(x))\n",
        "\n",
        "log1pexp(3.)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray(3.0485873, dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PL36r_cD0oE8",
        "colab_type": "text"
      },
      "source": [
        "Since it's written in terms of `jax.numpy`, it's JAX-transformable:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgtGKFld02UD",
        "colab_type": "code",
        "outputId": "06691c5d-a3c6-4632-a5c7-96eb7379c978",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from jax import jit, grad, vmap\n",
        "\n",
        "print(jit(log1pexp)(3.))\n",
        "print(jit(grad(log1pexp))(3.))\n",
        "print(vmap(jit(grad(log1pexp)))(np.arange(3.)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.0485873\n",
            "0.95257413\n",
            "[0.5        0.7310586  0.88079715]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o56Nr3V61PKS",
        "colab_type": "text"
      },
      "source": [
        "But there's a numerical stability problem lurking here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVM6iwIO22sB",
        "colab_type": "code",
        "outputId": "39338c73-dd3a-4915-de26-8c634ba96375",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(grad(log1pexp)(100.))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zu9sR2I73wuO",
        "colab_type": "text"
      },
      "source": [
        "That doesn't seem right! After all, the derivative of $x \\mapsto \\log (1 + e^x)$ is $x \\mapsto \\frac{e^x}{1 + e^x}$, and so for large values of $x$ we'd expect the value to be about 1.\n",
        "\n",
        "We can get a bit more insight into what's going on by looking at the jaxpr for the gradient computation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dO6uZlYR4TVp",
        "colab_type": "code",
        "outputId": "3f573ed5-a7a6-49f8-bb03-a26abc4fd4ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "from jax import make_jaxpr\n",
        "\n",
        "make_jaxpr(grad(log1pexp))(100.)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{ lambda  ; a.\n",
              "  let b = exp a\n",
              "      c = add b 1.0\n",
              "      d = div 1.0 c\n",
              "      e = mul d b\n",
              "  in (e,) }"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52HR5EW26PEt",
        "colab_type": "text"
      },
      "source": [
        "Stepping through how the jaxpr would be evaluated, we can see that the last line would involve multiplying values that floating point math will round to 0 and $\\infty$, respectively, which is never a good idea. That is, we're effectively evaluating `lambda x: (1 / (1 + np.exp(x))) * np.exp(x)` for large `x`, which effectively turns into `0. * np.inf`.\n",
        "\n",
        "Instead of generating such large and small values, hoping for a cancellation that floats can't always provide, we'd rather just express the derivative function as a more numerically stable program. In particular, we can write a program that more closely evaluates the equal mathematical expression $1 - \\frac{1}{1 + e^x}$, with no cancellation in sight.\n",
        "\n",
        "This problem is interesting because even though our definition of `log1pexp` could already be JAX-differentiated (and transformed with `jit`, `vmap`, ...), we're not happy with the result of applying standard autodiff rules to the primitives comprising `log1pexp` and composing the result. Instead, we'd like to specify how the whole function `log1pexp` should be differentiated, as a unit, and thus arrange those exponentials better.\n",
        "\n",
        "This is one application of custom derivative rules for Python functions that are already JAX transformable: specifying how a composite function should be differentiated, while still using its original Python definition for other transformations (like `jit`, `vmap`, ...).\n",
        "\n",
        "Here's a solution using `jax.custom_jvp`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQt6MAuTJewG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from jax import custom_jvp\n",
        "\n",
        "@custom_jvp\n",
        "def log1pexp(x):\n",
        "  return np.log(1. + np.exp(x))\n",
        "\n",
        "@log1pexp.defjvp\n",
        "def log1pexp_jvp(primals, tangents):\n",
        "  x, = primals\n",
        "  x_dot, = tangents\n",
        "  ans = log1pexp(x)\n",
        "  ans_dot = (1 - 1/(1 + np.exp(x))) * x_dot\n",
        "  return ans, ans_dot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhiMHulfKBIF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "36c4065b-5e77-4a4c-eedb-857f00a24cf7"
      },
      "source": [
        "print(grad(log1pexp)(100.))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cLDuAo6KGUu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c0f25e98-e4a0-47fe-ee74-a843f3697d55"
      },
      "source": [
        "print(jit(log1pexp)(3.))\n",
        "print(jit(grad(log1pexp))(3.))\n",
        "print(vmap(jit(grad(log1pexp)))(np.arange(3.)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.0485873\n",
            "0.95257413\n",
            "[0.5       0.7310586 0.8807971]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9tHAfrSF1N-",
        "colab_type": "text"
      },
      "source": [
        "### Enforcing a differentiation convention\n",
        "\n",
        "A related application is to enforce a differentiation convention, perhaps at a boundary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_6tdb-QGK-H",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Consider the function $f : \\mathbb{R}_+ \\mapsto \\mathbb{R}_+$ with $f(x) = \\frac{x}{1 + \\sqrt{x}}$, where we take $\\mathbb{R}_+ = [0, \\infty)$. We might implement $f$ as a program like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfF5P7x_GaSe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def f(x):\n",
        "  return x / (1 + np.sqrt(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVcEkF3ZGgv1",
        "colab_type": "text"
      },
      "source": [
        "As a mathematical function on $\\mathbb{R}$ (the full real line), $f$ is not differentiable at zero (because the limit defining the derivative doesn't exist from the left). Correspondingly, autodiff produces a `nan` value:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piI0u5MiHhQh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "5e1ed5f0-271a-4c35-860a-d69cd069b439"
      },
      "source": [
        "print(grad(f)(0.))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IP0H2b7ZHkzD",
        "colab_type": "text"
      },
      "source": [
        "But mathematically if we think of $f$ as a function on $\\mathbb{R}_+$ then it is differentiable at 0 [Rudin's Principles of Mathematical Analysis Definition 5.1, or Tao's Analysis I 3rd ed. Definition 10.1.1 and Example 10.1.6]. Alternatively, we might say as a convention we want to consider the directional derivative from the right. So there is a sensible value for the Python function `grad(f)` to return at `0.0`, namely `1.0`, even though JAX's machinery for differentiation over reals doesn't produce it.\n",
        "\n",
        "We can use a custom JVP rule! In particular, we can define the JVP rule in terms of the derivative function $x \\mapsto \\frac{\\sqrt{x} + 2}{2(\\sqrt{x} + 1)^2}$ on $\\mathbb{R}_+$,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksHmCkcSKQJr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@custom_jvp\n",
        "def f(x):\n",
        "  return x / (1 + np.sqrt(x))\n",
        "\n",
        "@f.defjvp\n",
        "def f_jvp(primals, tangents):\n",
        "  x, = primals\n",
        "  x_dot, = tangents\n",
        "  ans = f(x)\n",
        "  ans_dot = ((np.sqrt(x) + 2) / (2 * (np.sqrt(x) + 1)**2)) * x_dot\n",
        "  return ans, ans_dot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gsh9ZvMTKi1O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "4ce69b63-166b-4e18-cf79-6550046b510a"
      },
      "source": [
        "print(grad(f)(0.))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CICQuI86WK4_"
      },
      "source": [
        "### Python debugging\n",
        "\n",
        "Another application that is motivated by development workflow rather than numerics is to set a `pdb` debugger trace in the backward pass of reverse-mode autodiff.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgxMjNTrGjJn",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "When trying to track down the source of a `nan` runtime error, or just examine carefully the cotangent (gradient) values being propagated, it can be useful to insert a debugger at a point in the backward pass that corresponds to a specific point in the primal computation.\n",
        "\n",
        "We'll defer an example until the next section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IC7tEcr1-Fc5",
        "colab_type": "text"
      },
      "source": [
        "### Implicit function differentiation of iterative implementations\n",
        "\n",
        "This example gets pretty deep in the mathematical weeds!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szAt97t80hew",
        "colab_type": "text"
      },
      "source": [
        "Another application for `jax.custom_vjp` is reverse-mode differentiation of functions that are JAX-transformable (by `jit`, `vmap`, ...) but not efficiently JAX-differentiable for some reason, perhaps because they involve `lax.while_loop`. (It's not possible to produce an XLA HLO program that efficiently computes the reverse-mode derivative of an XLA HLO While loop because that would require a program with unbounded memory use, which isn't possible to express in XLA HLO, at least without side-effecting interactions through infeed/outfeed.)\n",
        "\n",
        "For example, consider this `fixed_point` routine which computes a fixed point by iteratively applying a function in a `while_loop`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uA8X2izXH2b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from jax.lax import while_loop\n",
        "\n",
        "def fixed_point(f, a, x_guess):\n",
        "  def cond_fun(carry):\n",
        "    x_prev, x = carry\n",
        "    return np.abs(x_prev - x) > 1e-6\n",
        "\n",
        "  def body_fun(carry):\n",
        "    _, x = carry\n",
        "    return x, f(a, x)\n",
        "\n",
        "  _, x_star = while_loop(cond_fun, body_fun, (x_guess, f(a, x_guess)))\n",
        "  return x_star"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2xFQAte19sF",
        "colab_type": "text"
      },
      "source": [
        "This is an iterative procedure for numerically solving the equation $x = f(a, x)$ for $x$, by iterating $x_{t+1} = f(a, x_t)$ until $x_{t+1}$ is sufficiently close to $x_t$. The result $x^*$ depends on the parameters $a$, and so we can think of there being a function $a \\mapsto x^*(a)$ that is implicity defined by equation $x = f(a, x)$.\n",
        "\n",
        "We can use `fixed_point` to run iterative procedures to convergence, for example running Newton's method to calculate square roots while only executing adds, multiplies, and divides:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDDwM8bYYzRT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def newton_sqrt(a):\n",
        "  update = lambda a, x: 0.5 * (x + a / x)\n",
        "  return fixed_point(update, a, a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42Ydd7_6aLXU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "29833920-bea5-4853-d55b-10c66a27fd32"
      },
      "source": [
        "print(newton_sqrt(2.))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.4142135\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yFtYWH13QWm",
        "colab_type": "text"
      },
      "source": [
        "We can `vmap` or `jit` the function as well:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_YSXieT3Yyk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a6b00ffe-cba1-4177-c50d-0f1b55ceed31"
      },
      "source": [
        "print(jit(vmap(newton_sqrt))(np.array([1., 2., 3., 4.])))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.        1.4142135 1.7320508 2.       ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emwWIt3d3h1T",
        "colab_type": "text"
      },
      "source": [
        "We can't apply reverse-mode automatic differentiation because of the `while_loop`, but it turns out we wouldn't want to anyway: instead of differentiating through the implementation of `fixed_point` and all its iterations, we can exploit the mathematical structure to do something that is much more memory-efficient (and FLOP-efficient in this case, too!). We can instead use the implicit function theorem [Prop A.25 of Bertsekas's Nonlinear Programming, 2nd ed.], which guarantees (under some conditions) the existence of the mathematical objects we're about to use. In essence, we linearize at the solution and solve those linear equations iteratively to compute the derivatives we want.\n",
        "\n",
        "Consider again the equation $x = f(a, x)$ and the function $x^*$. We want to evaluate vector-Jacobian products like $v^\\mathsf{T} \\mapsto v^\\mathsf{T} \\partial x^*(a_0)$.\n",
        "\n",
        "At least in an open neighborhood around the point $a_0$ at which we want to differentiate, let's assume that the equation $x^*(a) = f(a, x^*(a))$ holds for all $a$. Since the two sides are equal as functions of $a$, their derivatives must be equal as well, so let's differentiate both sides:\n",
        "\n",
        "$\\qquad \\partial x^*(a) = \\partial_0 f(a, x^*(a)) + \\partial_1 f(a, x^*(a))  \\partial x^*(a)$.\n",
        "\n",
        "Setting $A = \\partial_1 f(a_0, x^*(a_0))$ and $B = \\partial_0 f(a_0, x^*(a_0))$, we can write the quantity we're after more simply as\n",
        "\n",
        "$\\qquad \\partial x^*(a_0) = B + A \\partial x^*(a_0)$,\n",
        "\n",
        "or, by rearranging,\n",
        "\n",
        "$\\qquad \\partial x^*(a_0) = (I - A)^{-1} B$.\n",
        "\n",
        "That means we can evaluate vector-Jacobian products like\n",
        "\n",
        "$\\qquad v^\\mathsf{T} \\partial x^*(a_0) = v^\\mathsf{T} (I - A)^{-1} B = w^\\mathsf{T} B$,\n",
        "\n",
        "where $w^\\mathsf{T} = v^\\mathsf{T} (I - A)^{-1}$, or equivalently $w^\\mathsf{T} = v^\\mathsf{T} + w^\\mathsf{T} A$, or equivalently $w^\\mathsf{T}$ is the fixed point of the map $u^\\mathsf{T} \\mapsto v^\\mathsf{T} + u^\\mathsf{T} A$. That last characterization gives us a way to write the VJP for `fixed_point` in terms of a call to `fixed_point`! Moreover, after expanding $A$ and $B$ back out, we can see we need only to evaluate VJPs of $f$ at $(a_0, x^*(a_0))$.\n",
        "\n",
        "Here's the upshot:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4jo-xlvdiym",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from functools import partial\n",
        "\n",
        "from jax import custom_vjp\n",
        "from jax import vjp\n",
        "\n",
        "@partial(custom_vjp, nondiff_argnums=(0,))\n",
        "def fixed_point(f, a, x_guess):\n",
        "  def cond_fun(carry):\n",
        "    x_prev, x = carry\n",
        "    return np.abs(x_prev - x) > 1e-6\n",
        "\n",
        "  def body_fun(carry):\n",
        "    _, x = carry\n",
        "    return x, f(a, x)\n",
        "\n",
        "  _, x_star = while_loop(cond_fun, body_fun, (x_guess, f(a, x_guess)))\n",
        "  return x_star\n",
        "\n",
        "def fixed_point_fwd(f, a, x_init):\n",
        "  x_star = fixed_point(f, a, x_init)\n",
        "  return x_star, (a, x_star)\n",
        "\n",
        "def fixed_point_rev(f, res, x_star_bar):\n",
        "  a, x_star = res\n",
        "  _, vjp_a = vjp(lambda a: f(a, x_star), a)\n",
        "  a_bar, = vjp_a(fixed_point(partial(rev_iter, f),\n",
        "                             (a, x_star, x_star_bar),\n",
        "                             x_star_bar))\n",
        "  return a_bar, np.zeros_like(x_star)\n",
        "  \n",
        "def rev_iter(f, packed, u):\n",
        "  a, x_star, x_star_bar = packed\n",
        "  _, vjp_x = vjp(lambda x: f(a, x), x_star)\n",
        "  return x_star_bar + vjp_x(u)[0]\n",
        "\n",
        "fixed_point.defvjp(fixed_point_fwd, fixed_point_rev)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKzfT6d_mEoB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "99482a53-f7b5-4715-ffcb-66fb0346a7b3"
      },
      "source": [
        "print(newton_sqrt(2.))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.4142135\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hmcpjr6gmtkO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f76cc1d0-de93-4e1c-c1f3-509d3f04df32"
      },
      "source": [
        "print(grad(newton_sqrt)(2.))\n",
        "print(grad(grad(newton_sqrt))(2.))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.35355335\n",
            "-0.088388346\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvVmlaPD7W-4",
        "colab_type": "text"
      },
      "source": [
        "We can check our answers by differentiating `np.sqrt`, which uses a totally different implementation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jj_JnI9Pm4jg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bae61d39-5f13-452b-b700-897d1f9a38d3"
      },
      "source": [
        "print(grad(np.sqrt)(2.))\n",
        "print(grad(grad(np.sqrt))(2.))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.35355338\n",
            "-0.08838835\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HowvqayEuy-H",
        "colab_type": "text"
      },
      "source": [
        "A limitation to this approach is that the argument `f` can't close over any values involved in differentiation. That is, you might notice that we kept the parameter `a` explicit in the argument list of `fixed_point`. While other JAX mechanisms can handle closed-over transformation-traced values in the arguments to higher-order functions (as is done for the control flow primitives like `lax.cond`, `lax.scan`, and `lax.while_loop` itself), `jax.custom_vjp` used as above cannot. A `fixed_point` routine that used a bit more of JAX's internals could have a more convenient and robust API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr0aNkBslfQf",
        "colab_type": "text"
      },
      "source": [
        "## Basic usage of `jax.custom_jvp` and `jax.custom_vjp` APIs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MojTOg4tmQNT",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Use `jax.custom_jvp` to define forward-mode (and, indirectly, reverse-mode) rules\n",
        "\n",
        "Here's a canonical basic example of using `jax.custom_jvp`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVkhbIFAOGZk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from jax import custom_jvp\n",
        "import jax.numpy as np\n",
        "\n",
        "# f :: a -> b\n",
        "@custom_jvp\n",
        "def f(x):\n",
        "  return np.sin(x)\n",
        "\n",
        "# f_jvp :: (a, T a) -> (b, T b)\n",
        "def f_jvp(primals, tangents):\n",
        "  x, = primals\n",
        "  t, = tangents\n",
        "  return f(x), np.cos(x) * t\n",
        "\n",
        "f.defjvp(f_jvp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxhlECvW7Krj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "3dc9640f-ae25-458d-854b-e60330a1812d"
      },
      "source": [
        "from jax import jvp\n",
        "\n",
        "print(f(3.))\n",
        "\n",
        "y, y_dot = jvp(f, (3.,), (1.,))\n",
        "print(y)\n",
        "print(y_dot)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.14112\n",
            "0.14112\n",
            "-0.9899925\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaoQVRzSQ9Qd",
        "colab_type": "text"
      },
      "source": [
        "In words, we start with a a primal function `f` that takes inputs of type `a` and produces outputs of type `b`. We associate with it a JVP rule function `f_jvp` that takes a pair of inputs representing the primal inputs of type `a` and the corresponding tangent inputs of type `T a`, and produces a pair of outputs representing the primal outputs of type `b` and tangent outputs of type `T b`. The tangent outputs should be a linear function of the tangent inputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xGky7yMOavq",
        "colab_type": "text"
      },
      "source": [
        "You can also use `f.defjvp` as a decorator, as in\n",
        "\n",
        "```python\n",
        "@custom_jvp\n",
        "def f(x):\n",
        "  ...\n",
        "\n",
        "@f.defjvp\n",
        "def f_jvp(primals, tangents):\n",
        "  ...\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9R-ppvdQIOC",
        "colab_type": "text"
      },
      "source": [
        "Even though we defined only a JVP rule and no VJP rule, we can use both forward- and reverse-mode differentiation on `f`. JAX will automatically transpose the linear computation on tangent values from our custom JVP rule, computing the VJP as efficiently as if we had written the rule by hand:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hl9Io86pQD6s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3d0ce20f-823c-4cca-8f7b-b40cf3f0c3bd"
      },
      "source": [
        "from jax import grad\n",
        "\n",
        "print(grad(f)(3.))\n",
        "print(grad(grad(f))(3.))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-0.9899925\n",
            "-0.14112\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRlKe5D90svj",
        "colab_type": "text"
      },
      "source": [
        "For automatic transposition to work, the JVP rule's output tangents must be linear as a function of the input tangents. Otherwise a transposition error is raised."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRu-0yg96lXE",
        "colab_type": "text"
      },
      "source": [
        "Multiple arguments work like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFLXlXuq6pRf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@custom_jvp\n",
        "def f(x, y):\n",
        "  return x ** 2 * y\n",
        "\n",
        "@f.defjvp\n",
        "def f_jvp(primals, tangents):\n",
        "  x, y = primals\n",
        "  x_dot, y_dot = tangents\n",
        "  primal_out = f(x, y)\n",
        "  tangent_out = 2 * x * y * x_dot + x ** 2 * y_dot\n",
        "  return primal_out, tangent_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpKwA0oA8DfE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "53871c47-509d-4f4f-cdae-75ffc414467f"
      },
      "source": [
        "print(grad(f)(2., 3.))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZ0yc-Ihoezk",
        "colab_type": "text"
      },
      "source": [
        "Calling a `jax.custom_jvp` function with keyword arguments, or writing a `jax.custom_jvp` function definition with default arguments, are both allowed so long as they can be unambiguosly mapped to positional arguments based on the function signature retrieved by the standard library `inspect.signature` mechanism."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FGwfT67PDs9",
        "colab_type": "text"
      },
      "source": [
        "When you're not performing differentiation, the function `f` is called just as if it weren't decorated by `jax.custom_jvp`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-tB3xCHPRFt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@custom_jvp\n",
        "def f(x):\n",
        "  print('called f!')  # a harmless side-effect\n",
        "  return np.sin(x)\n",
        "\n",
        "@f.defjvp\n",
        "def f_jvp(primals, tangents):\n",
        "  print('called f_jvp!')\n",
        "  x, = primals\n",
        "  t, = tangents\n",
        "  return f(x), np.cos(x) * t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAlRea95PjA5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "26fcf10c-22bb-4bae-94a6-be6f22d5ef34"
      },
      "source": [
        "from jax import vmap, jit\n",
        "\n",
        "print(f(3.))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "called f!\n",
            "0.14112\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyD2ow4NmpI-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "b88e980a-6fe0-4c17-d76b-e03e4692f16f"
      },
      "source": [
        "print(vmap(f)(np.arange(3.)))\n",
        "print(jit(f)(3.))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "called f!\n",
            "[0.        0.841471  0.9092974]\n",
            "called f!\n",
            "0.14112\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzB75KZ5Pz7m",
        "colab_type": "text"
      },
      "source": [
        "The custom JVP rule is invoked during differentiation, whether forward or reverse:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKF0xyAxPyLZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "2ec1142f-0273-4c21-9b28-5d4490bcba12"
      },
      "source": [
        "y, y_dot = jvp(f, (3.,), (1.,))\n",
        "print(y_dot)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "called f_jvp!\n",
            "called f!\n",
            "-0.9899925\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1KaEgA58MEG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "f231d379-b296-4a36-8fc3-924476c57edb"
      },
      "source": [
        "print(grad(f)(3.))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "called f_jvp!\n",
            "called f!\n",
            "-0.9899925\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8JFxk3lQhOs",
        "colab_type": "text"
      },
      "source": [
        "Notice that `f_jvp` calls `f` to compute the primal outputs. In the context of higher-order differentiation, each application of a differentiation transform will use the custom JVP rule if and only if the rule calls the original `f` to compute the primal outputs. (This represents a kind of fundamental tradeoff, where we can't make use of intermediate values from the evaluation of `f` in our rule _and also_ have the rule apply in all orders of higher-order differentiation.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6PLJooTQgVp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "10b1dd9f-f6fa-4774-bb77-7d222a6bc2c4"
      },
      "source": [
        "grad(grad(f))(3.)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "called f_jvp!\n",
            "called f_jvp!\n",
            "called f!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray(-0.14112, dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNxAmFSsaaro",
        "colab_type": "text"
      },
      "source": [
        "You can use Python control flow with `jax.custom_jvp`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkXlSJL6adU2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@custom_jvp\n",
        "def f(x):\n",
        "  if x > 0:\n",
        "    return np.sin(x)\n",
        "  else:\n",
        "    return np.cos(x)\n",
        "\n",
        "@f.defjvp\n",
        "def f_jvp(primals, tangents):\n",
        "  x, = primals\n",
        "  x_dot, = tangents\n",
        "  ans = f(x)\n",
        "  if x > 0:\n",
        "    return ans, 2 * x_dot\n",
        "  else:\n",
        "    return ans, 3 * x_dot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCHmJ56Na2G3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "13c86bfa-03f7-45db-aefd-2958b2b60578"
      },
      "source": [
        "print(grad(f)(1.))\n",
        "print(grad(f)(-1.))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0\n",
            "3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cVdgR7ilt8l",
        "colab_type": "text"
      },
      "source": [
        "### Use `jax.custom_vjp` to define custom reverse-mode-only rules\n",
        "\n",
        "While `jax.custom_jvp` suffices for controlling both forward- and, via JAX's automatic transposition, reverse-mode differentiation behavior, in some cases we may want to directly control a VJP rule, for example in the latter two example problems presented above. We can do that with `jax.custom_vjp`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAZk1n3dUw76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from jax import custom_vjp\n",
        "import jax.numpy as np\n",
        "\n",
        "# f :: a -> b\n",
        "@custom_vjp\n",
        "def f(x):\n",
        "  return np.sin(x)\n",
        "\n",
        "# f_fwd :: a -> (b, c)\n",
        "def f_fwd(x):\n",
        "  return f(x), np.cos(x)\n",
        "\n",
        "# f_bwd :: (c, CT b) -> CT a\n",
        "def f_bwd(cos_x, y_bar):\n",
        "  return (cos_x * y_bar,)\n",
        "\n",
        "f.defvjp(f_fwd, f_bwd)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8W-H2S0Ngdr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d0f25cc0-8454-42a4-b13b-10863beeb6ea"
      },
      "source": [
        "from jax import grad\n",
        "\n",
        "print(f(3.))\n",
        "print(grad(f)(3.))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.14112\n",
            "-0.9899925\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLING7qEVGGN",
        "colab_type": "text"
      },
      "source": [
        "In words, we again start with a a primal function `f` that takes inputs of type `a` and produces outputs of type `b`. We associate with it two functions, `f_fwd` and `f_bwd`, which describe how to perform the forward- and backward-passes of reverse-mode autodiff, respectively.\n",
        "\n",
        "The function `f_fwd` describes the forward pass, not only the primal computation but also what values to save for use on the backward pass. Its input signature is just like that of the primal function `f`, in that it takes a primal input of type `a`. But as output it produces a pair, where the first element is the primal output `b` and the second element is any \"residual\" data of type `c` to be stored for use by the backward pass. (This second output is analogous to [PyTorch's `save_for_backward` mechanism](https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html).)\n",
        "\n",
        "The function `f_bwd` describes the backward pass. It takes two inputs, where the first is the residual data of type `c` produced by `f_fwd` and the second is the output cotangents of type `CT b` corresponding to the output of the primal function. It produces an output of type `CT a` representing the cotangents corresponding to the input of the primal function. In particular, the output of `f_bwd` must be a sequence (e.g. a tuple) of length equal to the number of arguments to the primal function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1b5v67Oncfz",
        "colab_type": "text"
      },
      "source": [
        "So multiple arguments work like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhMb64gkngAt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from jax import custom_vjp\n",
        "\n",
        "@custom_vjp\n",
        "def f(x, y):\n",
        "  return np.sin(x) * y\n",
        "\n",
        "def f_fwd(x, y):\n",
        "  return f(x, y), (np.cos(x), np.sin(x), y)\n",
        "\n",
        "def f_bwd(res, g):\n",
        "  cos_x, sin_x, y = res\n",
        "  return (cos_x * g * y, -sin_x * g)\n",
        "\n",
        "f.defvjp(f_fwd, f_bwd)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnRtIhhLnkry",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b949c30c-62b8-4ed5-ea76-6910aca4da69"
      },
      "source": [
        "print(grad(f)(2., 3.))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-1.2484405\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwC26P9kn8qw",
        "colab_type": "text"
      },
      "source": [
        "Calling a `jax.custom_vjp` function with keyword arguments, or writing a `jax.custom_vjp` function definition with default arguments, are both allowed so long as they can be unambiguosly mapped to positional arguments based on the function signature retrieved by the standard library `inspect.signature` mechanism."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfH-ae8bYt6-",
        "colab_type": "text"
      },
      "source": [
        "As with `jax.custom_jvp`, the custom VJP rule comprised by `f_fwd` and `f_bwd` is not invoked if differentiation is not applied. If function is evaluated, or transformed with `jit`, `vmap`, or other non-differentiation transformations, then only `f` is called."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-_Dbqi-N5Ij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@custom_vjp\n",
        "def f(x):\n",
        "  print(\"called f!\")\n",
        "  return np.sin(x)\n",
        "\n",
        "def f_fwd(x):\n",
        "  print(\"called f_fwd!\")\n",
        "  return f(x), np.cos(x)\n",
        "\n",
        "def f_bwd(cos_x, y_bar):\n",
        "  print(\"called f_bwd!\")\n",
        "  return (cos_x * y_bar,)\n",
        "\n",
        "f.defvjp(f_fwd, f_bwd)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0aZ79OmOAR5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0b06b579-ba48-405a-98eb-c54069ac2f59"
      },
      "source": [
        "print(f(3.))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "called f!\n",
            "0.14112\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ToB9BYlm6uN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "8413a918-95b3-40ed-9062-202002550af4"
      },
      "source": [
        "print(grad(f)(3.))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "called f_fwd!\n",
            "called f!\n",
            "called f_bwd!\n",
            "-0.9899925\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1Pn_qCIODcF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e9921a2e-4488-4cf9-bb0e-e2f2cae98806"
      },
      "source": [
        "from jax import vjp\n",
        "\n",
        "y, f_vjp = vjp(f, 3.)\n",
        "print(y)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "called f_fwd!\n",
            "called f!\n",
            "0.14112\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvgQtDHaOHuo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "cbc26c6f-871a-46fc-a0b5-2c855b236f4b"
      },
      "source": [
        "print(f_vjp(1.))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "called f_bwd!\n",
            "(DeviceArray(-0.9899925, dtype=float32),)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFIIpkFcZCNP",
        "colab_type": "text"
      },
      "source": [
        "**Forward-mode autodiff cannot be used on the `jax.custom_vjp` function** and will raise an error:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RGQRbI_OSEX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "2c4a7602-566b-4bb0-a591-5f2568a5427e"
      },
      "source": [
        "from jax import jvp\n",
        "\n",
        "try:\n",
        "  jvp(f, (3.,), (1.,))\n",
        "except TypeError as e:\n",
        "  print('ERROR! {}'.format(e))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "called f_fwd!\n",
            "called f!\n",
            "ERROR! can't apply forward-mode autodiff (jvp) to a custom_vjp function.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u04I9j2dntAU",
        "colab_type": "text"
      },
      "source": [
        "If you want to use both forward- and reverse-mode, use `jax.custom_jvp` instead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YN97y7LEZbWV",
        "colab_type": "text"
      },
      "source": [
        "We can use `jax.custom_vjp` together with `pdb` to insert a debugger trace in the backward pass:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DvRKsHPZk_g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pdb\n",
        "\n",
        "@custom_vjp\n",
        "def debug(x):\n",
        "  return x  # acts like identity\n",
        "\n",
        "def debug_fwd(x):\n",
        "  return x, x\n",
        "\n",
        "def debug_bwd(x, g):\n",
        "  import pdb; pdb.set_trace()\n",
        "  return g\n",
        "\n",
        "debug.defvjp(debug_fwd, debug_bwd)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49GdkP4pZ2IV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def foo(x):\n",
        "  y = x ** 2\n",
        "  y = debug(y)  # insert pdb in corresponding backward pass step\n",
        "  return np.sin(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGLnRcPwaKoX",
        "colab_type": "text"
      },
      "source": [
        "```python\n",
        "jax.grad(foo)(3.)\n",
        "\n",
        "> <ipython-input-113-b19a2dc1abf7>(12)debug_bwd()\n",
        "-> return g\n",
        "(Pdb) p x\n",
        "DeviceArray(9., dtype=float32)\n",
        "(Pdb) p g\n",
        "DeviceArray(-0.91113025, dtype=float32)\n",
        "(Pdb) q\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaTfAJLAl1Lb",
        "colab_type": "text"
      },
      "source": [
        "## More features and details\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQF_UDApl_UV",
        "colab_type": "text"
      },
      "source": [
        "### Working with `list` / `tuple` / `dict` containers (and other pytrees)\n",
        "\n",
        "You should expect standard Python containers like lists, tuples, namedtuples, and dicts to just work, along with nested versions of those. In general, any [pytrees](https://github.com/google/jax/blob/master/docs/notebooks/JAX_pytrees.ipynb) are permissible, so long as their structures are consistent according to the type constraints. \n",
        "\n",
        "Here's a contrived example with `jax.custom_jvp`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sDLZ3dAn3P2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import namedtuple\n",
        "Point = namedtuple(\"Point\", [\"x\", \"y\"])\n",
        "\n",
        "@custom_jvp\n",
        "def f(pt):\n",
        "  x, y = pt.x, pt.y\n",
        "  return {'a': x ** 2,\n",
        "          'b': (np.sin(x), np.cos(y))}\n",
        "\n",
        "@f.defjvp\n",
        "def f_jvp(primals, tangents):\n",
        "  pt, = primals\n",
        "  pt_dot, =  tangents\n",
        "  ans = f(pt)\n",
        "  ans_dot = {'a': 2 * pt.x * pt_dot.x,\n",
        "             'b': (np.cos(pt.x) * pt_dot.x, -np.sin(pt.y) * pt_dot.y)}\n",
        "  return ans, ans_dot\n",
        "\n",
        "def fun(pt):\n",
        "  dct = f(pt)\n",
        "  return dct['a'] + dct['b'][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "My8pbOlPppJj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2ef4c080-7ecb-425f-dbf9-8e2807529365"
      },
      "source": [
        "pt = Point(1., 2.)\n",
        "\n",
        "print(f(pt))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'a': 1.0, 'b': (DeviceArray(0.841471, dtype=float32), DeviceArray(-0.4161468, dtype=float32))}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9qyiCAhqLd3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "36dfdb67-f5ca-4833-e37e-4e2488b96c0d"
      },
      "source": [
        "print(grad(fun)(pt))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Point(x=DeviceArray(2.5403023, dtype=float32), y=array(0.))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWLN9tu4qWQd",
        "colab_type": "text"
      },
      "source": [
        "And an analogous contrived example with `jax.custom_vjp`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkdbwGkJqS3J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@custom_vjp\n",
        "def f(pt):\n",
        "  x, y = pt.x, pt.y\n",
        "  return {'a': x ** 2,\n",
        "          'b': (np.sin(x), np.cos(y))}\n",
        "\n",
        "def f_fwd(pt):\n",
        "  return f(pt), pt\n",
        "\n",
        "def f_bwd(pt, g):\n",
        "  a_bar, (b0_bar, b1_bar) = g['a'], g['b']\n",
        "  x_bar = 2 * pt.x * a_bar + np.cos(pt.x) * b0_bar\n",
        "  y_bar = -np.sin(pt.y) * b1_bar\n",
        "  return (Point(x_bar, y_bar),)\n",
        "\n",
        "f.defvjp(f_fwd, f_bwd)\n",
        "\n",
        "def fun(pt):\n",
        "  dct = f(pt)\n",
        "  return dct['a'] + dct['b'][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3onW7t6nrJ4E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ad13ae57-72e4-4b90-ac25-d4d6aa0f8161"
      },
      "source": [
        "pt = Point(1., 2.)\n",
        "\n",
        "print(f(pt))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'a': 1.0, 'b': (DeviceArray(0.841471, dtype=float32), DeviceArray(-0.4161468, dtype=float32))}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryyeKIXtrNpd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4ce36200-6e27-4ba5-912a-8e86e7500bd0"
      },
      "source": [
        "print(grad(fun)(pt))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Point(x=DeviceArray(2.5403023, dtype=float32), y=DeviceArray(-0., dtype=float32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKTNivxbmKWO",
        "colab_type": "text"
      },
      "source": [
        "### Handling  non-differentiable arguments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7g9sXSp_uc36",
        "colab_type": "text"
      },
      "source": [
        "Some use cases, like the final example problem, call for non-differentiable arguments to be passed to functions with custom differentiation rules, and for those arguments to also be passed to the rules themselves. In the case of `fixed_point`, the function argument `f` was such a non-differentiable argument. A similar situation arises with `jax.experimental.odeint`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yNIOzyBCvE5",
        "colab_type": "text"
      },
      "source": [
        "#### `jax.custom_jvp` with `nondiff_argnums`\n",
        "\n",
        "Use the optional `nondiff_argnums` parameter to `jax.custom_jvp` to indicate arguments like these. Here's an example with `jax.custom_jvp`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3YMxxTBvy0I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from functools import partial\n",
        "\n",
        "@partial(custom_jvp, nondiff_argnums=(0,))\n",
        "def app(f, x):\n",
        "  return f(x)\n",
        "\n",
        "@app.defjvp\n",
        "def app_jvp(f, primals, tangents):\n",
        "  x, = primals\n",
        "  x_dot, = tangents\n",
        "  return f(x), 2. * x_dot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5W-yEw9IB34S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d6bea705-49c9-4821-e1d5-f0a6e431130f"
      },
      "source": [
        "print(app(lambda x: x ** 3, 3.))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "27.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbVIlOmqB7_O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d690ebab-e3b9-4836-adf8-2be0a743a5d6"
      },
      "source": [
        "print(grad(app, 1)(lambda x: x ** 3, 3.))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-b_B_4WaBI2D",
        "colab_type": "text"
      },
      "source": [
        "Notice the gotcha here: no matter where in the argument list these parameters appear, they're placed at the *start* of the signature of the corresponding JVP rule. Here's another example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hokWmyHBgKK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@partial(custom_jvp, nondiff_argnums=(0, 2))\n",
        "def app2(f, x, g):\n",
        "  return f(g((x)))\n",
        "\n",
        "@app2.defjvp\n",
        "def app2_jvp(f, g, primals, tangents):\n",
        "  x, = primals\n",
        "  x_dot, = tangents\n",
        "  return f(g(x)), 3. * x_dot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7GsvJTgCfS0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "76b6754c-9465-493a-b191-492915c0614c"
      },
      "source": [
        "print(app2(lambda x: x ** 3, 3., lambda y: 5 * y))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3375.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPP8Jt1CCb1X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "152e181f-6ede-4fc6-9dbe-10bdc6483d64"
      },
      "source": [
        "print(grad(app2, 1)(lambda x: x ** 3, 3., lambda y: 5 * y))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECbalHIkC4ts",
        "colab_type": "text"
      },
      "source": [
        "#### `jax.custom_vjp` with `nondiff_argnums`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0u0jn4aWC8k1",
        "colab_type": "text"
      },
      "source": [
        "A similar option exists for `jax.custom_vjp`, and similarly the convention is that the non-differentiable arguments are passed as the first arguments to the rules, no matter where they appear in the original function's signature. Here's an example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCdu-_9GClWs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@partial(custom_vjp, nondiff_argnums=(0,))\n",
        "def app(f, x):\n",
        "  return f(x)\n",
        "\n",
        "def app_fwd(f, x):\n",
        "  return f(x), x\n",
        "\n",
        "def app_bwd(f, x, g):\n",
        "  return (5 * g,)\n",
        "\n",
        "app.defvjp(app_fwd, app_bwd)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSgcWa1eDj4r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "28289e33-a96e-4f2d-e8c3-479232dfb36d"
      },
      "source": [
        "print(app(lambda x: x ** 2, 4.))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tccagflcDmaz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "12eae540-65a9-4c17-ea82-e0965fae924e"
      },
      "source": [
        "print(grad(app, 1)(lambda x: x ** 2, 4.))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTEnNTk5D0sM",
        "colab_type": "text"
      },
      "source": [
        "See `fixed_point` above for another usage example."
      ]
    }
  ]
}
