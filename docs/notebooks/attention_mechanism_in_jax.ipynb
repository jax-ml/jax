{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "intro_header"
            },
            "source": [
                "# Understanding Attention Mechanism in JAX\n",
                "\n",
                "<!--* freshness: { reviewed: '2024-12-08' } *-->\n",
                "\n",
                "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jax-ml/jax/blob/main/docs/notebooks/attention_mechanism_in_jax.ipynb) [![Open in Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/jax-ml/jax/blob/main/docs/notebooks/attention_mechanism_in_jax.ipynb)\n",
                "\n",
                "**Copyright 2024 The JAX Authors.**\n",
                "\n",
                "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License.\n",
                "You may obtain a copy of the License at\n",
                "\n",
                "https://www.apache.org/licenses/LICENSE-2.0\n",
                "\n",
                "Unless required by applicable law or agreed to in writing, software\n",
                "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
                "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
                "See the License for the specific language governing permissions and\n",
                "limitations under the License."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "intro_content"
            },
            "source": [
                "![JAX](https://raw.githubusercontent.com/jax-ml/jax/main/images/jax_logo_250px.png)\n",
                "\n",
                "This tutorial provides a comprehensive guide to implementing the **Attention Mechanism** from scratch using JAX. The attention mechanism is the core building block of modern deep learning architectures like Transformers, which power models such as GPT, BERT, and LLaMA.\n",
                "\n",
                "By the end of this tutorial, you will understand:\n",
                "- The mathematical foundation of attention\n",
                "- How to implement Scaled Dot-Product Attention\n",
                "- How to build Multi-Head Attention using `jax.vmap`\n",
                "- How to create Positional Encodings\n",
                "- How to combine everything into a simple Transformer block\n",
                "\n",
                "**Prerequisites:**\n",
                "- Basic understanding of JAX (`jit`, `grad`, `vmap`)\n",
                "- Familiarity with neural networks and linear algebra\n",
                "- Python and NumPy experience"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "setup_section"
            },
            "source": [
                "## Setup\n",
                "\n",
                "Let's start by importing the necessary libraries."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "imports"
            },
            "outputs": [],
            "source": [
                "import jax\n",
                "import jax.numpy as jnp\n",
                "from jax import random, grad, jit, vmap\n",
                "import numpy as np\n",
                "\n",
                "# For visualization\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Check JAX version\n",
                "print(f\"JAX version: {jax.__version__}\")\n",
                "print(f\"Available devices: {jax.devices()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "attention_theory"
            },
            "source": [
                "## Part 1: What is Attention?\n",
                "\n",
                "### The Intuition\n",
                "\n",
                "Imagine you're reading a sentence: *\"The cat sat on the mat because it was tired.\"*\n",
                "\n",
                "When you process the word \"it\", your brain automatically focuses on \"cat\" to understand what \"it\" refers to. This selective focus is what the **attention mechanism** models computationally.\n",
                "\n",
                "### The Key Concepts: Query, Key, Value\n",
                "\n",
                "The attention mechanism works with three vectors:\n",
                "\n",
                "- **Query (Q)**: \"What am I looking for?\"\n",
                "- **Key (K)**: \"What do I contain?\"\n",
                "- **Value (V)**: \"What information do I provide?\"\n",
                "\n",
                "The attention score between a query and a key determines how much the value should contribute to the output.\n",
                "\n",
                "### Scaled Dot-Product Attention\n",
                "\n",
                "The formula for scaled dot-product attention is:\n",
                "\n",
                "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
                "\n",
                "Where:\n",
                "- $Q$ is the query matrix of shape `(seq_len, d_k)`\n",
                "- $K$ is the key matrix of shape `(seq_len, d_k)`\n",
                "- $V$ is the value matrix of shape `(seq_len, d_v)`\n",
                "- $d_k$ is the dimension of keys (used for scaling)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "scaled_attention_header"
            },
            "source": [
                "## Part 2: Implementing Scaled Dot-Product Attention\n",
                "\n",
                "Let's implement the attention mechanism step by step."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "attention_implementation"
            },
            "outputs": [],
            "source": [
                "def scaled_dot_product_attention(query, key, value, mask=None):\n",
                "    \"\"\"\n",
                "    Compute scaled dot-product attention.\n",
                "    \n",
                "    Args:\n",
                "        query: Array of shape (..., seq_len_q, d_k)\n",
                "        key: Array of shape (..., seq_len_k, d_k)\n",
                "        value: Array of shape (..., seq_len_k, d_v)\n",
                "        mask: Optional boolean mask of shape (..., seq_len_q, seq_len_k)\n",
                "    \n",
                "    Returns:\n",
                "        output: Weighted sum of values, shape (..., seq_len_q, d_v)\n",
                "        attention_weights: Attention weights, shape (..., seq_len_q, seq_len_k)\n",
                "    \"\"\"\n",
                "    # Get the dimension of keys for scaling\n",
                "    d_k = query.shape[-1]\n",
                "    \n",
                "    # Step 1: Compute attention scores (QK^T)\n",
                "    # query: (..., seq_len_q, d_k)\n",
                "    # key.T: (..., d_k, seq_len_k)\n",
                "    # scores: (..., seq_len_q, seq_len_k)\n",
                "    scores = jnp.matmul(query, jnp.swapaxes(key, -2, -1))\n",
                "    \n",
                "    # Step 2: Scale by sqrt(d_k) to prevent softmax saturation\n",
                "    scaled_scores = scores / jnp.sqrt(d_k)\n",
                "    \n",
                "    # Step 3: Apply mask if provided (for decoder self-attention)\n",
                "    if mask is not None:\n",
                "        # Replace masked positions with very negative values\n",
                "        scaled_scores = jnp.where(mask, scaled_scores, -1e9)\n",
                "    \n",
                "    # Step 4: Apply softmax to get attention weights\n",
                "    attention_weights = jax.nn.softmax(scaled_scores, axis=-1)\n",
                "    \n",
                "    # Step 5: Multiply weights by values\n",
                "    output = jnp.matmul(attention_weights, value)\n",
                "    \n",
                "    return output, attention_weights"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "test_attention_header"
            },
            "source": [
                "### Testing Scaled Dot-Product Attention\n",
                "\n",
                "Let's create some sample data and test our implementation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "test_attention"
            },
            "outputs": [],
            "source": [
                "# Initialize random key\n",
                "key = random.key(42)\n",
                "\n",
                "# Create sample Q, K, V matrices\n",
                "seq_len = 4\n",
                "d_k = 8  # dimension of keys\n",
                "d_v = 8  # dimension of values\n",
                "\n",
                "# Split key for random generation\n",
                "key, q_key, k_key, v_key = random.split(key, 4)\n",
                "\n",
                "# Random Q, K, V matrices\n",
                "Q = random.normal(q_key, (seq_len, d_k))\n",
                "K = random.normal(k_key, (seq_len, d_k))\n",
                "V = random.normal(v_key, (seq_len, d_v))\n",
                "\n",
                "print(f\"Query shape: {Q.shape}\")\n",
                "print(f\"Key shape: {K.shape}\")\n",
                "print(f\"Value shape: {V.shape}\")\n",
                "\n",
                "# Compute attention\n",
                "output, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
                "\n",
                "print(f\"\\nOutput shape: {output.shape}\")\n",
                "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
                "\n",
                "# Verify attention weights sum to 1\n",
                "print(f\"\\nAttention weights sum (per query): {attention_weights.sum(axis=-1)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "visualize_attention_header"
            },
            "source": [
                "### Visualizing Attention Weights\n",
                "\n",
                "Attention weights show which positions the model \"attends to\" for each query."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "visualize_attention"
            },
            "outputs": [],
            "source": [
                "def plot_attention_weights(attention_weights, title=\"Attention Weights\"):\n",
                "    \"\"\"\n",
                "    Visualize attention weights as a heatmap.\n",
                "    \"\"\"\n",
                "    fig, ax = plt.subplots(figsize=(6, 5))\n",
                "    im = ax.imshow(attention_weights, cmap='viridis', aspect='auto')\n",
                "    \n",
                "    ax.set_xlabel('Key Position')\n",
                "    ax.set_ylabel('Query Position')\n",
                "    ax.set_title(title)\n",
                "    \n",
                "    # Add colorbar\n",
                "    plt.colorbar(im, ax=ax)\n",
                "    \n",
                "    # Add text annotations\n",
                "    for i in range(attention_weights.shape[0]):\n",
                "        for j in range(attention_weights.shape[1]):\n",
                "            text = ax.text(j, i, f'{attention_weights[i, j]:.2f}',\n",
                "                          ha='center', va='center', color='white', fontsize=9)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "# Visualize our attention weights\n",
                "plot_attention_weights(np.array(attention_weights), \"Scaled Dot-Product Attention\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "multihead_header"
            },
            "source": [
                "## Part 3: Multi-Head Attention\n",
                "\n",
                "### Why Multiple Heads?\n",
                "\n",
                "A single attention head can only focus on one type of relationship. **Multi-Head Attention** runs multiple attention operations in parallel, allowing the model to:\n",
                "\n",
                "1. Attend to different positions\n",
                "2. Learn different types of relationships\n",
                "3. Capture both local and global patterns\n",
                "\n",
                "The formula is:\n",
                "\n",
                "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n",
                "\n",
                "Where:\n",
                "$$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "multihead_implementation"
            },
            "outputs": [],
            "source": [
                "def init_multihead_attention_params(key, d_model, num_heads):\n",
                "    \"\"\"\n",
                "    Initialize parameters for Multi-Head Attention.\n",
                "    \n",
                "    Args:\n",
                "        key: JAX random key\n",
                "        d_model: Model dimension\n",
                "        num_heads: Number of attention heads\n",
                "    \n",
                "    Returns:\n",
                "        Dictionary of parameters\n",
                "    \"\"\"\n",
                "    assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
                "    d_k = d_model // num_heads\n",
                "    \n",
                "    # Split key for each weight matrix\n",
                "    keys = random.split(key, 4)\n",
                "    \n",
                "    # Initialize weight matrices with Xavier/Glorot initialization\n",
                "    scale = jnp.sqrt(2.0 / (d_model + d_k))\n",
                "    \n",
                "    params = {\n",
                "        'W_q': random.normal(keys[0], (d_model, d_model)) * scale,\n",
                "        'W_k': random.normal(keys[1], (d_model, d_model)) * scale,\n",
                "        'W_v': random.normal(keys[2], (d_model, d_model)) * scale,\n",
                "        'W_o': random.normal(keys[3], (d_model, d_model)) * scale,\n",
                "    }\n",
                "    \n",
                "    return params\n",
                "\n",
                "\n",
                "def multihead_attention(params, query, key, value, num_heads, mask=None):\n",
                "    \"\"\"\n",
                "    Compute Multi-Head Attention.\n",
                "    \n",
                "    Args:\n",
                "        params: Dictionary containing W_q, W_k, W_v, W_o\n",
                "        query: Input query of shape (batch_size, seq_len, d_model)\n",
                "        key: Input key of shape (batch_size, seq_len, d_model)\n",
                "        value: Input value of shape (batch_size, seq_len, d_model)\n",
                "        num_heads: Number of attention heads\n",
                "        mask: Optional attention mask\n",
                "    \n",
                "    Returns:\n",
                "        output: Multi-head attention output\n",
                "        attention_weights: Attention weights from all heads\n",
                "    \"\"\"\n",
                "    batch_size = query.shape[0]\n",
                "    seq_len = query.shape[1]\n",
                "    d_model = query.shape[2]\n",
                "    d_k = d_model // num_heads\n",
                "    \n",
                "    # Step 1: Linear projections\n",
                "    Q = jnp.matmul(query, params['W_q'])  # (batch, seq_len, d_model)\n",
                "    K = jnp.matmul(key, params['W_k'])\n",
                "    V = jnp.matmul(value, params['W_v'])\n",
                "    \n",
                "    # Step 2: Reshape for multi-head attention\n",
                "    # (batch, seq_len, d_model) -> (batch, num_heads, seq_len, d_k)\n",
                "    Q = Q.reshape(batch_size, seq_len, num_heads, d_k).transpose(0, 2, 1, 3)\n",
                "    K = K.reshape(batch_size, seq_len, num_heads, d_k).transpose(0, 2, 1, 3)\n",
                "    V = V.reshape(batch_size, seq_len, num_heads, d_k).transpose(0, 2, 1, 3)\n",
                "    \n",
                "    # Step 3: Apply attention to all heads in parallel\n",
                "    # Q, K, V are now (batch, num_heads, seq_len, d_k)\n",
                "    attention_output, attention_weights = scaled_dot_product_attention(Q, K, V, mask)\n",
                "    \n",
                "    # Step 4: Concatenate heads\n",
                "    # (batch, num_heads, seq_len, d_k) -> (batch, seq_len, d_model)\n",
                "    attention_output = attention_output.transpose(0, 2, 1, 3).reshape(\n",
                "        batch_size, seq_len, d_model\n",
                "    )\n",
                "    \n",
                "    # Step 5: Final linear projection\n",
                "    output = jnp.matmul(attention_output, params['W_o'])\n",
                "    \n",
                "    return output, attention_weights"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "test_multihead"
            },
            "outputs": [],
            "source": [
                "# Test Multi-Head Attention\n",
                "batch_size = 2\n",
                "seq_len = 6\n",
                "d_model = 64\n",
                "num_heads = 8\n",
                "\n",
                "# Initialize parameters\n",
                "key = random.key(0)\n",
                "key, params_key, input_key = random.split(key, 3)\n",
                "\n",
                "mha_params = init_multihead_attention_params(params_key, d_model, num_heads)\n",
                "\n",
                "# Create random input\n",
                "x = random.normal(input_key, (batch_size, seq_len, d_model))\n",
                "\n",
                "# Apply multi-head attention (self-attention: Q=K=V)\n",
                "output, attn_weights = multihead_attention(mha_params, x, x, x, num_heads)\n",
                "\n",
                "print(f\"Input shape: {x.shape}\")\n",
                "print(f\"Output shape: {output.shape}\")\n",
                "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
                "print(f\"  -> (batch_size, num_heads, seq_len_q, seq_len_k)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "positional_header"
            },
            "source": [
                "## Part 4: Positional Encoding\n",
                "\n",
                "### Why Positional Encoding?\n",
                "\n",
                "Unlike RNNs, attention is **position-invariant** â€” it treats the sequence as a set, not an ordered list. To incorporate position information, we add **positional encodings** to the input embeddings.\n",
                "\n",
                "### Sinusoidal Positional Encoding\n",
                "\n",
                "The original Transformer uses sinusoidal functions:\n",
                "\n",
                "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
                "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
                "\n",
                "This allows the model to learn to attend to relative positions because for any fixed offset $k$, $PE_{pos+k}$ can be represented as a linear function of $PE_{pos}$."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "positional_encoding"
            },
            "outputs": [],
            "source": [
                "def sinusoidal_positional_encoding(seq_len, d_model):\n",
                "    \"\"\"\n",
                "    Create sinusoidal positional encoding.\n",
                "    \n",
                "    Args:\n",
                "        seq_len: Length of the sequence\n",
                "        d_model: Dimension of the model\n",
                "    \n",
                "    Returns:\n",
                "        Positional encoding matrix of shape (seq_len, d_model)\n",
                "    \"\"\"\n",
                "    # Create position indices: (seq_len, 1)\n",
                "    position = jnp.arange(seq_len)[:, jnp.newaxis]\n",
                "    \n",
                "    # Create dimension indices: (d_model/2,)\n",
                "    div_term = jnp.exp(\n",
                "        jnp.arange(0, d_model, 2) * (-jnp.log(10000.0) / d_model)\n",
                "    )\n",
                "    \n",
                "    # Compute sin and cos\n",
                "    pe = jnp.zeros((seq_len, d_model))\n",
                "    pe = pe.at[:, 0::2].set(jnp.sin(position * div_term))\n",
                "    pe = pe.at[:, 1::2].set(jnp.cos(position * div_term))\n",
                "    \n",
                "    return pe\n",
                "\n",
                "# Create and visualize positional encoding\n",
                "seq_len = 50\n",
                "d_model = 64\n",
                "pe = sinusoidal_positional_encoding(seq_len, d_model)\n",
                "\n",
                "print(f\"Positional encoding shape: {pe.shape}\")\n",
                "\n",
                "# Visualize\n",
                "plt.figure(figsize=(12, 4))\n",
                "plt.imshow(np.array(pe).T, cmap='RdBu', aspect='auto')\n",
                "plt.xlabel('Position')\n",
                "plt.ylabel('Dimension')\n",
                "plt.title('Sinusoidal Positional Encoding')\n",
                "plt.colorbar()\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "transformer_block_header"
            },
            "source": [
                "## Part 5: Transformer Block\n",
                "\n",
                "Now let's combine everything into a complete **Transformer Encoder Block**.\n",
                "\n",
                "A Transformer block consists of:\n",
                "1. Multi-Head Self-Attention\n",
                "2. Add & Normalize (Residual connection + Layer Normalization)\n",
                "3. Feed-Forward Network (FFN)\n",
                "4. Add & Normalize\n",
                "\n",
                "### Layer Normalization\n",
                "\n",
                "Layer normalization normalizes across the feature dimension:\n",
                "\n",
                "$$\\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "layer_norm"
            },
            "outputs": [],
            "source": [
                "def layer_norm(x, gamma, beta, eps=1e-6):\n",
                "    \"\"\"\n",
                "    Apply Layer Normalization.\n",
                "    \n",
                "    Args:\n",
                "        x: Input tensor of shape (..., d_model)\n",
                "        gamma: Scale parameter of shape (d_model,)\n",
                "        beta: Shift parameter of shape (d_model,)\n",
                "        eps: Small constant for numerical stability\n",
                "    \n",
                "    Returns:\n",
                "        Normalized tensor of same shape as x\n",
                "    \"\"\"\n",
                "    mean = jnp.mean(x, axis=-1, keepdims=True)\n",
                "    variance = jnp.var(x, axis=-1, keepdims=True)\n",
                "    normalized = (x - mean) / jnp.sqrt(variance + eps)\n",
                "    return gamma * normalized + beta\n",
                "\n",
                "\n",
                "def feed_forward_network(x, params):\n",
                "    \"\"\"\n",
                "    Feed-Forward Network with ReLU activation.\n",
                "    \n",
                "    FFN(x) = max(0, xW1 + b1)W2 + b2\n",
                "    \n",
                "    Args:\n",
                "        x: Input tensor of shape (..., d_model)\n",
                "        params: Dictionary with W1, b1, W2, b2\n",
                "    \n",
                "    Returns:\n",
                "        Output tensor of shape (..., d_model)\n",
                "    \"\"\"\n",
                "    # First linear layer + ReLU\n",
                "    hidden = jax.nn.relu(jnp.matmul(x, params['W1']) + params['b1'])\n",
                "    # Second linear layer\n",
                "    output = jnp.matmul(hidden, params['W2']) + params['b2']\n",
                "    return output"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "transformer_block"
            },
            "outputs": [],
            "source": [
                "def init_transformer_block_params(key, d_model, num_heads, d_ff):\n",
                "    \"\"\"\n",
                "    Initialize all parameters for a Transformer block.\n",
                "    \n",
                "    Args:\n",
                "        key: JAX random key\n",
                "        d_model: Model dimension\n",
                "        num_heads: Number of attention heads\n",
                "        d_ff: Dimension of feed-forward hidden layer\n",
                "    \n",
                "    Returns:\n",
                "        Dictionary of all parameters\n",
                "    \"\"\"\n",
                "    keys = random.split(key, 6)\n",
                "    \n",
                "    # Multi-head attention parameters\n",
                "    mha_params = init_multihead_attention_params(keys[0], d_model, num_heads)\n",
                "    \n",
                "    # Layer norm parameters\n",
                "    ln1_gamma = jnp.ones(d_model)\n",
                "    ln1_beta = jnp.zeros(d_model)\n",
                "    ln2_gamma = jnp.ones(d_model)\n",
                "    ln2_beta = jnp.zeros(d_model)\n",
                "    \n",
                "    # Feed-forward parameters\n",
                "    scale1 = jnp.sqrt(2.0 / (d_model + d_ff))\n",
                "    scale2 = jnp.sqrt(2.0 / (d_ff + d_model))\n",
                "    \n",
                "    ffn_params = {\n",
                "        'W1': random.normal(keys[1], (d_model, d_ff)) * scale1,\n",
                "        'b1': jnp.zeros(d_ff),\n",
                "        'W2': random.normal(keys[2], (d_ff, d_model)) * scale2,\n",
                "        'b2': jnp.zeros(d_model),\n",
                "    }\n",
                "    \n",
                "    return {\n",
                "        'mha': mha_params,\n",
                "        'ln1_gamma': ln1_gamma,\n",
                "        'ln1_beta': ln1_beta,\n",
                "        'ln2_gamma': ln2_gamma,\n",
                "        'ln2_beta': ln2_beta,\n",
                "        'ffn': ffn_params,\n",
                "    }\n",
                "\n",
                "\n",
                "def transformer_block(params, x, num_heads, mask=None, dropout_key=None, dropout_rate=0.1):\n",
                "    \"\"\"\n",
                "    Apply a single Transformer encoder block.\n",
                "    \n",
                "    Args:\n",
                "        params: Block parameters\n",
                "        x: Input tensor of shape (batch_size, seq_len, d_model)\n",
                "        num_heads: Number of attention heads\n",
                "        mask: Optional attention mask\n",
                "        dropout_key: Optional key for dropout\n",
                "        dropout_rate: Dropout probability\n",
                "    \n",
                "    Returns:\n",
                "        Output tensor of same shape as input\n",
                "    \"\"\"\n",
                "    # 1. Multi-Head Self-Attention\n",
                "    attn_output, attn_weights = multihead_attention(\n",
                "        params['mha'], x, x, x, num_heads, mask\n",
                "    )\n",
                "    \n",
                "    # 2. Add & Normalize (first residual connection)\n",
                "    x = layer_norm(\n",
                "        x + attn_output,\n",
                "        params['ln1_gamma'],\n",
                "        params['ln1_beta']\n",
                "    )\n",
                "    \n",
                "    # 3. Feed-Forward Network\n",
                "    ffn_output = feed_forward_network(x, params['ffn'])\n",
                "    \n",
                "    # 4. Add & Normalize (second residual connection)\n",
                "    output = layer_norm(\n",
                "        x + ffn_output,\n",
                "        params['ln2_gamma'],\n",
                "        params['ln2_beta']\n",
                "    )\n",
                "    \n",
                "    return output, attn_weights"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "test_transformer_block"
            },
            "outputs": [],
            "source": [
                "# Test the complete Transformer block\n",
                "batch_size = 2\n",
                "seq_len = 10\n",
                "d_model = 64\n",
                "num_heads = 8\n",
                "d_ff = 256  # Feed-forward hidden dimension (usually 4x d_model)\n",
                "\n",
                "# Initialize\n",
                "key = random.key(42)\n",
                "key, params_key, input_key = random.split(key, 3)\n",
                "\n",
                "block_params = init_transformer_block_params(params_key, d_model, num_heads, d_ff)\n",
                "\n",
                "# Create input with positional encoding\n",
                "x = random.normal(input_key, (batch_size, seq_len, d_model))\n",
                "pe = sinusoidal_positional_encoding(seq_len, d_model)\n",
                "x = x + pe  # Add positional encoding\n",
                "\n",
                "# Apply transformer block\n",
                "output, attn_weights = transformer_block(block_params, x, num_heads)\n",
                "\n",
                "print(f\"Input shape: {x.shape}\")\n",
                "print(f\"Output shape: {output.shape}\")\n",
                "print(f\"\\nTransformer block maintains the input shape!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "training_header"
            },
            "source": [
                "## Part 6: Training Example - Sequence Classification\n",
                "\n",
                "Let's put it all together and train a simple Transformer for sequence classification.\n",
                "\n",
                "We'll create a toy task: classify whether the sum of a sequence is above or below average."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "complete_model"
            },
            "outputs": [],
            "source": [
                "def init_classifier_params(key, vocab_size, d_model, num_heads, d_ff, num_classes):\n",
                "    \"\"\"\n",
                "    Initialize a simple Transformer classifier.\n",
                "    \"\"\"\n",
                "    keys = random.split(key, 3)\n",
                "    \n",
                "    # Embedding layer\n",
                "    embedding = random.normal(keys[0], (vocab_size, d_model)) * 0.02\n",
                "    \n",
                "    # Transformer block\n",
                "    block_params = init_transformer_block_params(keys[1], d_model, num_heads, d_ff)\n",
                "    \n",
                "    # Classification head\n",
                "    classifier = {\n",
                "        'W': random.normal(keys[2], (d_model, num_classes)) * 0.02,\n",
                "        'b': jnp.zeros(num_classes)\n",
                "    }\n",
                "    \n",
                "    return {\n",
                "        'embedding': embedding,\n",
                "        'transformer': block_params,\n",
                "        'classifier': classifier\n",
                "    }\n",
                "\n",
                "\n",
                "def forward(params, x, num_heads):\n",
                "    \"\"\"\n",
                "    Forward pass of the classifier.\n",
                "    \n",
                "    Args:\n",
                "        params: Model parameters\n",
                "        x: Input token indices of shape (batch_size, seq_len)\n",
                "        num_heads: Number of attention heads\n",
                "    \n",
                "    Returns:\n",
                "        Logits of shape (batch_size, num_classes)\n",
                "    \"\"\"\n",
                "    batch_size, seq_len = x.shape\n",
                "    d_model = params['embedding'].shape[1]\n",
                "    \n",
                "    # Embed tokens\n",
                "    embedded = params['embedding'][x]  # (batch, seq_len, d_model)\n",
                "    \n",
                "    # Add positional encoding\n",
                "    pe = sinusoidal_positional_encoding(seq_len, d_model)\n",
                "    embedded = embedded + pe\n",
                "    \n",
                "    # Apply transformer block\n",
                "    transformed, _ = transformer_block(params['transformer'], embedded, num_heads)\n",
                "    \n",
                "    # Global average pooling\n",
                "    pooled = jnp.mean(transformed, axis=1)  # (batch, d_model)\n",
                "    \n",
                "    # Classify\n",
                "    logits = jnp.matmul(pooled, params['classifier']['W']) + params['classifier']['b']\n",
                "    \n",
                "    return logits"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "training_loop"
            },
            "outputs": [],
            "source": [
                "# Hyperparameters\n",
                "vocab_size = 100\n",
                "d_model = 32\n",
                "num_heads = 4\n",
                "d_ff = 128\n",
                "num_classes = 2\n",
                "seq_len = 8\n",
                "batch_size = 32\n",
                "learning_rate = 0.001\n",
                "num_epochs = 100\n",
                "\n",
                "# Initialize model\n",
                "key = random.key(42)\n",
                "key, init_key = random.split(key)\n",
                "params = init_classifier_params(init_key, vocab_size, d_model, num_heads, d_ff, num_classes)\n",
                "\n",
                "# Create toy dataset: classify if sum of sequence is above/below threshold\n",
                "def generate_batch(key, batch_size, seq_len, vocab_size):\n",
                "    \"\"\"Generate a batch of sequences with labels.\"\"\"\n",
                "    x = random.randint(key, (batch_size, seq_len), 0, vocab_size)\n",
                "    sums = x.sum(axis=1)\n",
                "    threshold = vocab_size * seq_len / 2\n",
                "    labels = (sums > threshold).astype(jnp.int32)\n",
                "    return x, labels\n",
                "\n",
                "# Loss function\n",
                "def cross_entropy_loss(params, x, labels, num_heads):\n",
                "    logits = forward(params, x, num_heads)\n",
                "    one_hot = jax.nn.one_hot(labels, num_classes)\n",
                "    log_probs = jax.nn.log_softmax(logits)\n",
                "    return -jnp.mean(jnp.sum(one_hot * log_probs, axis=-1))\n",
                "\n",
                "# Gradient function\n",
                "@jit\n",
                "def update(params, x, labels, learning_rate):\n",
                "    loss, grads = jax.value_and_grad(cross_entropy_loss)(params, x, labels, num_heads)\n",
                "    # Simple SGD update\n",
                "    params = jax.tree.map(lambda p, g: p - learning_rate * g, params, grads)\n",
                "    return params, loss\n",
                "\n",
                "# Accuracy function\n",
                "@jit\n",
                "def compute_accuracy(params, x, labels):\n",
                "    logits = forward(params, x, num_heads)\n",
                "    predictions = jnp.argmax(logits, axis=-1)\n",
                "    return jnp.mean(predictions == labels)\n",
                "\n",
                "# Training loop\n",
                "print(\"Training Transformer Classifier...\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "losses = []\n",
                "accuracies = []\n",
                "\n",
                "for epoch in range(num_epochs):\n",
                "    key, batch_key = random.split(key)\n",
                "    x_batch, y_batch = generate_batch(batch_key, batch_size, seq_len, vocab_size)\n",
                "    \n",
                "    params, loss = update(params, x_batch, y_batch, learning_rate)\n",
                "    acc = compute_accuracy(params, x_batch, y_batch)\n",
                "    \n",
                "    losses.append(float(loss))\n",
                "    accuracies.append(float(acc))\n",
                "    \n",
                "    if (epoch + 1) % 20 == 0:\n",
                "        print(f\"Epoch {epoch + 1:3d} | Loss: {loss:.4f} | Accuracy: {acc:.4f}\")\n",
                "\n",
                "print(\"=\" * 50)\n",
                "print(\"Training complete!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "plot_training"
            },
            "outputs": [],
            "source": [
                "# Plot training progress\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
                "\n",
                "ax1.plot(losses)\n",
                "ax1.set_xlabel('Epoch')\n",
                "ax1.set_ylabel('Loss')\n",
                "ax1.set_title('Training Loss')\n",
                "ax1.grid(True, alpha=0.3)\n",
                "\n",
                "ax2.plot(accuracies)\n",
                "ax2.set_xlabel('Epoch')\n",
                "ax2.set_ylabel('Accuracy')\n",
                "ax2.set_title('Training Accuracy')\n",
                "ax2.set_ylim([0, 1])\n",
                "ax2.grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "conclusion"
            },
            "source": [
                "## Conclusion\n",
                "\n",
                "In this tutorial, we've learned:\n",
                "\n",
                "1. **Scaled Dot-Product Attention**: The fundamental building block that computes weighted combinations based on query-key similarities.\n",
                "\n",
                "2. **Multi-Head Attention**: Running multiple attention operations in parallel to capture different types of relationships.\n",
                "\n",
                "3. **Positional Encoding**: Adding position information to the otherwise position-invariant attention mechanism.\n",
                "\n",
                "4. **Transformer Block**: Combining attention with feed-forward networks and residual connections.\n",
                "\n",
                "5. **Training**: Putting it all together to train a simple classifier.\n",
                "\n",
                "### Key JAX Features Used\n",
                "\n",
                "- `jax.numpy` for NumPy-like array operations\n",
                "- `jax.nn.softmax` for softmax activation\n",
                "- `jax.random` for reproducible random operations\n",
                "- `jax.value_and_grad` for efficient gradient computation\n",
                "- `jax.jit` for JIT compilation\n",
                "- `jax.tree.map` for applying functions to nested structures\n",
                "\n",
                "### Next Steps\n",
                "\n",
                "- Add decoder with masked self-attention\n",
                "- Implement cross-attention for encoder-decoder architectures\n",
                "- Explore advanced attention variants (Flash Attention, Linear Attention)\n",
                "- Scale up with multiple Transformer layers\n",
                "\n",
                "### Resources\n",
                "\n",
                "- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - The original Transformer paper\n",
                "- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) - Visual guide\n",
                "- [JAX Documentation](https://jax.readthedocs.io/) - Official JAX docs"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "collapsed_sections": [],
            "name": "Understanding Attention Mechanism in JAX",
            "provenance": [],
            "toc_visible": true
        },
        "jupytext": {
            "formats": "ipynb,md:myst"
        },
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}