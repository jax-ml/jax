{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c98ce88",
   "metadata": {},
   "source": [
    "# Introduction to multi-controller JAX (aka multi-process/multi-host JAX)\n",
    "\n",
    "By reading this tutorial, you'll learn how to scale JAX computations to more\n",
    "devices that can fit in a single host machine.\n",
    "\n",
    "The main idea is to run multiple Python processes, which we sometimes call\n",
    "\"controllers.\" We can run one (or more) process per host machine. A `jax.Array`\n",
    "can span all processes, and if each process applies the same JAX function to\n",
    "it, it's like programming against one big device. To control how data is\n",
    "distributed and computation is parallelized, you use the same unified sharding\n",
    "mechanism as in single-process / single-controller JAX. XLA automatically\n",
    "exploits high-speed networking links between hosts, like TPU ICI or NVLink,\n",
    "when available.\n",
    "\n",
    "The big idea:\n",
    "* **Run multiple Python processes**, which we sometimes call \"controllers.\" We can\n",
    "  run one (or more) process per host machine.\n",
    "* **A `jax.Array` can span all processes**, and if each process applies the\n",
    "  same JAX function to it, it's like programming against one big device.\n",
    "* **Use the same unified sharding mechanism as in single-controller JAX** to\n",
    "  control how data is distributed and computation is parallelized. XLA\n",
    "  automatically exploits high-speed networking links between hosts, like TPU ICI\n",
    "  or NVLink, when available.\n",
    "* All processes (usually) run the same Python program. You write this Python\n",
    "  code almost exactly the same as you would for a single process — just run\n",
    "  multiple instances of it and JAX takes care of the rest. In other words, except\n",
    "  for Array creation, you can write your JAX code as if there were one giant\n",
    "  machine with all devices attached to it.\n",
    "\n",
    "This tutorial assumes you've read [Distributed arrays and automatic\n",
    "parallelization](https://docs.jax.dev/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html#way-batch-data-parallelism), which is about single-controller JAX.\n",
    "\n",
    "![Multi-process JAX](../_static/mcjax.png)\n",
    "\n",
    "# Toy example\n",
    "\n",
    "Before we define terms and walk through the details, here's a toy example:\n",
    "making a process-spanning `Array` of values and applying `jax.numpy` functions\n",
    "to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9103bc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call this file toy.py, to be run in each process simultaneously\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.sharding import NamedSharding, PartitionSpec as P\n",
    "\n",
    "# in this example, get multi-process parameters from sys.argv\n",
    "import sys\n",
    "proc_id = int(sys.argv[1])\n",
    "num_procs = int(sys.argv[2])\n",
    "\n",
    "# initialize the distributed system\n",
    "jax.distributed.initialize('localhost:10000', num_procs, proc_id)\n",
    "\n",
    "# make a mesh that refers to devices from all processes\n",
    "num_local_devices = jax.local_device_count()  # same as len(jax.local_devices())\n",
    "mesh = jax.make_mesh((num_procs, num_local_devices), ('i', 'j'))\n",
    "\n",
    "# make process-local arrays, one per local device\n",
    "local_arrays = [\n",
    "    jax.device_put(jnp.array([[proc_id * num_local_devices + i]]), device)\n",
    "    for i, device in enumerate(jax.local_devices())\n",
    "]\n",
    "print(f'process={proc_id} has arrays: {local_arrays}')\n",
    "\n",
    "# make a contoller- and device-spanning array\n",
    "global_array = jax.make_array_from_single_device_arrays(\n",
    "    shape=(num_procs, num_local_devices),\n",
    "    sharding=NamedSharding(mesh, P('i', 'j')),\n",
    "    arrays=local_arrays,\n",
    ")\n",
    "\n",
    "# apply a simple computation, automatically partitioned\n",
    "global_result = jnp.sum(jnp.sin(global_array))\n",
    "print(f'process={proc_id} got result: {global_result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdc3e49",
   "metadata": {},
   "source": [
    "Here, `mesh` contains devices from all processes. Each process has its own array\n",
    "called `local_array`, each with distinct values. In contrast, `global_array` is\n",
    "logically a single shared array, stored distributed across devices from all\n",
    "processes.\n",
    "\n",
    "Every process must apply the same operations, in the same order, to\n",
    "`global_array`. XLA automatically partitions those computations, for example\n",
    "inserting communication collectives to compute the `jnp.sum` over the full\n",
    "array. We can print the final result because its value is replicated across\n",
    "processes.\n",
    "\n",
    "We can run this code locally on CPU, e.g. using 4 processes and 2 CPU devices\n",
    "per process:\n",
    "\n",
    "```bash\n",
    "$ export JAX_NUM_CPU_DEVICES=2\n",
    "$ for i in {0..3}; do\n",
    "  python toy.py $i 4 &\n",
    "done\n",
    "\n",
    "# prints (in some order):\n",
    "process=0 has arrays: [Array([[0]], dtype=int32), Array([[1]], dtype=int32)]\n",
    "process=2 has arrays: [Array([[4]], dtype=int32), Array([[5]], dtype=int32)]\n",
    "process=3 has arrays: [Array([[6]], dtype=int32), Array([[7]], dtype=int32)]\n",
    "process=1 has arrays: [Array([[2]], dtype=int32), Array([[3]], dtype=int32)]\n",
    "process=0 got result: 0.5537326335906982\n",
    "process=3 got result: 0.5537326335906982\n",
    "process=2 got result: 0.5537326335906982\n",
    "process=1 got result: 0.5537326335906982\n",
    "```\n",
    "\n",
    "# Terminology\n",
    "We sometimes call each Python process running JAX computations a _controller_,\n",
    "but the two terms are essentially synonymous. Each process has a set of _local\n",
    "devices_, meaning it can transfer data to and from those devices' memories and\n",
    "run computation on those devices without involving any other processes. We\n",
    "sometimes use the term _addressable_ to mean the same thing as local. A device\n",
    "can only be local to one process; that is, the local device sets are disjoint.\n",
    "\n",
    "![Multi-process JAX controller and its devices](../_static/mcjax_controller.png)\n",
    "\n",
    "A process's local devices can be queried by evaluating `jax.local_devices()`.\n",
    "The list of all devices, across all processes, is queried by `jax.devices()`.\n",
    "That list of all devices is populated by running\n",
    "`jax.distributed.initialize(...)` on all processes, which sets up a simple\n",
    "distributed system connecting the processes.\n",
    "\n",
    "If a `Mesh` only has process-local devices, i.e. those returned by\n",
    "`jax.local_devices()`, we call it a _local mesh_. If it uses all devices across\n",
    "processes, i.e. all those returned by `jax.devices()`, we call it a _global\n",
    "mesh_. In the less common case that a mesh has some devices that are not\n",
    "process-local, but does not include all devices returned by `jax.devices()`, we\n",
    "call it a _process-spanning mesh_.\n",
    "\n",
    "Similarly, when an `Array`'s mesh is local, global, or process-spanning, we call\n",
    "it a _local array_, _global array_, or _process-spanning array_, respectively.\n",
    "\n",
    "# Setting up multiple JAX processes\n",
    "\n",
    "In practice, setting up multiple JAX processes looks a bit different from the\n",
    "toy example. For instance, instead of all the processes running on one host\n",
    "machine, we usually launch them on separate hosts. We can do that directly\n",
    "using `ssh`, or with a cluster manager like Slurm or Kubernetes.\n",
    "\n",
    "However they're launched, the Python processes need to run\n",
    "`jax.distributed.initialize(...)`. On Cloud TPU, or using Slurm or Kubernetes,\n",
    "we can run jax.distributed.initialize() with no arguments as they're\n",
    "automatically populated. Initializing the system means we can run\n",
    "`jax.devices()` to report all devices across all processes.\n",
    "\n",
    "For example, on Cloud TPU, after setting up a `v5litepod-16` (which has 4 host\n",
    "machines) and installing JAX on each host, we might want to test that we can\n",
    "connect the processes and list all devices:\n",
    "\n",
    "```bash\n",
    "$ TPU_NAME=jax-demo\n",
    "$ EXTERNAL_IPS=$(gcloud compute tpus tpu-vm describe $TPU_NAME --zone 'us-central1-a' \\\n",
    "                 | grep externalIp | cut -d: -f2)\n",
    "$ cat << EOF > demo.py\n",
    "import jax\n",
    "jax.distributed.initialize()\n",
    "if jax.process_index() == 0:\n",
    "  print(jax.devices())\n",
    "EOF\n",
    "$ echo $EXTERNAL_IPS | xargs -n 1 -P 0 bash -c '\n",
    "scp demo.py $0:\n",
    "ssh $0 \"python demo.py\" '\n",
    "```\n",
    "\n",
    "Here we're using `xargs` to run multiple ssh commands in parallel, each one\n",
    "running the same Python program on one of the TPU host machines. In the Python\n",
    "code, we use `jax.process_index()` to print only on one process. Here's what it\n",
    "prints:\n",
    "\n",
    "```\n",
    "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=5, process_index=0, coords=(1,1,0), core_on_chip=0), TpuDevice(id=2, process_index=1, coords=(2,0,0), core_on_chip=0), TpuDevice(id=3, process_index=1, coords=(3,0,0), core_on_chip=0), TpuDevice(id=6, process_index=1, coords=(2,1,0), core_on_chip=0), TpuDevice(id=7, process_index=1, coords=(3,1,0), core_on_chip=0), TpuDevice(id=8, process_index=2, coords=(0,2,0), core_on_chip=0), TpuDevice(id=9, process_index=2, coords=(1,2,0), core_on_chip=0), TpuDevice(id=12, process_index=2, coords=(0,3,0), core_on_chip=0), TpuDevice(id=13, process_index=2, coords=(1,3,0), core_on_chip=0), TpuDevice(id=10, process_index=3, coords=(2,2,0), core_on_chip=0), TpuDevice(id=11, process_index=3, coords=(3,2,0), core_on_chip=0), TpuDevice(id=14, process_index=3, coords=(2,3,0), core_on_chip=0), TpuDevice(id=15, process_index=3, coords=(3,3,0), core_on_chip=0)]\n",
    "```\n",
    "\n",
    "Woohoo, look at all those TPU cores!\n",
    "\n",
    "Once the processes are set up, we can start building global `jax.Array`s and\n",
    "running computations. The remaining Python code examples in this tutorial are\n",
    "meant to be run on all processes simultaneously, after running\n",
    "`jax.distributed.initialize(...)`. (A lightweight way to follow along is to edit\n",
    "demo.py locally, then run the same `xargs` command as above to execute it on all\n",
    "host machines in parallel.)\n",
    "\n",
    "# Meshes, `Sharding`s, and computations can span processes and hosts\n",
    "\n",
    "Recall a `Mesh` pairs an array of `jax.Devices` with a sequence of names, with\n",
    "one name per array axis. By creating a `Mesh` using devices from multiple\n",
    "processes, then using that mesh in `Sharding`s, we can construct `Array`s\n",
    "sharded over devices from multiple processes.\n",
    "\n",
    "Here's an example that directly constructs a global `Mesh` using\n",
    "`jax.devices()` to get devices from all processes:\n",
    "\n",
    "```python\n",
    "from jax.sharding import Mesh\n",
    "mesh = Mesh(jax.devices(), ('a',))\n",
    "\n",
    "# in this case, the same as\n",
    "mesh = jax.make_mesh((jax.device_count(),), ('a',))  # use this in practice\n",
    "```\n",
    "\n",
    "You should probably use the `jax.make_mesh` helper in practice, not only because\n",
    "it's simpler but also because it can choose more performant device orderings\n",
    "automatically, but we're spelling it out here. By default it includes all\n",
    "devices across processes, just like `jax.devices()`.\n",
    "\n",
    "Once we have a global mesh, we can shard arrays over it. There are a few ways\n",
    "to efficiently build process-spanning arrays, detailed in the next section, but\n",
    "for now we'll stick to `jax.device_put` for simplicity:\n",
    "\n",
    "```python\n",
    "arr = jax.device_put(jnp.ones((32, 32)), NamedSharding(mesh, P('a')))\n",
    "if jax.process_index() == 0:\n",
    "  jax.debug.visualize_array_sharding(arr)\n",
    "```\n",
    "\n",
    "On process 0, this is printed:\n",
    "\n",
    "```\n",
    "┌───────────────────────┐\n",
    "│           TPU 0       │\n",
    "├───────────────────────┤\n",
    "│           TPU 1       │\n",
    "├───────────────────────┤\n",
    "│           TPU 4       │\n",
    "├───────────────────────┤\n",
    "│           TPU 5       │\n",
    "├───────────────────────┤\n",
    "│           TPU 2       │\n",
    "├───────────────────────┤\n",
    "│           TPU 3       │\n",
    "├───────────────────────┤\n",
    "│           TPU 6       │\n",
    "├───────────────────────┤\n",
    "│           TPU 7       │\n",
    "├───────────────────────┤\n",
    "│           TPU 8       │\n",
    "├───────────────────────┤\n",
    "│           TPU 9       │\n",
    "├───────────────────────┤\n",
    "│          TPU 12       │\n",
    "├───────────────────────┤\n",
    "│          TPU 13       │\n",
    "├───────────────────────┤\n",
    "│          TPU 10       │\n",
    "├───────────────────────┤\n",
    "│          TPU 11       │\n",
    "├───────────────────────┤\n",
    "│          TPU 14       │\n",
    "├───────────────────────┤\n",
    "│          TPU 15       │\n",
    "└───────────────────────┘\n",
    "```\n",
    "\n",
    "Let's try a slightly more interesting computation:\n",
    "\n",
    "```python\n",
    "mesh = jax.make_mesh((jax.device_count() // 2, 2), ('a', 'b'))\n",
    "\n",
    "def device_put(x, spec):\n",
    "  return jax.device_put(x, NamedSharding(mesh, spec))\n",
    "\n",
    "# construct global arrays by sharding over the global mesh\n",
    "x = device_put(jnp.ones((4096, 2048)), P('a', 'b'))\n",
    "y = device_put(jnp.ones((2048, 4096)), P('b', None))\n",
    "\n",
    "# run a distributed matmul\n",
    "z = jax.nn.relu(x @ y)\n",
    "\n",
    "# inspect the sharding of the result\n",
    "if jax.process_index() == 0:\n",
    "  jax.debug.visualize_array_sharding(z)\n",
    "  print()\n",
    "  print(z.sharding)\n",
    "```\n",
    "\n",
    "On process 0, this is printed:\n",
    "\n",
    "\n",
    "```\n",
    "┌───────────────────────┐\n",
    "│          TPU 0,1      │\n",
    "├───────────────────────┤\n",
    "│          TPU 4,5      │\n",
    "├───────────────────────┤\n",
    "│          TPU 8,9      │\n",
    "├───────────────────────┤\n",
    "│         TPU 12,13     │\n",
    "├───────────────────────┤\n",
    "│          TPU 2,3      │\n",
    "├───────────────────────┤\n",
    "│          TPU 6,7      │\n",
    "├───────────────────────┤\n",
    "│         TPU 10,11     │\n",
    "├───────────────────────┤\n",
    "│         TPU 14,15     │\n",
    "└───────────────────────┘\n",
    "\n",
    "NamedSharding(mesh=Mesh('a': 8, 'b': 2), spec=PartitionSpec('a',), memory_kind=device)\n",
    "```\n",
    "\n",
    "Here, just from evaluating `x @ y` on all processes, XLA is automatically\n",
    "generating and running a distributed matrix multiplication. The result is\n",
    "sharded against the mesh like `P('a', None)`, since in this case the matmul\n",
    "included a `psum` over the `'b'` axis.\n",
    "\n",
    "\n",
    "⚠️  When applying JAX computations to process-spanning arrays, to avoid\n",
    "deadlocks and hangs, **it's crucial that all processes with participating\n",
    "devices run the same computation at the same time**, or at least in the right\n",
    "order. That's because the computation may involve collective communication\n",
    "barriers. If a device over which an array is sharded does not join in the\n",
    "collective because its controller didn't issue the same computation, the other\n",
    "devices are left waiting. For example, if only the first three processes\n",
    "evaluated `x @ y`, while the last process evaluated `y @ x`, the computation would\n",
    "likely hang indefinitely. This assumption, computations on process-spanning\n",
    "arrays are run on all participating processes in the same order, is mostly\n",
    "unchecked.\n",
    "\n",
    "So the easiest way to avoid deadlocks in multi-process JAX is to run the same\n",
    "Python code on every process, and beware of any control flow that depends on\n",
    "`jax.process_index()` and includes communication.\n",
    "\n",
    "If a process-spanning array is sharded over devices on different processes, it\n",
    "is an error to perform operations on the array that require the data to be\n",
    "available locally to a process, like printing. For example, if we run `print(z)`\n",
    "in the preceding example, we see\n",
    "\n",
    "```\n",
    "RuntimeError: Fetching value for `jax.Array` that spans non-addressable (non process local) devices is not possible. You can use `jax.experimental.multihost_utils.process_allgather` to print the global array or use `.addressable_shards` method of jax.Array to inspect the addressable (process local) shards.\n",
    "```\n",
    "\n",
    "To print the full array value, we must first ensure it's replicated over\n",
    "processes (but not necessarily over each process's local devices), e.g. using\n",
    "`jax.device_put`. In the above example, we can write at the end:\n",
    "\n",
    "```python\n",
    "w = device_put(z, P(None, None))\n",
    "if jax.process_index() == 0:\n",
    "  print(w)\n",
    "```\n",
    "\n",
    "Be careful not to write the `device_put` under the `if process_index() == 0`,\n",
    "because that would lead to a deadlock as only process 0 initiates the\n",
    "collective communication and waits indefinitely for the other processes.\n",
    "\n",
    "Alternatively, to print or otherwise perform Python operations on only\n",
    "process-local data, we can access `z.addressable_shards`. Accessing that\n",
    "attribute does not require any communication, so any subset of processes can do\n",
    "it without needing the others. That attribute is not available under a\n",
    "`jax.jit`.\n",
    "\n",
    "# Making process-spanning arrays from process-local data\n",
    "\n",
    "There are three main ways to create process-spanning arrays:\n",
    "1. Create or load the full array on all processes, then reshard using\n",
    "   `jax.device_put` or `jax.lax.with_sharding_constraint`, likely resulting in\n",
    "   cross-process and cross-device data movement;\n",
    "2. Create or load on each process an array representing the data that will be\n",
    "   stored on that process's devices, then assemble it without any cross-process\n",
    "   data movement using `jax.make_array_from_process_local_data`, which may\n",
    "   result in local device data movement;\n",
    "3. Create or load on each process's devices separate arrays, each representing\n",
    "   the data to be stored on that device, then assemble them without any data\n",
    "   movement using `jax.make_array_from_single_device_arrays`.\n",
    "\n",
    "The latter two are most often used in practice.\n",
    "\n",
    "\n",
    "`jax.make_array_from_process_local_data` is often used for distributed data\n",
    "loading. It's not as general as `jax.make_array_from_single_device_arrays` and\n",
    "sometimes the result must be resharded among local devices to achieve the final\n",
    "intended sharding. Here's an example, using our `v5litepod-16`:\n",
    "\n",
    "```python\n",
    "# TODO\n",
    "```\n",
    "\n",
    "`jax.make_array_from_single_device_arrays` is the most general way to build a\n",
    "process-spanning array. It's often used after performing `jax.device_puts` to\n",
    "send to each device its required data. It is the most low-level, since all data\n",
    "movement is performed manually (via e.g. `jax.device_put`). Here's an example:\n",
    "\n",
    "```python\n",
    "# TODO\n",
    "```\n",
    "\n",
    "# Toy examples\n",
    "\n",
    "TODO\n",
    "\n",
    "# Serializing distributed `Array`s\n",
    "\n",
    "# Further reading\n",
    "* Fault tolerance\n",
    "* Distributed data loading"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
