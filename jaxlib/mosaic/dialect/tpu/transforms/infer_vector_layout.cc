/* Copyright 2023 The JAX Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/

#include <algorithm>
#include <array>
#include <cstdint>
#include <functional>
#include <memory>
#include <optional>
#include <ostream>
#include <string>
#include <type_traits>
#include <utility>
#include <variant>

#include "llvm/ADT/ArrayRef.h"
#include "llvm/ADT/SmallVector.h"
#include "llvm/Support/raw_ostream.h"
#include "mlir/Dialect/Arith/IR/Arith.h"
#include "mlir/Dialect/ControlFlow/IR/ControlFlowOps.h"
#include "mlir/Dialect/Func/IR/FuncOps.h"
#include "mlir/Dialect/MemRef/IR/MemRef.h"
#include "mlir/Dialect/SCF/IR/SCF.h"
#include "mlir/Dialect/Vector/IR/VectorOps.h"
#include "mlir/IR/AffineExpr.h"
#include "mlir/IR/BuiltinAttributes.h"
#include "mlir/IR/BuiltinTypes.h"
#include "mlir/IR/Value.h"
#include "mlir/Support/LLVM.h"
#include "mlir/Support/LogicalResult.h"
#include "absl/log/check.h"
#include "absl/log/log.h"
#include "mlir/include/mlir/Dialect/Arith/IR/Arith.h"
#include "mlir/include/mlir/Dialect/Vector/IR/VectorOps.h"
#include "jaxlib/mosaic/dialect/tpu/layout.h"
#include "jaxlib/mosaic/dialect/tpu/tpu_dialect.h"
#include "xla/layout.h"

namespace mlir::tpu {

#define GEN_PASS_DECL_INFERVECTORLAYOUTPASS
#define GEN_PASS_DEF_INFERVECTORLAYOUTPASS
#include "jaxlib/mosaic/dialect/tpu/tpu_passes.h.inc"

namespace {

using ImplicitDim = VectorLayout::ImplicitDim;

static constexpr int kLayoutLog = 10;

class Print {
 public:
  explicit Print(Operation* t) : payload_(t) {}
  Operation* payload_;
 private:
  friend std::ostream &operator<<(std::ostream &, Print);
};

std::ostream &operator<<(std::ostream &os, Print p) {
  std::string s;
  llvm::raw_string_ostream tmp_os(s);
  p.payload_->print(tmp_os);
  os << tmp_os.str();
  return os;
}

bool is_fully_replicated(const Layout &layout) {
  static LayoutOffsets replicated_offsets = {std::nullopt, std::nullopt};
  return layout.has_value() && layout->offsets() == replicated_offsets;
}

TiledLayoutAttr getMemRefLayout(Value ref) {
  if (auto erase_op = ref.getDefiningOp<tpu::EraseLayoutOp>()) {
    ref = erase_op.getOperand();
  }
  return cast<TiledLayoutAttr>(cast<MemRefType>(ref.getType()).getLayout());
}

// TODO(apaszke): Test that this pass fills in NoLayout for all operations that
// have corresponding native instructions.
class VectorLayoutInferer {
 public:
  explicit VectorLayoutInferer(std::array<int64_t, 2> target_shape)
      : target_shape_({target_shape[0], target_shape[1]}),
        default_tiling_(target_shape) {}

#define TPU_CHECK_OP(cond, msg) \
  if (!(cond)) {                \
    op->emitOpError(msg);       \
    return failure();           \
  }

#define NYI(msg)                            \
  op->emitOpError("not implemented: " msg); \
  return failure();

  LogicalResult inferBlock(
      Block &block,
      const std::function<LogicalResult(Operation *)> &match_terminator) {
    for (Operation &any_op : block.without_terminator()) {
      VLOG(kLayoutLog) << Print(&any_op);
      if (any_op.hasAttr("in_layout") || any_op.hasAttr("out_layout")) {
        any_op.emitOpError("layout attributes already attached");
        return failure();
      }
      bool has_vector_io = false;
      for (auto op : any_op.getOperands()) {
        has_vector_io |= op.getType().isa<VectorType>();
      }
      for (auto r : any_op.getResults()) {
        has_vector_io |= r.getType().isa<VectorType>();
      }
      if (!has_vector_io && any_op.getRegions().empty()) {
        llvm::SmallVector<Layout, 4> in_layout(any_op.getNumOperands(),
                                               kNoLayout);
        if (any_op.getNumResults() == 0) {
          setInLayout(&any_op, in_layout);
        } else if (any_op.getNumResults() == 1) {
          setLayout(&any_op, in_layout, kNoLayout);
        } else {
          any_op.emitOpError("Multi-result ops not supported");
          return failure();
        }
      } else if (auto op = dyn_cast<arith::ExtFOp>(any_op)) {
        if (infer(op).failed()) {
          return failure();
        }
      } else if (auto op = dyn_cast<arith::TruncFOp>(any_op)) {
        if (infer(op).failed()) {
          return failure();
        }
      } else if (auto op = dyn_cast<arith::ExtSIOp>(any_op)) {
        if (infer(op).failed()) {
          return failure();
        }
      } else if (auto op = dyn_cast<arith::TruncIOp>(any_op)) {
        if (infer(op).failed()) {
          return failure();
        }
      } else if (auto op = dyn_cast<arith::SelectOp>(any_op)) {
        auto true_ty = dyn_cast<VectorType>(op.getTrueValue().getType());
        auto false_ty = dyn_cast<VectorType>(op.getFalseValue().getType());
        TPU_CHECK_OP(static_cast<bool>(true_ty) == static_cast<bool>(false_ty),
                     "Only one side of arith is a vector?");
        if (true_ty) {
          TPU_CHECK_OP(true_ty.getElementTypeBitWidth() == kNativeBitwidth &&
                           false_ty.getElementTypeBitWidth() == kNativeBitwidth,
                       "Only 32-bit select supported");
        }
        if (inferElementwise(&any_op, /*check_bitwidth=*/false).failed()) {
          return failure();
        }
      } else if (auto op = dyn_cast<arith::ExtUIOp>(any_op)) {
        auto in_ty = dyn_cast<VectorType>(op.getIn().getType());
        auto out_ty = dyn_cast<VectorType>(op.getType());
        TPU_CHECK_OP(static_cast<bool>(in_ty) == static_cast<bool>(out_ty),
                     "Input and output are not both vectors?");
        if (in_ty) {
          TPU_CHECK_OP(in_ty.getElementTypeBitWidth() == 1 &&
                           out_ty.getElementTypeBitWidth() == 32,
                       "Only 1 bit -> 32 bit extensison supported");
        }
        if (inferElementwise(&any_op, /*check_bitwidth=*/false).failed()) {
          return failure();
        }
      } else if (isa<arith::CmpIOp>(any_op) || isa<arith::CmpFOp>(any_op)) {
        auto lhs_ty = dyn_cast<VectorType>(any_op.getOperand(0).getType());
        auto rhs_ty = dyn_cast<VectorType>(any_op.getOperand(1).getType());
        TPU_CHECK_OP(static_cast<bool>(lhs_ty) == static_cast<bool>(rhs_ty),
                     "Only one side of cmp is a vector?");
        if (lhs_ty) {
          TPU_CHECK_OP(lhs_ty.getElementTypeBitWidth() == kNativeBitwidth &&
                           rhs_ty.getElementTypeBitWidth() == kNativeBitwidth,
                       "Only 32-bit cmp supported");
        }
        if (inferElementwise(&any_op, /*check_bitwidth=*/false).failed()) {
          return failure();
        }
      } else if (OpTrait::hasElementwiseMappableTraits(&any_op)) {
        if (inferElementwise(&any_op).failed()) {
          return failure();
        }
      } else if (auto op = dyn_cast<arith::ConstantOp>(any_op)) {
        if (infer(op).failed()) {
          return failure();
        }
      } else if (auto op = dyn_cast<cf::AssertOp>(any_op)) {
        if (infer(op).failed()) {
          return failure();
        }
      } else if (auto op = dyn_cast<memref::LoadOp>(any_op)) {
        if (infer(op).failed()) {
          return failure();
        }
      } else if (auto op = dyn_cast<scf::IfOp>(any_op)) {
        if (infer(op).failed()) {
          return failure();
        }
      } else if (auto op = dyn_cast<tpu::ConcatenateOp>(any_op)) {
        if (infer(op).failed()) {
          return failure();
        }
      } else if (auto op = dyn_cast<tpu::LoadOp>(any_op)) {
        if (infer(op).failed()) {
          return failure();
        }
      } else if (auto op = dyn_cast<tpu::MatmulOp>(any_op)) {
        if (infer(op).failed()) {
          return failure();
        }
      } else if (auto op = dyn_cast<tpu::StoreOp>(any_op)) {
        if (infer(op).failed()) {
          return failure();
        }
      } else if (auto op = dyn_cast<tpu::EraseLayoutOp>(any_op)) {
        if (infer(op).failed()) {
          return failure();
        }
      } else if (auto op = dyn_cast<tpu::IotaOp>(any_op)) {
        if (infer(op).failed()) {
          return failure();
        }
      } else if (auto op = dyn_cast<tpu::GatherOp>(any_op)) {
        if (infer(op).failed()) {
          return failure();
        }
      } else if (auto op = dyn_cast<tpu::RepeatOp>(any_op)) {
        if (infer(op).failed()) {
          return failure();
        }
      } else if (auto op = dyn_cast<tpu::TraceOp>(any_op)) {
        if (infer(op).failed()) {
          return failure();
        }
      } else if (auto op = dyn_cast<tpu::RegionOp>(any_op)) {
        if (infer(op).failed()) {
          return failure();
        }
      } else if (auto op = dyn_cast<vector::BroadcastOp>(any_op)) {
        if (infer(op).failed()) {
          return failure();
        }
      } else if (auto op = dyn_cast<vector::ContractionOp>(any_op)) {
        if (infer(op).failed()) {
          return failure();
        }
      } else if (auto op = dyn_cast<vector::LoadOp>(any_op)) {
        if (infer(op).failed()) {
          return failure();
        }
      } else if (auto op = dyn_cast<vector::MultiDimReductionOp>(any_op)) {
        if (infer(op).failed()) {
          return failure();
        }
      } else if (auto op = dyn_cast<vector::ShapeCastOp>(any_op)) {
        if (infer(op).failed()) {
          return failure();
        }
      } else if (auto op = dyn_cast<vector::StoreOp>(any_op)) {
        if (infer(op).failed()) {
          return failure();
        }
      } else if (auto op = dyn_cast<vector::TransposeOp>(any_op)) {
        if (infer(op).failed()) {
          return failure();
        }
      } else if (auto op =
                     llvm::dyn_cast<vector::ExtractStridedSliceOp>(any_op)) {
        if (infer(op).failed()) {
          return failure();
        }
      } else {
        any_op.emitOpError("unsupported in vector layout inference");
        return failure();
      }
      CHECK(any_op.getNumResults() == 0 || any_op.hasAttr("out_layout"));
      CHECK(any_op.getNumOperands() == 0 || any_op.hasAttr("in_layout"));
    }
    return match_terminator(block.getTerminator());
  }

  LogicalResult infer(arith::ConstantOp op) {
    if (op.getType().isSignlessIntOrIndexOrFloat()) {
      setOutLayout(op, kNoLayout);
      return success();
    }
    if (auto ty = dyn_cast<VectorType>(op.getType())) {
      auto elems = dyn_cast<DenseElementsAttr>(op.getValue());
      TPU_CHECK_OP(ty.getElementType().isSignlessIntOrIndexOrFloat(),
                   "expected scalar element type in vector");
      TPU_CHECK_OP(ty.getRank() > 0, "rank 0 vectors unsupported");
      TPU_CHECK_OP(elems, "expected vector constants to use DenseElementsAttr");
      auto bitwidth = ty.getElementTypeBitWidth();
      if (elems.isSplat()) {
        if (ty.getRank() == 1) {
          // Here, we choose to lay out along lanes arbitrarily. It would be
          // equally valid to go with sublanes. Still, this value is so easy
          // to relayout that it shouldn't really make a difference.
          setOutLayout(op, VectorLayout(bitwidth, {std::nullopt, std::nullopt},
                                        nativeTiling(bitwidth),
                                        ImplicitDim::kSecondMinor));
        } else {  // ty.getRank() >= 2
          setOutLayout(
              op, VectorLayout(bitwidth, {std::nullopt, std::nullopt},
                               nativeTiling(bitwidth), ImplicitDim::kNone));
        }
      } else {
        TPU_CHECK_OP(ty.getElementTypeBitWidth() == kNativeBitwidth,
                     "Only 32-bit non-splat constants supported");
        if (ty.getRank() == 1) {
          if (ty.getDimSize(0) <= target_shape_[0]) {
            // Use 2D layout with replication.
            NYI("small 1D constants");
          } else {  // NOLINT(readability-else-after-return)
            NYI("large 1D constants");
          }
        } else {  // ty.getRank() >= 2
          setOutLayout(op, VectorLayout(kNativeBitwidth, {0, 0},
                                        default_tiling_, ImplicitDim::kNone));
        }
      }
      return success();
    }
    op.emitOpError("unsupported constant type");
    return failure();
  }

  LogicalResult infer(arith::ExtFOp op) {
    auto src_ty = dyn_cast<VectorType>(op.getIn().getType());
    if (!src_ty) {
      setLayout(op, kNoLayout, kNoLayout);
      return success();
    }
    auto dst_ty = cast<VectorType>(op.getOut().getType());
    auto some_layout = getLayout(op.getIn());
    TPU_CHECK_OP(src_ty.getElementTypeBitWidth() == 16 &&
                     dst_ty.getElementTypeBitWidth() == 32,
                 "Only 16-bit to 32-bit extensions supported");
    TPU_CHECK_OP(some_layout.has_value(), "missing vector layout");
    auto &layout = *some_layout;
    if (layout.implicit_dim() == ImplicitDim::kNone) {
      // TODO(apaszke): Support at least (16,128) here.
      auto src_layout = VectorLayout(16, layout.offsets(), default_tiling_,
                                     ImplicitDim::kNone);
      auto dst_layout = VectorLayout(32, layout.offsets(), default_tiling_,
                                     ImplicitDim::kNone);
      setLayout(op, src_layout, dst_layout);
      return success();
    }
    if (layout.implicit_dim() == ImplicitDim::kSecondMinor) {
      TPU_CHECK_OP(layout.tiling() == nativeTiling(16), "unsupported tiling");
      auto dst_layout = VectorLayout(32, layout.offsets(), default_tiling_,
                                     layout.implicit_dim());
      setLayout(op, some_layout, dst_layout);
      return success();
    }
    op.emitOpError("unsupported extension layout");
    return failure();
  }

  LogicalResult infer(arith::TruncFOp op) {
    auto src_ty = dyn_cast<VectorType>(op.getIn().getType());
    if (!src_ty) {
      setLayout(op, kNoLayout, kNoLayout);
      return success();
    }
    auto dst_ty = cast<VectorType>(op.getOut().getType());
    auto some_layout = getLayout(op.getIn());
    TPU_CHECK_OP(src_ty.getElementTypeBitWidth() == 32 &&
                     dst_ty.getElementTypeBitWidth() == 16,
                 "Only 32-bit to 16-bit truncation supported");
    auto &layout = *some_layout;
    if (layout.implicit_dim() == ImplicitDim::kNone) {
      bool only_feeds_into_matrix_ops = true;
      for (OpOperand &operand : op->getUses()) {
        if (isa<vector::ContractionOp>(operand.getOwner())) {
          continue;
        }
        if (auto transpose =
                dyn_cast<vector::TransposeOp>(operand.getOwner())) {
          auto perm_attrs = transpose.getTransp().getValue();
          auto rank = perm_attrs.size();
          if (rank >= 2 &&
              cast<IntegerAttr>(perm_attrs[rank - 1]).getInt() == rank - 2 &&
              cast<IntegerAttr>(perm_attrs[rank - 2]).getInt() == rank - 1) {
            continue;
          }
          // Fall through.
        }
        only_feeds_into_matrix_ops = false;
        break;
      }
      auto src_layout = VectorLayout(32, layout.offsets(), default_tiling_,
                                     ImplicitDim::kNone);
      auto dst_layout = VectorLayout(
          16, layout.offsets(),
          only_feeds_into_matrix_ops ? nativeTiling(16) : default_tiling_,
          ImplicitDim::kNone);
      setLayout(op, src_layout, dst_layout);
      return success();
    }
    return failure();
  }

  LogicalResult infer(arith::ExtSIOp op) {
    auto src_ty = dyn_cast<VectorType>(op.getIn().getType());
    if (!src_ty) {
      setLayout(op, kNoLayout, kNoLayout);
      return success();
    }
    auto dst_ty = cast<VectorType>(op.getOut().getType());
    auto some_layout = getLayout(op.getIn());
    TPU_CHECK_OP(dst_ty.getElementTypeBitWidth() == 32,
                 "Only extensions to 32-bit supported");
    TPU_CHECK_OP(some_layout.has_value(), "missing vector layout");
    auto &layout = *some_layout;
    if (layout.implicit_dim() == ImplicitDim::kNone) {
      // TODO(apaszke): Support native layouts here.
      auto src_layout = VectorLayout(layout.bitwidth(), layout.offsets(),
                                     default_tiling_, ImplicitDim::kNone);
      auto dst_layout = VectorLayout(32, layout.offsets(), default_tiling_,
                                     ImplicitDim::kNone);
      setLayout(op, src_layout, dst_layout);
      return success();
    }
    if (layout.implicit_dim() == ImplicitDim::kSecondMinor) {
      TPU_CHECK_OP(layout.tiling() == nativeTiling(16), "unsupported tiling");
      auto dst_layout = VectorLayout(32, layout.offsets(), default_tiling_,
                                     layout.implicit_dim());
      setLayout(op, some_layout, dst_layout);
      return success();
    }
    op.emitOpError("unsupported extension layout");
    return failure();
  }

  LogicalResult infer(arith::TruncIOp op) {
    auto src_ty = dyn_cast<VectorType>(op.getIn().getType());
    if (!src_ty) {
      setLayout(op, kNoLayout, kNoLayout);
      return success();
    }
    auto dst_ty = cast<VectorType>(op.getOut().getType());
    auto some_layout = getLayout(op.getIn());
    TPU_CHECK_OP(src_ty.getElementTypeBitWidth() == 32,
                 "Only 32-bit truncation supported");
    TPU_CHECK_OP(some_layout.has_value(), "missing vector layout");
    auto &layout = *some_layout;
    if (layout.implicit_dim() == ImplicitDim::kNone) {
      // TODO(apaszke): Support native layouts here.
      auto dst_layout =
          VectorLayout(dst_ty.getElementTypeBitWidth(), layout.offsets(),
                       default_tiling_, ImplicitDim::kNone);
      setLayout(op, some_layout, dst_layout);
      return success();
    }
    return failure();
  }

  LogicalResult infer(cf::AssertOp op) {
    setInLayout(op, {kNoLayout});
    return success();
  }

  LogicalResult infer(func::FuncOp op) {
    if (!op.getBody().hasOneBlock()) {
      op.emitOpError("Only one block functions supported");
      return failure();
    }
    return inferBlock(
        op.getBody().front(), [this](Operation *op) -> LogicalResult {
          TPU_CHECK_OP(isa<func::ReturnOp>(op),
                       "Expected func.return terminator");
          for (Value o : op->getOperands()) {
            TPU_CHECK_OP(!isa<VectorType>(o.getType()),
                         "vector returns unsupported");
          }
          SmallVector<Layout, 4> in_layout(op->getNumOperands(), {kNoLayout});
          setInLayout(op, in_layout);
          return success();
        });
  }

  LogicalResult infer(memref::LoadOp op) {
    TPU_CHECK_OP(op.getType().isSignlessIntOrIndexOrFloat(),
                 "memref.load with non-scalar result");
    SmallVector<Layout, 5> in_layout(op.getNumOperands(), {kNoLayout});
    setLayout(op, in_layout, kNoLayout);
    return success();
  }

  LogicalResult infer(scf::IfOp op) {
    static LogicalResult (*match_yield)(Operation *) = [](Operation *op) {
      TPU_CHECK_OP(isa<scf::YieldOp>(op), "expected yield terminator");
      return success();
    };
    TPU_CHECK_OP(op->getNumOperands() == 1, "expected one operand");
    setInLayout(op, {kNoLayout});
    if (inferBlock(*op.thenBlock(), match_yield).failed()) {
      return failure();
    }
    auto then_yield = op.thenBlock()->getTerminator();
    TPU_CHECK_OP(then_yield->getOperandTypes() == op->getResultTypes(),
                 "scf if results and then branch yield operands do not match");
    llvm::SmallVector<Layout, 4> result_layout;
    result_layout.reserve(then_yield->getNumOperands());
    for (const auto &operand : then_yield->getOperands()) {
      if (operand.getType().isSignlessIntOrIndexOrFloat()) {
        result_layout.push_back(kNoLayout);
      } else if (isa<VectorType>(operand.getType())) {
        result_layout.push_back(getLayout(operand));
      } else {
        op.emitOpError("unsupported scf.yield type");
        return failure();
      }
    }

    if (auto else_block = op.elseBlock()) {
      if (inferBlock(*else_block, match_yield).failed()) {
        return failure();
      }
    }
    if (op->getNumResults() == 0) {
      return success();
    }
    // If the if op has results, it should have both then and else regions with
    // yield op.
    auto else_yield = op.elseBlock()->getTerminator();
    TPU_CHECK_OP(else_yield->getOperandTypes() == op->getResultTypes(),
                 "scf if results and else branch yield operands do not match");

    // Use the in_layout of the yield op in the then branch as the in_layout of
    // the yield op in the else branch and the if op.
    setInLayout(then_yield, result_layout);
    setInLayout(else_yield, result_layout);
    setOutLayout(op, result_layout);
    return success();
  }

  LogicalResult infer(tpu::ConcatenateOp op) {
    TPU_CHECK_OP(op.getDimension() - op.getType().getRank() < -2,
                 "Concatenation is not supported along the last two axes");
    TPU_CHECK_OP(!op.getSources().empty(),
                 "Need at least one vector to concatenate");
    // Fix all the layouts to the layout of the first operand.
    // This might not be the best strategy, but it works.
    SmallVector<Layout> in_layouts(op.getNumOperands(),
                                   getLayout(op.getSources().front()));
    setLayout(op, in_layouts, in_layouts.back());
    return success();
  }

  LogicalResult infer(tpu::LoadOp op) {
    auto res_ty = op.getResult().getType();
    int8_t bitwidth = res_ty.getElementTypeBitWidth();

    // We expect the result is already a native-sized vreg.
    TPU_CHECK_OP(bitwidth == 32 && res_ty.getShape()[0] == target_shape_[0] &&
                     res_ty.getShape()[1] == target_shape_[1],
                 "Only 32-bit loads suppored");
    SmallVector<Layout, 4> in_layout(op->getNumOperands(), kNoLayout);
    auto out_layout = VectorLayout(bitwidth, {0, 0}, nativeTiling(bitwidth),
                                   ImplicitDim::kNone);
    setLayout(op, in_layout, out_layout);
    return success();
  }

  LogicalResult infer(tpu::MatmulOp op) {
    return inferMatmul(op);
  }

  LogicalResult infer(tpu::StoreOp op) {
    auto store_ty = op.getValueToStore().getType();
    int8_t bitwidth = store_ty.getElementTypeBitWidth();

    // We expect the value to store is already a native-sized vreg.
    TPU_CHECK_OP(bitwidth == 32 &&
                     store_ty.getShape()[0] == target_shape_[0] &&
                     store_ty.getShape()[1] == target_shape_[1],
                 "Only 32-bit stores suppored");
    auto store_layout = VectorLayout(bitwidth, {0, 0}, nativeTiling(bitwidth),
                                     ImplicitDim::kNone);
    SmallVector<Layout, 5> in_layout{store_layout};
    in_layout.insert(in_layout.end(), op.getIndices().size() + 1, kNoLayout);
    setInLayout(op, in_layout);
    return success();
  }

  LogicalResult infer(tpu::EraseLayoutOp op) {
    setLayout(op, kNoLayout, kNoLayout);
    return success();
  }

  LogicalResult infer(tpu::GatherOp op) {
    auto src_layout = getLayout(op.getSource());
    setLayout(op, src_layout, src_layout);
    return success();
  }

  LogicalResult infer(tpu::RepeatOp op) {
    auto src_layout = getLayout(op.getSource());
    setLayout(op, src_layout, src_layout);
    return success();
  }

  LogicalResult infer(tpu::TraceOp op) {
    static LogicalResult (*match_yield)(Operation *) = [](Operation *op) {
      TPU_CHECK_OP(isa<tpu::YieldOp>(op), "expected yield terminator");
      return success();
    };
    TPU_CHECK_OP(op->getNumOperands() == 0, "expected no operands");
    TPU_CHECK_OP(op->getNumResults() == 0, "results unsupported");
    return inferBlock(*op.getBody(), match_yield);
  }

  LogicalResult infer(tpu::RegionOp op) {
    static LogicalResult (*match_region)(Operation *) = [](Operation *op) {
      TPU_CHECK_OP(isa<tpu::YieldOp>(op), "expected yield terminator");
      return success();
    };
    TPU_CHECK_OP(op->getNumOperands() == 0, "expected no operands");
    TPU_CHECK_OP(op->getNumResults() == 0, "results unsupported");
    return inferBlock((*op).getRegion(0).getBlocks().front(), match_region);
  }

  LogicalResult infer(tpu::IotaOp op) {
    auto ty = op.getResult().getType();
    TPU_CHECK_OP(ty.getElementType().isSignlessInteger(32),
                 "Only 32-bit integer iota supported");
    TPU_CHECK_OP(ty.getRank() >= 2, "iota rank below 2D unsupported");
    LayoutOffsets offsets = {0, 0};
    if (op.getDimension() == ty.getRank() - 1) {
      offsets[0] = std::nullopt;
    }
    if (op.getDimension() == ty.getRank() - 2) {
      offsets[1] = std::nullopt;
    }
    setOutLayout(op, VectorLayout(kNativeBitwidth, offsets, default_tiling_,
                                  ImplicitDim::kNone));
    return success();
  }

  LogicalResult infer(vector::BroadcastOp op) {
    auto some_src_ty = op.getSourceType();
    auto res_ty = op.getResultVectorType();
    TPU_CHECK_OP(res_ty.getRank() > 0, "rank 0 vectors unsupported");
    if (some_src_ty.isSignlessIntOrIndexOrFloat()) {
      TPU_CHECK_OP(some_src_ty.getIntOrFloatBitWidth() == kNativeBitwidth,
                   "Only 32-bit broadcasts supported");
      if (res_ty.getRank() == 1) {
        NYI("scalar-1d broadcast");
      } else {  // rank >= 2  // NOLINT(readability-else-after-return)
        setLayout(op, kNoLayout,
                  VectorLayout(kNativeBitwidth, {std::nullopt, std::nullopt},
                               default_tiling_, ImplicitDim::kNone));
      }
      return success();
    }
    if (auto src_ty = dyn_cast<VectorType>(some_src_ty)) {
      TPU_CHECK_OP(src_ty.getRank() >= 2, "source rank below 2D unsupported");
      TPU_CHECK_OP(res_ty.getRank() >= 2, "result rank below 2D unsupported");
      auto some_layout = getLayout(op.getSource());
      TPU_CHECK_OP(some_layout.has_value(), "missing vector layout");
      // We want to force the layout to be (8, 128) instead of (1, 128) if we
      // are broadcasting sublane dim from 1 to at least 8.
      if (some_layout->bitwidth() == kNativeBitwidth &&
          some_layout->implicit_dim() == ImplicitDim::kNone &&
          some_layout->tiling()[0] == 1 &&
          some_layout->tiling()[1] == default_tiling_[1] &&
          src_ty.getDimSize(src_ty.getRank() - 2) == 1 &&
          res_ty.getDimSize(res_ty.getRank() - 2) >= 8) {
        *some_layout = VectorLayout(
            some_layout->bitwidth(), {std::nullopt, some_layout->offsets()[1]},
            default_tiling_, some_layout->implicit_dim());
      }
      auto &layout = *some_layout;
      if (layout.implicit_dim() == ImplicitDim::kSecondMinor &&
          src_ty.getDimSize(src_ty.getRank() - 2) == 1) {
        // Treat the layout as a 2D layout if possible.
        layout = VectorLayout(layout.bitwidth(), layout.offsets(),
                              layout.tiling(), ImplicitDim::kNone);
      }
      TPU_CHECK_OP(layout.implicit_dim() == ImplicitDim::kNone,
                   "expected 2D layout");
      auto src_tiled_shape = src_ty.getShape().take_back(2);
      auto dst_tiled_shape = res_ty.getShape().take_back(2);
      LayoutOffsets offsets = layout.offsets();
      if (layout.bitwidth() == kNativeBitwidth &&
          layout.tiling() == default_tiling_) {
        for (int i = 0; i < 2; ++i) {
          if (src_tiled_shape[i] != dst_tiled_shape[i]) {
            offsets[i] = std::nullopt;
          }
        }
      }
      setLayout(op, some_layout,
                VectorLayout(layout.bitwidth(), offsets, layout.tiling(),
                             ImplicitDim::kNone));
      return success();
    }
    op.emitOpError("unsupported broadcast source type");
    return failure();
  }

  LogicalResult infer(vector::ContractionOp op) {
    // TODO(apaszke): Support layout here, at least on batch dimensions.
    TPU_CHECK_OP(op.getKind() == vector::CombiningKind::ADD,
                 "Only ADD supported");
    auto ctx = op.getContext();
    const auto matmul_iterator_types = mlir::ArrayAttr::get(ctx, {
      vector::IteratorTypeAttr::get(ctx, vector::IteratorType::parallel),
      vector::IteratorTypeAttr::get(ctx, vector::IteratorType::parallel),
      vector::IteratorTypeAttr::get(ctx, vector::IteratorType::reduction)
    });
    TPU_CHECK_OP(op.getIteratorTypes() == matmul_iterator_types,
                 "Not a matmul");
    const auto matmul_indexing_maps = mlir::ArrayAttr::get(
        ctx,
        {AffineMapAttr::get(AffineMap::get(
             3, 0, {getAffineDimExpr(0, ctx), getAffineDimExpr(2, ctx)}, ctx)),
         AffineMapAttr::get(AffineMap::get(
             3, 0, {getAffineDimExpr(2, ctx), getAffineDimExpr(1, ctx)}, ctx)),
         AffineMapAttr::get(AffineMap::get(
             3, 0, {getAffineDimExpr(0, ctx), getAffineDimExpr(1, ctx)},
             ctx))});
    const auto matmul_indexing_maps_transposed = mlir::ArrayAttr::get(
        ctx,
        {AffineMapAttr::get(AffineMap::get(
             3, 0, {getAffineDimExpr(0, ctx), getAffineDimExpr(2, ctx)}, ctx)),
         AffineMapAttr::get(AffineMap::get(
             3, 0, {getAffineDimExpr(1, ctx), getAffineDimExpr(2, ctx)}, ctx)),
         AffineMapAttr::get(AffineMap::get(
             3, 0, {getAffineDimExpr(0, ctx), getAffineDimExpr(1, ctx)},
             ctx))});
    TPU_CHECK_OP(op.getIndexingMaps() == matmul_indexing_maps ||
                     op.getIndexingMaps() == matmul_indexing_maps_transposed,
                 "Not a matmul");
    return inferMatmul(op);
  }

  LogicalResult infer(vector::LoadOp op) {
    auto src_ty = op.getMemRefType();
    auto res_ty = op.getVectorType();
    TPU_CHECK_OP(src_ty.getRank() == res_ty.getRank(),
                 "memref and vector rank mismatch");
    int64_t rank = res_ty.getRank();
    int8_t bitwidth = res_ty.getElementTypeBitWidth();
    auto maybe_tiling =
        verifyMemoryTiling(op, getMemRefLayout(op.getBase()).getTiles(),
                           src_ty.getRank(), src_ty.getElementTypeBitWidth());
    if (!maybe_tiling) {
      return failure();
    }
    auto tiling = *maybe_tiling;

    SmallVector<Layout, 4> in_layout(op->getNumOperands(), kNoLayout);
    CHECK_EQ(op->getNumOperands(), op.getIndices().size() + 1);
    SmallVector<int64_t, 4> indices;
    indices.reserve(rank);
    for (Value v : op.getIndices()) {
      auto cst_op = v.getDefiningOp<arith::ConstantOp>();
      TPU_CHECK_OP(cst_op, "only constant indices are supported");
      indices.push_back(cast<IntegerAttr>(cst_op.getValue()).getInt());
    }
    for (int64_t i = 0; i < rank; ++i) {
      TPU_CHECK_OP(indices[i] + res_ty.getDimSize(i) <= src_ty.getDimSize(i),
                   "Loading elements out of bounds");
    }

    if (rank == 0) {
      op.emitOpError("rank 0 vectors unsupported");
      return failure();
    }
    if (rank == 1) {
      TPU_CHECK_OP(tiling.size() == 1, "Expected 1D tiling in 1D loads");
      auto tile = tiling.front();
      TPU_CHECK_OP(tile % target_shape_[1] == 0,
                   "Unsupported tiling for 1D load");
      int64_t idx = indices.front();
      int64_t offset = idx % kVmemAlignment32;
      // TODO(apaszke): We could generate replicated loads for short values.
      setLayout(op, in_layout,
                VectorLayout(bitwidth, {0, offset}, {1, tile},
                             ImplicitDim::kSecondMinor));
    } else {  // rank >= 2
      TPU_CHECK_OP(tiling.size() == 2, "Expected 2D tiling in 2D+ loads");
      std::array<std::optional<int64_t>, 2> offsets;
      const auto tile_indices = ArrayRef<int64_t>(indices).take_back(2);
      const auto tile_src_shape = src_ty.getShape().take_back(2);
      const auto tile_res_shape = res_ty.getShape().take_back(2);
      const int64_t num_sublanes = tile_res_shape[0];
      // For now, we focus on tilings that span full sublanes.
      TPU_CHECK_OP(tiling[1] == target_shape_[1],
                   "Unsupported tiling for 2d load");
      // We can load starting from any row if the source has few columns,
      // because the tiling structure degenerates to regular layout there.
      // There is also no extra need for alignment if we load a single sublane.
      // TODO(apaszke): Also no need to align if we don't exceed the base chunk!
      if (bitwidth == 32 &&
          (tile_src_shape[1] <= target_shape_[1] || num_sublanes == 1)) {
        offsets[0] = 0;
      } else {
        offsets[0] = tile_indices[0] % tiling[0];
      }
      offsets[1] = tile_indices[1] % target_shape_[1];
      std::array<int64_t, 2> layout_tiling{tiling[0], tiling[1]};
      if (num_sublanes == 1 && bitwidth == 32 &&
          tiling[1] == target_shape_[1] &&
          tile_res_shape[1] > target_shape_[1]) {
        // We can strided load sublanes if we're loading a single sublane for
        // multiple times. Enabling this helps load one entire row from memref
        // more efficiently.
        setLayout(op, in_layout,
                  VectorLayout(bitwidth, offsets, {1, layout_tiling[1]},
                               ImplicitDim::kNone));
      } else if (num_sublanes == 1 && bitwidth == 32 &&
                 tiling == target_shape_) {
        // We can use replicated loads if we're only loading a single sublane.
        setLayout(op, in_layout,
                  VectorLayout(bitwidth, {std::nullopt, offsets[1]},
                               layout_tiling, ImplicitDim::kNone));
      } else {
        setLayout(
            op, in_layout,
            VectorLayout(bitwidth, offsets, layout_tiling, ImplicitDim::kNone));
      }
    }
    return success();
  }

  LogicalResult infer(vector::ExtractStridedSliceOp op) {
    auto input_layout = getLayout(op.getVector());
    TPU_CHECK_OP(input_layout, "missing vector layout");
    TPU_CHECK_OP(input_layout->implicit_dim() == ImplicitDim::kNone,
                 "only 2D layouts supported");
    TPU_CHECK_OP(op.getType().getElementTypeBitWidth() == 32,
                 "Only 32-bit types supported");
    auto offsets = op.getOffsets().getValue();
    auto sizes = op.getSizes().getValue();
    auto strides = op.getStrides().getValue();
    for (auto offset_attr : offsets.take_back(2)) {
      int off = offset_attr.cast<IntegerAttr>().getInt();
      TPU_CHECK_OP(off == 0, "Only zero-offset slices supported.");
    }
    sizes = sizes.take_back(2);
    TPU_CHECK_OP(
        (sizes[0].cast<IntegerAttr>().getInt() % target_shape_[0] == 0) &&
            (sizes[1].cast<IntegerAttr>().getInt() % target_shape_[1] == 0),
        "Only lane and sublane aligned slices allowed.");
    for (auto stride : strides) {
      TPU_CHECK_OP(stride.cast<IntegerAttr>().getInt() == 1,
                   "Only trivial strides supported.");
    }

    setLayout(op, input_layout, input_layout);
    return success();
  }

  LogicalResult infer(vector::MultiDimReductionOp op) {
    auto src_ty = op.getSourceVectorType();
    auto dst_ty = dyn_cast<VectorType>(op.getDestType());
    TPU_CHECK_OP(dst_ty, "only reductions with vector results supported");
    TPU_CHECK_OP(src_ty.getRank() == dst_ty.getRank() + 1,
                 "only 1D reductions supported");
    int64_t dim = cast<IntegerAttr>(op.getReductionDims()[0]).getInt();
    int64_t src_rank = src_ty.getRank();
    auto acc_pad = getLayout(op.getAcc());
    TPU_CHECK_OP(is_fully_replicated(acc_pad),
                 "only constant accumulators supported");
    TPU_CHECK_OP(src_ty.getElementTypeBitWidth() == kNativeBitwidth,
                 "only 32-bit reductions supported");
    auto some_src_layout = getLayout(op.getSource());
    TPU_CHECK_OP(some_src_layout, "missing vector layout");
    auto &src_layout = *some_src_layout;
    TPU_CHECK_OP(src_layout.implicit_dim() == ImplicitDim::kNone,
                 "only 2D layouts supported");
    if (dim == src_rank - 1) {
      setLayout(
          op, {src_layout, acc_pad},
          VectorLayout(kNativeBitwidth, {src_layout.offsets()[0], std::nullopt},
                       default_tiling_, ImplicitDim::kMinor));
    } else if (dim == src_rank - 2) {
      setLayout(
          op, {src_layout, acc_pad},
          VectorLayout(kNativeBitwidth, {std::nullopt, src_layout.offsets()[1]},
                       default_tiling_, ImplicitDim::kSecondMinor));
    } else {
      // Reduction happens over the unrolled dimension --- we can keep layout.
      setLayout(op, {src_layout, acc_pad}, src_layout);
    }
    return success();
  }

  LogicalResult infer(vector::ShapeCastOp op) {
    auto src_ty = op.getSourceVectorType();
    auto src_shape = src_ty.getShape();
    int64_t src_rank = src_ty.getRank();
    auto res_ty = op.getResultVectorType();
    auto res_shape = res_ty.getShape();
    int64_t res_rank = res_ty.getRank();
    auto some_src_layout = getLayout(op.getSource());
    TPU_CHECK_OP(some_src_layout, "missing vector layout");
    auto layout = *some_src_layout;
    if (layout.implicit_dim() == ImplicitDim::kNone) {
      // Nothing changes in the last two dims.
      if (res_rank >= 2 && src_shape.take_back(2) == res_shape.take_back(2)) {
        setLayout(op, layout, layout);
        return success();
      }
      unsigned bitwidth = src_ty.getElementTypeBitWidth();
      auto native_tiling = nativeTiling(bitwidth);
      if (layout.tiling() != native_tiling) {
        layout = VectorLayout(bitwidth, layout.offsets(), native_tiling,
                              layout.implicit_dim());
      }
      TPU_CHECK_OP(src_ty.getRank() >= 2,
                   "expected 2D+ operand with 2D layout");
      ArrayRef<int64_t> layout_shape = src_ty.getShape().take_back(2);
      if (res_ty.getRank() >= 2) {
        // Squeeze out the sublane dim.
        if (layout_shape[0] == 1 &&
            res_shape.drop_back(1) == src_shape.drop_back(2) &&
            res_shape.back() == src_shape.back()) {
          setLayout(op, layout,
                    VectorLayout(bitwidth, layout.offsets(), layout.tiling(),
                                 ImplicitDim::kSecondMinor));
          return success();
        }
        // Sublane (un)tiling
        if (src_ty.getElementTypeBitWidth() == kNativeBitwidth &&
            src_ty.getDimSize(src_ty.getRank() - 1) ==
                res_shape[res_shape.size() - 1] &&
            layout.offsets() == LayoutOffsets{0, 0} &&
            src_ty.getDimSize(src_ty.getRank() - 2) % target_shape_[0] == 0 &&
            res_shape[res_shape.size() - 2] % target_shape_[0] == 0) {
          setLayout(op, layout, layout);
          return success();
        }
        // Insert a singleton lane dimension. The old lane dimension ends up
        // in the sublane dimension. Other axes can be reshaped arbitrarily.
        if (src_ty.getElementTypeBitWidth() == kNativeBitwidth &&
            src_shape.back() == res_shape[res_shape.size() - 2] &&
            res_shape.back() == 1) {
          setLayout(op, layout,
                    VectorLayout(kNativeBitwidth, {0, std::nullopt},
                                 default_tiling_, ImplicitDim::kNone));
          return success();
        }
      } else if (res_ty.getRank() == 1) {
        bool all_one = true;
        for (int64_t s : src_ty.getShape().drop_back(2)) {
          all_one &= s == 1;
        }
        // Squeeze out everything, but lanes
        if (layout_shape[0] == 1 && all_one &&
            res_ty.getShape().back() == layout_shape[1]) {
          setLayout(op, layout,
                    VectorLayout(bitwidth, layout.offsets(), layout.tiling(),
                                 ImplicitDim::kSecondMinor));
          return success();
        }
        // Squeeze out everything, but sublanes
        if (layout_shape[1] == 1 && all_one &&
            res_ty.getShape().back() == layout_shape[0]) {
          TPU_CHECK_OP(src_ty.getElementTypeBitWidth() == kNativeBitwidth,
                       "only 32-bit shape casts supported");
          setLayout(op, layout,
                    VectorLayout(kNativeBitwidth, layout.offsets(),
                                 layout.tiling(), ImplicitDim::kMinor));
          return success();
        }
      }
    } else {
      // Nothing changes in the last dim.
      if (res_ty.getRank() >= 1 &&
          src_shape.back() == res_shape.back()) {
        setLayout(op, layout, layout);
        return success();
      }
      TPU_CHECK_OP(src_ty.getElementTypeBitWidth() == kNativeBitwidth,
                   "only 32-bit shape casts supported");
      // Insert a singleton innermost dim.
      if (res_ty.getRank() == src_ty.getRank() + 1 &&
          src_ty.getDimSize(src_rank - 1) == res_ty.getDimSize(res_rank - 2) &&
          res_ty.getDimSize(res_rank - 1) == 1) {
        if (layout.implicit_dim() == ImplicitDim::kMinor) {
          setLayout(op, layout,
                    VectorLayout(kNativeBitwidth, layout.offsets(),
                                 default_tiling_, ImplicitDim::kNone));
        } else {
          TPU_CHECK_OP(layout.implicit_dim() == ImplicitDim::kSecondMinor,
                       "unexpected implicit dim value");
          setLayout(op, layout,
                    VectorLayout(kNativeBitwidth, {0, std::nullopt},
                                 default_tiling_, ImplicitDim::kNone));
        }
        return success();
      }
    }
    op.emitOpError("unsupported shape cast");
    return failure();
  }

  LogicalResult infer(vector::StoreOp op) {
    auto ref_ty = op.getMemRefType();
    auto store_ty = op.getValueToStore().getType();
    TPU_CHECK_OP(ref_ty.getRank() == store_ty.getRank(),
                 "memref and vector rank mismatch");
    int64_t rank = ref_ty.getRank();
    int8_t bitwidth = store_ty.getElementTypeBitWidth();
    auto maybe_tiling =
        verifyMemoryTiling(op, getMemRefLayout(op.getBase()).getTiles(),
                           ref_ty.getRank(), ref_ty.getElementTypeBitWidth());
    if (!maybe_tiling) {
      return failure();
    }
    auto tiling = *maybe_tiling;

    SmallVector<int64_t, 4> indices;
    indices.reserve(rank);
    for (Value v : op.getIndices()) {
      auto cst_op = v.getDefiningOp<arith::ConstantOp>();
      TPU_CHECK_OP(cst_op, "only constant indices are supported");
      indices.push_back(cast<IntegerAttr>(cst_op.getValue()).getInt());
    }
    for (int64_t i = 0; i < rank; ++i) {
      TPU_CHECK_OP(indices[i] + store_ty.getDimSize(i) <= ref_ty.getDimSize(i),
                   "storing elements out of bounds");
    }

    Layout store_layout;
    if (rank == 0) {
      op.emitOpError("rank 0 vectors unsupported");
      return failure();
    }
    if (rank == 1) {
      TPU_CHECK_OP(tiling.size() == 1, "Expected 1D tiling in 1D store");
      auto tile = tiling.front();
      TPU_CHECK_OP(tile % target_shape_[1] == 0,
                   "Unsupported 1D tiling for 1D store");
      int64_t idx = indices.front();
      int64_t offset = idx % kVmemAlignment32;
      store_layout = VectorLayout(bitwidth, {0, offset}, {1, tile},
                                  ImplicitDim::kSecondMinor);
    } else {  // rank >= 2  // NOLINT(readability-else-after-return)
      TPU_CHECK_OP(tiling.size() == 2, "Expected 2D tiling in 2D+ store");
      std::array<std::optional<int64_t>, 2> offsets;
      const auto tile_indices = ArrayRef<int64_t>(indices).take_back(2);
      const auto tile_ref_shape = ref_ty.getShape().take_back(2);
      const auto tile_store_shape = store_ty.getShape().take_back(2);
      const int64_t num_sublanes = tile_store_shape[0];
      // For now, we focus on tilings that span full sublanes.
      TPU_CHECK_OP(tiling[1] == target_shape_[1],
                   "Unsupported tiling for 2d store");
      // We can store starting from any row if the source has few columns,
      // because the tiling structure degenerates to regular layout there.
      // There is also no extra need for alignment if we store a single sublane.
      // TODO(apaszke): Also no need to align if we don't exceed the base chunk!
      if (bitwidth == 32 &&
          (tile_ref_shape[1] <= target_shape_[1] || num_sublanes == 1)) {
        offsets[0] = 0;
      } else {
        offsets[0] = tile_indices[0] % tiling[0];
      }
      offsets[1] = tile_indices[1] % target_shape_[1];
      if (num_sublanes == 1 && bitwidth == 32 &&
          tiling[1] == target_shape_[1] &&
          tile_store_shape[1] > target_shape_[1]) {
        // We can strided store sublanes if we're storing a single sublane for
        // multiple times. Enabling this helps store one entire row to memref
        // more efficiently.
        store_layout = VectorLayout(store_ty.getElementTypeBitWidth(), offsets,
                                    {1, tiling[1]}, ImplicitDim::kNone);
      } else {
        store_layout = VectorLayout(store_ty.getElementTypeBitWidth(), offsets,
                                    {tiling[0], tiling[1]}, ImplicitDim::kNone);
      }
    }
    SmallVector<Layout, 5> in_layout{store_layout};
    in_layout.insert(in_layout.end(), op.getIndices().size() + 1, kNoLayout);
    setInLayout(op, in_layout);
    return success();
  }

  LogicalResult infer(vector::TransposeOp op) {
    auto permutation_attrs = op.getTransp().getValue();
    auto some_layout = getLayout(op.getVector());
    TPU_CHECK_OP(some_layout.has_value(), "missing vector layout");
    auto &layout = *some_layout;
    auto src_ty = op.getSourceVectorType();
    TPU_CHECK_OP(permutation_attrs.size() == src_ty.getRank(),
                 "Transpose permutation has incorrect rank");
    if (layout.implicit_dim() == ImplicitDim::kNone) {
      TPU_CHECK_OP((layout.offsets() == LayoutOffsets{0, 0}),
                   "Padded transposes unsupported");
      auto xlu_width = target_shape_[1];
      for (int64_t s : src_ty.getShape().take_back(2)) {
        TPU_CHECK_OP(s % xlu_width == 0, "Padded transposes unsupported");
      }
      for (auto attr : permutation_attrs.drop_back(2)) {
        TPU_CHECK_OP(
            cast<IntegerAttr>(attr).getInt() < src_ty.getRank() - 2,
            "Unsupported transpose permutation - minor dims into major");
      }
      for (auto attr : permutation_attrs.take_back(2)) {
        TPU_CHECK_OP(
            cast<IntegerAttr>(attr).getInt() >= src_ty.getRank() - 2,
            "Unsupported transpose permutation - major dims into minor");
      }
      Layout required_layout = some_layout;
      if (permutation_attrs.size() < 2) {
        return failure();
      }
      // Require native tiling if we're going to use the XLU.
      if (cast<IntegerAttr>(permutation_attrs[permutation_attrs.size() - 1])
              .getInt() == permutation_attrs.size() - 2) {
        auto native_tiling = nativeTiling(layout.bitwidth());
        required_layout = VectorLayout(layout.bitwidth(), layout.offsets(),
                                       native_tiling, ImplicitDim::kNone);
      }
      setLayout(op, required_layout, required_layout);
      return success();
    }
    op.emitOpError("Unsupported transpose");
    return failure();
  }

  LogicalResult inferElementwise(Operation *op, bool check_bitwidth = true) {
    TPU_CHECK_OP(op->getNumResults() == 1, "only one result supported");
    TPU_CHECK_OP(op->getNumOperands() > 0,
                 "elementwise ops with no operands unsupported");
    // Elementwise operators can be parameterized by both scalars and shaped
    // types, so make sure we infer layout based on a shaped-typed operand.
    std::optional<VectorLayout> out_layout_candidate;
    std::optional<VectorLayout> out_layout;
    SmallVector<std::optional<Layout>, 4> in_layouts;
    int64_t bit_width = -1;
    for (int64_t i = 0; i < op->getNumOperands(); ++i) {
      if (auto vty = dyn_cast<VectorType>(op->getOperand(i).getType())) {
        if (bit_width == -1) {
          bit_width = vty.getElementTypeBitWidth();
        }
        TPU_CHECK_OP(
            !check_bitwidth || bit_width == vty.getElementTypeBitWidth(),
            "Generic elementwise rule only supports operands of same width");
        auto some_layout = getLayout(op->getOperand(i));
        TPU_CHECK_OP(some_layout.has_value(), "missing vector layout");
        auto &layout = *some_layout;
        // If the input is fully replicated, don't use it to commit to any
        // layout. Replicated values are easy to relayout.
        if (is_fully_replicated(some_layout)) {
          in_layouts.push_back(std::nullopt);
          out_layout_candidate = layout;
          continue;
        }
        if (!out_layout) {
          // TODO(apaszke): There are probably smarter ways to choose layout.
          out_layout = layout;
          in_layouts.push_back(some_layout);
        } else {
          if (auto new_out =
                  VectorLayout::join(layout, *out_layout, vty.getShape())) {
            out_layout = *new_out;
            in_layouts.push_back(some_layout);
          } else {
            in_layouts.push_back(std::nullopt);
          }
        }
      } else {
        TPU_CHECK_OP(op->getOperand(i).getType().isSignlessIntOrIndexOrFloat(),
                     "expected only vector and scalar operands");
        in_layouts.push_back({kNoLayout});
      }
    }
    Layout final_out_layout = std::nullopt;
    if (auto out_vty = dyn_cast<VectorType>(op->getResult(0).getType())) {
      TPU_CHECK_OP(
          !check_bitwidth || bit_width == out_vty.getElementTypeBitWidth(),
          "Generic elementwise rule can't change element type width");
      if (out_layout) {
        final_out_layout = *out_layout;
      } else if (out_layout_candidate) {
        final_out_layout = *out_layout_candidate;
      } else {
        op->emitOpError(
            "Elementwise op has no vector operands but returns a vector?");
        return failure();
      }
    }
    CHECK_EQ(in_layouts.size(), op->getNumOperands()) << Print(op);
    SmallVector<Layout, 4> final_in_layouts;
    for (int i = 0; i < in_layouts.size(); ++i) {
      if (in_layouts[i]) {
        final_in_layouts.push_back(*in_layouts[i]);
      } else {
        final_in_layouts.push_back(final_out_layout);
      }
    }
    setLayout(op, final_in_layouts, final_out_layout);
    return success();
  }

  LogicalResult inferMatmul(Operation* op) {
    auto get_unpadded_layout = [&](Value v) -> std::optional<VectorLayout> {
      auto pad = getLayout(v);
      if (!pad.has_value() || pad->implicit_dim() != ImplicitDim::kNone) {
        return std::nullopt;
      }
      auto vty = cast<VectorType>(v.getType());
      auto tiling = nativeTiling(vty.getElementTypeBitWidth());
      auto shape = vty.getShape().take_back(2);
      if (pad->offsets()[0].value_or(0) != 0 ||
          pad->offsets()[1].value_or(0) != 0 || shape[0] % tiling[0] != 0 ||
          shape[1] % tiling[1] != 0) {
        return std::nullopt;
      }
      // Override tiling to match the native one.
      return VectorLayout(pad->bitwidth(), pad->offsets(), tiling,
                          ImplicitDim::kNone);
    };
    auto res_ty = dyn_cast<VectorType>(op->getResult(0).getType());
    TPU_CHECK_OP(res_ty, "only vector results supported");
    TPU_CHECK_OP(res_ty.getElementTypeBitWidth() == kNativeBitwidth,
                 "only 32-bit matmul results supported");
    std::array<Layout, 3> in_layout;
    CHECK_EQ(op->getNumOperands(), 3);
    for (int i = 0; i < 3; ++i) {
      if (auto layout = get_unpadded_layout(op->getOperand(i))) {
        in_layout[i] = *layout;
      } else {
        op->emitOpError("padded operands");
        return failure();
      }
    }
    setLayout(op, in_layout,
              VectorLayout(kNativeBitwidth, {0, 0}, default_tiling_,
                           ImplicitDim::kNone));
    return success();
  }

  void setInLayout(Operation *op, ArrayRef<Layout> in) {
    CHECK_EQ(in.size(), op->getNumOperands()) << Print(op);
    SmallVector<Attribute, 4> in_attrs;
    in_attrs.reserve(in.size());
    for (const Layout &p : in) {
      in_attrs.push_back(VectorLayoutAttr::get(op->getContext(), p));
    }
    op->setAttr("in_layout", ArrayAttr::get(op->getContext(), in_attrs));
  }

  void setOutLayout(Operation *op, Layout out) {
    setOutLayout(op, ArrayRef<Layout>(out));
  }

  void setOutLayout(Operation *op, ArrayRef<Layout> out) {
    SmallVector<Attribute, 4> out_attrs;
    out_attrs.reserve(out.size());
    for (const Layout &p : out) {
      out_attrs.push_back(VectorLayoutAttr::get(op->getContext(), p));
    }
    op->setAttr("out_layout", ArrayAttr::get(op->getContext(), out_attrs));
  }

  void setLayout(Operation *op, Layout in, Layout out) {
    setLayout(op, ArrayRef<Layout>(in), ArrayRef<Layout>(out));
  }

  void setLayout(Operation *op, ArrayRef<Layout> in, Layout out) {
    setLayout(op, in, ArrayRef<Layout>(out));
  }

  void setLayout(Operation *op, Layout in, ArrayRef<Layout> out) {
    setLayout(op, ArrayRef<Layout>(in), out);
  }

  void setLayout(Operation *op, ArrayRef<Layout> in, ArrayRef<Layout> out) {
    setInLayout(op, in);
    setOutLayout(op, out);
  }

  SmallVector<Layout, 4> getInLayout(Operation *op) {
    CHECK(op);
    CHECK(op->getAttr("in_layout"));
    auto in_attrs = op->getAttrOfType<ArrayAttr>("in_layout").getValue();
    CHECK_EQ(in_attrs.size(), op->getNumOperands());
    SmallVector<Layout, 4> in_layouts;
    in_layouts.reserve(op->getNumOperands());
    for (int i = 0; i < op->getNumOperands(); ++i) {
      in_layouts.push_back(cast<VectorLayoutAttr>(in_attrs[i]).getLayout());
    }
    return in_layouts;
  }

  SmallVector<Layout, 4> getOutLayout(Operation *op) {
    CHECK(op);
    CHECK(op->getAttr("out_layout"));
    auto out_attrs = op->getAttrOfType<ArrayAttr>("out_layout").getValue();
    CHECK_EQ(out_attrs.size(), op->getNumResults());
    SmallVector<Layout, 4> out_layouts;
    out_layouts.reserve(op->getNumResults());
    for (int i = 0; i < op->getNumResults(); ++i) {
      out_layouts.push_back(cast<VectorLayoutAttr>(out_attrs[i]).getLayout());
    }
    return out_layouts;
  }

  Layout getLayout(Value v) {
    auto op = v.getDefiningOp();
    CHECK(op);
    auto op_result = dyn_cast<OpResult>(v);
    CHECK(op_result);
    auto result_index = op_result.getResultNumber();
    auto out_attrs = op->getAttrOfType<ArrayAttr>("out_layout").getValue();
    CHECK(out_attrs.size() > result_index);
    return cast<VectorLayoutAttr>(out_attrs[result_index]).getLayout();
  }

 private:
  std::optional<absl::Span<const int64_t>> verifyMemoryTiling(
      Operation *op, ArrayRef<xla::Tile> mem_tiling, int64_t rank,
      int8_t bitwidth) {
    if (bitwidth == 32) {
      if (mem_tiling.size() != 1) {
        op->emitOpError("Only one-level tiling supported for 32-bit loads");
        return std::nullopt;
      }
    } else if (bitwidth < 32) {
      int64_t rows_per_tile;
      if (rank == 1) {
        if (mem_tiling.size() != 3) {
          op->emitOpError(
              "Only three-level tiling supported for 1D memory ops narrower "
              "than 32-bit");
          return std::nullopt;
        }
        auto first = mem_tiling[0].dimensions();
        auto second = mem_tiling[1].dimensions();
        if (first.size() != 1 || first[0] % target_shape_[1] != 0) {
          op->emitOpError("Invalid first-level tile in 1D memory op");
          return std::nullopt;
        }
        rows_per_tile = first[0] / target_shape_[1];
        if (second.size() != 1 || second[0] != target_shape_[1]) {
          op->emitOpError("Invalid second-level tile in 1D memory op");
          return std::nullopt;
        }
      } else {
        if (mem_tiling.size() != 2) {
          op->emitOpError(
              "Only two-level tiling supported for 2D+ memory ops narrower "
              "than 32-bit");
          return std::nullopt;
        }
        auto first = mem_tiling[0].dimensions();
        rows_per_tile = first[0];
      }
      auto row_compressed = mem_tiling[mem_tiling.size() - 1].dimensions();
      if (row_compressed.size() != 2) {
        op->emitOpError("Expected 2D tiling for packed layout");
        return std::nullopt;
      }
      if (row_compressed[0] != (32 / bitwidth) || row_compressed[1] != 1) {
        op->emitOpError("Expected compressed packed layout");
        return std::nullopt;
      }
      if (row_compressed[0] > rows_per_tile) {
        op->emitOpError("Packing cannot introduce padding");
        return std::nullopt;
      }
    } else {
      op->emitOpError("Loads of types wider than 32-bit unsupported");
      return std::nullopt;
    }
    return mem_tiling[0].dimensions();
  }

  std::array<int64_t, 2> nativeTiling(int8_t bitwidth) {
    return {default_tiling_[0] * kNativeBitwidth / bitwidth,
            default_tiling_[1]};
  }

  std::array<int64_t, 2> target_shape_;
  std::array<int64_t, 2> default_tiling_;

  // Address alignment requirement, counted in 32-bit increments.
  static constexpr int64_t kVmemAlignment32 = 128;
  // TODO(apaszke): This is not really native on newer generations of TPUs.
  // Get rid of this temporary stopgap.
  static constexpr int8_t kNativeBitwidth = 32;
};

struct InferVectorLayoutPass
    : public impl::InferVectorLayoutPassBase<InferVectorLayoutPass> {
  InferVectorLayoutPass(int lane_count, int sublane_count) {
    this->sublane_count = sublane_count;
    this->lane_count = lane_count;
  }
  void runOnOperation() override {
    func::FuncOp func = getOperation();
    VectorLayoutInferer run({sublane_count, lane_count});
    if (run.infer(func).failed()) {
      signalPassFailure();
    }
  }
};

}  // namespace

std::unique_ptr<OperationPass<func::FuncOp>> createInferVectorLayoutPass(
    int lane_count, int sublane_count) {
  return std::make_unique<InferVectorLayoutPass>(lane_count, sublane_count);
}

}  // namespace mlir::tpu
