# Copyright 2021 The JAX Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

from __future__ import annotations

import builtins
from collections.abc import Callable, Iterator, Mapping, Sequence, Set
import enum
import inspect
import types
from typing import Any, ClassVar, TypeVar, overload

import numpy as np

from . import config as config
from . import ffi as ffi
from . import guard_lib as guard_lib
from . import ifrt_programs as ifrt_programs
from . import ifrt_proxy as ifrt_proxy
from . import jax_jit as jax_jit
from . import mlir as mlir
from . import pmap_lib as pmap_lib
from . import profiler as profiler
from . import pytree as pytree
from . import transfer_guard_lib as transfer_guard_lib

custom_call_targets = Any
hlo_sharding_util = Any

_LiteralSlice = Any
_Status = Any
_Dtype = Any

ifrt_version_number: int

_T = TypeVar("_T")

class XlaRuntimeError(RuntimeError):
  pass

class PrimitiveType(enum.IntEnum):
  PRIMITIVE_TYPE_INVALID = ...
  PRED = ...
  S2 = ...
  S4 = ...
  S8 = ...
  S16 = ...
  S32 = ...
  S64 = ...
  U2 = ...
  U4 = ...
  U8 = ...
  U16 = ...
  U32 = ...
  U64 = ...
  F4E2M1FN = ...
  F8E3M4 = ...
  F8E4M3 = ...
  F8E4M3FN = ...
  F8E4M3B11FNUZ = ...
  F8E4M3FNUZ = ...
  F8E5M2 = ...
  F8E5M2FNUZ = ...
  F8E8M0FNU = ...
  BF16 = ...
  F16 = ...
  F32 = ...
  F64 = ...
  C64 = ...
  C128 = ...
  TUPLE = ...
  OPAQUE_TYPE = ...
  TOKEN = ...

# === BEGIN xla_compiler.cc

class ArrayCopySemantics(enum.IntEnum):
  ALWAYS_COPY = ...
  REUSE_INPUT = ...
  DONATE_INPUT = ...

class Layout:
  @overload
  def __init__(self, minor_to_major: tuple[int, ...]): ...
  @overload
  def __init__(
      self,
      minor_to_major: tuple[int, ...],
      tiling: tuple[tuple[int, ...], ...],
      element_size_in_bits: int,
  ): ...
  def minor_to_major(self) -> tuple[int, ...]: ...
  def tiling(self) -> Sequence[tuple[int, ...]]: ...
  def element_size_in_bits(self) -> int: ...
  def to_string(self) -> str: ...
  def __eq__(self, other: Any) -> bool: ...
  def __ne__(self, other: Any) -> bool: ...
  def __hash__(self) -> int: ...

class Shape:
  def __init__(self, s: str): ...
  @staticmethod
  def tuple_shape(shapes: Sequence[Shape]) -> Shape: ...
  @staticmethod
  def array_shape(
      type: np.dtype | PrimitiveType,
      dims_seq: Any = ...,
      layout_seq: Any = ...,
      dynamic_dimensions: list[bool] | None = ...,
  ) -> Shape: ...
  @staticmethod
  def token_shape() -> Shape: ...
  @staticmethod
  def scalar_shape(type: np.dtype | PrimitiveType) -> Shape: ...
  def dimensions(self) -> tuple[int, ...]: ...
  def layout(self) -> Layout: ...
  def xla_element_type(self) -> PrimitiveType: ...
  def element_type(self) -> np.dtype: ...
  def numpy_dtype(self) -> np.dtype: ...
  def is_tuple(self) -> bool: ...
  def is_array(self) -> bool: ...
  def is_token(self) -> bool: ...
  def is_static(self) -> bool: ...
  def is_dynamic(self) -> bool: ...
  def is_dynamic_dimension(self, dimension: int) -> bool: ...
  def set_dynamic_dimension(self, dimension: int, is_dynamic: bool) -> None: ...
  def rank(self) -> int: ...
  def to_serialized_proto(self) -> bytes: ...
  def tuple_shapes(self) -> list[Shape]: ...
  def leaf_count(self) -> int: ...
  def with_major_to_minor_layout_if_absent(self) -> Shape: ...
  def __eq__(self, other: Any) -> bool: ...
  def __ne__(self, other: Any) -> bool: ...
  def __hash__(self) -> int: ...
  def __repr__(self) -> str: ...

class ProgramShape:
  def __init__(self, params: Sequence[Shape], result: Shape) -> None: ...
  def parameter_shapes(self) -> list[Shape]: ...
  def result_shape(self) -> Shape: ...
  def __repr__(self) -> str: ...

class Literal:
  def __init__(self, shape: Shape) -> None: ...
  def __repr__(self) -> str: ...
  def __array__(
      self, dtype: np.dtype | None = None, copy: bool | None = None
  ) -> np.ndarray: ...
  def shape(self) -> Shape: ...

class XlaComputation:
  def __init__(self, serialized_hlo_module_proto: bytes) -> None: ...
  def get_hlo_module(self) -> HloModule: ...
  def program_shape(self) -> ProgramShape: ...
  def as_serialized_hlo_module_proto(self) -> bytes: ...
  def as_hlo_text(self, print_large_constants: bool = False) -> str: ...
  def as_hlo_dot_graph(self) -> str: ...
  def hash(self) -> int: ...
  def as_hlo_module(self) -> HloModule: ...

class HloPrintOptions:
  def __init__(self) -> None: ...
  @staticmethod
  def short_parsable() -> HloPrintOptions: ...
  @staticmethod
  def canonical() -> HloPrintOptions: ...
  @staticmethod
  def fingerprint() -> HloPrintOptions: ...
  print_large_constants: bool
  print_metadata: bool
  print_backend_config: bool
  print_result_shape: bool
  print_operand_shape: bool
  print_operand_names: bool
  print_ids: bool
  print_extra_attributes: bool
  print_program_shape: bool
  print_percent: bool
  print_control_dependencies: bool
  compact_operands: bool
  include_layout_in_shapes: bool
  canonicalize_instruction_names: bool
  canonicalize_computations: bool
  indent_amount: int
  is_in_nested_computation: bool

class HloComputation:
  def render_html(self) -> None: ...

class HloModule:
  spmd_output_sharding: OpSharding | None
  spmd_parameters_shardings: list[OpSharding] | None
  @property
  def name(self) -> str: ...
  def to_string(self, options: HloPrintOptions = ...) -> str: ...
  def as_serialized_hlo_module_proto(self) -> bytes: ...
  @staticmethod
  def from_serialized_hlo_module_proto(
      serialized_hlo_module_proto: bytes,
  ) -> HloModule: ...
  def computations(self) -> list[HloComputation]: ...

class HloModuleGroup:
  def __init__(self, name: str, modules: list[HloModule]) -> None: ...
  @property
  def name(self) -> str: ...
  def to_string(self) -> str: ...
  def to_modules(self) -> list[HloModule]: ...

def hlo_module_to_dot_graph(hlo_module: HloModule) -> str: ...
def hlo_module_from_text(hlo_module_text: str) -> HloModule: ...
def hlo_module_cost_analysis(
    client: Client, module: HloModule
) -> dict[str, float]: ...

class DeviceAssignment:
  @staticmethod
  def create(array: np.ndarray) -> DeviceAssignment: ...
  def replica_count(self) -> int: ...
  def computation_count(self) -> int: ...
  def __repr__(self) -> str: ...
  def serialize(self) -> bytes: ...

class CompileOptions:
  @staticmethod
  def ParseFromString(s: bytes) -> CompileOptions: ...
  def __init__(self) -> None: ...
  def SerializeAsString(self) -> bytes: ...
  argument_layouts: list[Shape] | None
  parameter_is_tupled_arguments: bool
  executable_build_options: ExecutableBuildOptions
  tuple_arguments: bool
  num_replicas: int
  num_partitions: int
  profile_version: int
  device_assignment: DeviceAssignment | None
  compile_portable_executable: bool
  env_option_overrides: list[tuple[str, str]]

def register_custom_call_target(
    fn_name: str,
    capsule: Any,
    platform: str,
    api_version: int = ...,
) -> _Status: ...
def register_custom_call_partitioner(
    name: str,
    prop_user_sharding: Callable,
    partition: Callable,
    infer_sharding_from_operands: Callable,
    can_side_effecting_have_replicated_sharding: bool = ...,
    c_api: Any | None = ...,
) -> None: ...
def encode_inspect_sharding_callback(handler: Any) -> bytes: ...
def register_custom_call_as_batch_partitionable(
    target_name: str,
    c_api: Any | None = ...,
) -> None: ...
def register_custom_type_id(type_name: str, type_id: Any) -> None: ...

class AutotuneCacheMode(enum.IntEnum):
  UNSPECIFIED = ...
  UPDATE = ...
  READ = ...

class DebugOptions:
  def __repr__(self) -> str: ...
  xla_cpu_enable_fast_math: bool
  xla_cpu_fast_math_honor_infs: bool
  xla_cpu_fast_math_honor_nans: bool
  xla_cpu_fast_math_honor_division: bool
  xla_cpu_fast_math_honor_functions: bool
  xla_gpu_enable_fast_min_max: bool
  xla_backend_optimization_level: int
  xla_cpu_enable_xprof_traceme: bool
  xla_llvm_disable_expensive_passes: bool
  xla_test_all_input_layouts: bool
  xla_disable_hlo_passes: str
  xla_enable_hlo_passes_only: str
  xla_force_host_platform_device_count: int
  xla_dump_to: str
  xla_dump_hlo_module_re: str
  xla_dump_hlo_pass_re: str
  xla_dump_hlo_as_text: bool
  xla_dump_hlo_as_proto: bool
  xla_dump_hlo_as_dot: bool
  xla_dump_hlo_as_url: bool
  xla_dump_hlo_as_html: bool
  xla_dump_fusion_visualization: bool
  xla_dump_hlo_snapshots: bool
  xla_dump_max_hlo_modules: bool
  xla_dump_module_metadata: bool
  xla_dump_compress_protos: bool
  xla_dump_hlo_as_long_text: bool
  xla_dump_disable_metadata: bool
  xla_dump_hlo_pipeline_re: str
  xla_gpu_cuda_data_dir: str
  xla_detailed_logging: bool
  xla_enable_dumping: bool
  xla_gpu_dump_autotune_results_to: str
  xla_gpu_load_autotune_results_from: str
  xla_gpu_dump_autotune_logs_to: str
  xla_gpu_kernel_cache_file: str
  xla_gpu_enable_llvm_module_compilation_parallelism: bool
  xla_gpu_per_fusion_autotune_cache_dir: str
  xla_gpu_experimental_autotune_cache_mode: AutotuneCacheMode

class CompiledMemoryStats:
  generated_code_size_in_bytes: int
  argument_size_in_bytes: int
  output_size_in_bytes: int
  alias_size_in_bytes: int
  temp_size_in_bytes: int
  host_generated_code_size_in_bytes: int
  host_argument_size_in_bytes: int
  host_output_size_in_bytes: int
  host_alias_size_in_bytes: int
  host_temp_size_in_bytes: int
  serialized_buffer_assignment_proto: bytes
  def __str__(self) -> str: ...

class ExecutableBuildOptions:
  def __init__(self) -> None: ...
  def __repr__(self) -> str: ...
  result_layout: Shape | None
  fdo_profile: bytes | None
  num_replicas: int
  num_partitions: int
  debug_options: DebugOptions
  device_assignment: DeviceAssignment | None
  use_spmd_partitioning: bool
  use_auto_spmd_partitioning: bool
  auto_spmd_partitioning_mesh_shape: list[int]
  auto_spmd_partitioning_mesh_ids: list[int]
  use_shardy_partitioner: bool
  def compilation_environments_from_serialized_proto(
      self, serialized_proto: bytes
  ) -> None: ...

class OpSharding_Type(enum.IntEnum):
  REPLICATED = ...
  MAXIMAL = ...
  TUPLE = ...
  OTHER = ...
  MANUAL = ...
  UNKNOWN = ...

class OpSharding_ShardGroupType(enum.IntEnum):
  AS = ...
  LIKE = ...

class OpSharding:
  Type: type[OpSharding_Type]
  type: OpSharding_Type
  replicate_on_last_tile_dim: bool
  last_tile_dims: Sequence[OpSharding_Type]
  tile_assignment_dimensions: Sequence[int]
  tile_assignment_devices: Sequence[int]
  iota_reshape_dims: Sequence[int]
  iota_transpose_perm: Sequence[int]
  tuple_shardings: Sequence[OpSharding]
  is_shard_group: bool
  shard_group_id: int
  ShardGroupType: builtins.type[OpSharding_ShardGroupType]
  shard_group_type: OpSharding_ShardGroupType
  def ParseFromString(self, s: bytes) -> None: ...
  def SerializeToString(self) -> bytes: ...
  def clone(self) -> OpSharding: ...

class HloSharding:
  @staticmethod
  def from_proto(proto: OpSharding) -> HloSharding: ...
  @staticmethod
  def from_string(sharding: str) -> HloSharding: ...
  @staticmethod
  def tuple_sharding(
      shape: Shape, shardings: Sequence[HloSharding]
  ) -> HloSharding: ...
  @staticmethod
  def iota_tile(
      dims: Sequence[int],
      reshape_dims: Sequence[int],
      transpose_perm: Sequence[int],
      subgroup_types: Sequence[OpSharding_Type],
  ) -> HloSharding: ...
  @staticmethod
  def replicate() -> HloSharding: ...
  @staticmethod
  def manual() -> HloSharding: ...
  @staticmethod
  def unknown() -> HloSharding: ...
  @staticmethod
  def subgroup_with_device_ordering(
      tile_assignment: np.ndarray, subgroup_types: Sequence[OpSharding_Type]
  ) -> HloSharding: ...
  def __eq__(self, other: Any) -> bool: ...
  def __hash__(self) -> int: ...
  def __repr__(self) -> str: ...
  def tile(self, shape: Shape) -> Shape: ...
  def is_replicated(self) -> bool: ...
  def is_manual(self) -> bool: ...
  def is_unknown(self) -> bool: ...
  def is_tiled(self) -> bool: ...
  def is_maximal(self) -> bool: ...
  def tuple_elements(self) -> list[HloSharding]: ...
  def num_devices(self) -> int: ...
  def num_dimensions(self) -> int: ...
  def is_tile_assignment_iota(self) -> bool: ...
  def tile_assignment_dimensions(self) -> Sequence[int]: ...
  def tile_assignment_devices(self) -> Sequence[int]: ...
  def subgroup_types(self) -> Sequence[OpSharding_Type]: ...
  def replicate_on_last_tile_dim(self) -> bool: ...
  def to_proto(self) -> OpSharding: ...
  def get_axis_sizes(self) -> list[int]: ...

# === END xla_compiler.cc

class Device:
  id: int
  host_id: int
  process_index: int
  platform: str
  device_kind: str
  client: Client
  local_hardware_id: int | None
  def __repr__(self) -> str: ...
  def __str__(self) -> str: ...
  def transfer_to_infeed(self, literal: _LiteralSlice): ...
  def transfer_from_outfeed(self, shape: Shape): ...
  def memory(self, kind: str) -> Memory: ...
  def default_memory(self) -> Memory: ...
  def addressable_memories(self) -> list[Memory]: ...
  def live_buffers(self) -> list[Any]: ...
  def memory_stats(self) -> dict[str, int] | None: ...
  def get_stream_for_external_ready_events(self) -> int: ...
  def __getattr__(self, name: str) -> Any: ...

class Memory:
  process_index: int
  platform: str
  kind: str
  def __repr__(self) -> str: ...
  def __str__(self) -> str: ...
  def addressable_by_devices(self) -> list[Device]: ...

class PjRtLayout:
  def __str__(self) -> str: ...
  def __eq__(self, other: Any) -> bool: ...
  def __hash__(self) -> int: ...
  def __getstate__(self) -> Any: ...
  def __setstate__(self, _: Any): ...
  def _xla_layout(self) -> Layout: ...

class GpuAllocatorConfig:
  class Kind(enum.IntEnum):
    DEFAULT = ...
    PLATFORM = ...
    BFC = ...
    CUDA_ASYNC = ...

  def __init__(
      self,
      kind: Kind = ...,
      memory_fraction: float = ...,
      preallocate: bool = ...,
      collective_memory_size: int = ...,
  ) -> None: ...

class HostBufferSemantics(enum.IntEnum):
  IMMUTABLE_ONLY_DURING_CALL = ...
  IMMUTABLE_UNTIL_TRANSFER_COMPLETES = ...
  ZERO_COPY = ...

class Client:
  platform: str
  _raw_platform: str
  platform_version: str
  runtime_type: str
  def device_count(self) -> int: ...
  def local_device_count(self) -> int: ...
  def devices(self) -> list[Device]: ...
  def local_devices(self) -> list[Device]: ...
  def _get_all_devices(self) -> list[Device]: ...
  def device_from_local_hardware_id(self, int) -> Device: ...
  def live_buffers(self) -> list[Any]: ...
  def live_arrays(self) -> list[ArrayImpl]: ...
  def live_executables(self) -> list[LoadedExecutable]: ...
  def host_id(self) -> int: ...
  def process_index(self) -> int: ...
  def buffer_from_pyval(
      self,
      argument: Any,
      device: Device | None = ...,
      force_copy: bool = ...,
      host_buffer_semantics: HostBufferSemantics = ...,
  ) -> ArrayImpl: ...
  def compile(
      self,
      computation: str | bytes,
      executable_devices: DeviceList | Sequence[Device],
      compile_options: CompileOptions = ...,
  ) -> Executable: ...
  def compile_and_load(
      self,
      computation: str | bytes,
      executable_devices: DeviceList | Sequence[Device],
      compile_options: CompileOptions = ...,
      host_callbacks: Sequence[Any] = ...,
  ) -> LoadedExecutable: ...
  def compile_ifrt_program(
      self,
      program: ifrt_programs.Program,
      program_options: ifrt_programs.CompileOptions,
  ) -> LoadedExecutable: ...
  def compile_and_load_ifrt_program(
      self,
      program: ifrt_programs.Program,
      program_options: ifrt_programs.CompileOptions,
  ) -> LoadedExecutable: ...
  def serialize_executable(self, executable: LoadedExecutable) -> bytes: ...
  def deserialize_executable(
      self,
      serialized: bytes,
      executable_devices: DeviceList | Sequence[Device],
      options: CompileOptions | None,
      host_callbacks: Sequence[Any] = ...,
  ) -> LoadedExecutable: ...
  def heap_profile(self) -> bytes: ...
  def make_python_callback_from_host_send_and_recv(
      self,
      callable: Callable,
      operand_shapes: Sequence[Shape],
      result_shapes: Sequence[Shape],
      send_channel_ids: Sequence[int],
      recv_channel_ids: Sequence[int],
      serializer: Callable | None = ...,
  ) -> Any: ...
  def get_default_layout(
      self, dtype: np.dtype, shard_shape: Sequence[int], device: Device
  ) -> PjRtLayout: ...
  def __getattr__(self, name: str) -> Any: ...

class CompileOnlyPyClient(Client):
  def compile(
      self,
      computation: str | bytes,
      executable_devices: DeviceList | Sequence[Device],
      compile_options: CompileOptions = ...,
  ) -> Executable: ...

class CpuCollectives: ...

def make_gloo_tcp_collectives(
    distributed_client: DistributedRuntimeClient | None = ...,
    hostname: str | None = ...,
    interface: str | None = ...,
) -> CpuCollectives: ...

class MpiCollectives(CpuCollectives):
  def Init(self): ...
  def Finalize(self): ...

def make_mpi_collectives() -> MpiCollectives: ...
def get_tfrt_cpu_client(
    asynchronous: bool = ...,
    distributed_client: DistributedRuntimeClient | None = ...,
    node_id: int = ...,
    num_nodes: int = ...,
    collectives: CpuCollectives | None = ...,
    num_devices: int | None = ...,
    get_local_topology_timeout_minutes: int | None = ...,
    get_global_topology_timeout_minutes: int | None = ...,
) -> Client: ...
def get_mock_gpu_client(
    asynchronous: bool = ...,
    allocator_config: GpuAllocatorConfig = ...,
    distributed_client: DistributedRuntimeClient | None = ...,
    node_id: int = ...,
    allowed_devices: Any | None = ...,
    platform_name: str | None = ...,
) -> Client: ...
def get_c_api_client(
    platform_name: str,
    options: Mapping[str, str | int | list[int] | float | bool],
    distributed_client: DistributedRuntimeClient | None = ...,
) -> Client: ...
def get_default_c_api_topology(
    platform_name: str,
    topology_name: str,
    options: dict[str, str | int | list[int] | float],
) -> DeviceTopology: ...
def get_c_api_topology(
    c_api: Any,
    topology_name: str,
    options: dict[str, str | int | list[int] | float],
) -> DeviceTopology: ...
def get_topology_for_devices(devices: list[Device]) -> DeviceTopology: ...
def load_pjrt_plugin(
    platform_name: str, library_path: str | None, c_api: Any | None
) -> _Status: ...
def pjrt_plugin_loaded(plugin_name: str) -> bool: ...
def pjrt_plugin_initialized(plugin_name: str) -> bool: ...
def initialize_pjrt_plugin(platform_name: str) -> _Status: ...

Array = Any
ArrayImpl = Any

# TODO(phawkins): this type is problematic because it is not a subtype of
# jax.Array, and pytype notices.
# class ArrayImpl:
#   def __init__(self,
#                aval: Any,
#                sharding: Any,
#                arrays: Sequence[ArrayImpl],
#                committed: bool,
#                _skip_checks: bool = ...): ...
#   def block_until_ready(self) -> ArrayImpl: ...
#   def is_deleted(self) -> bool: ...
#   def is_ready(self) -> bool: ...
#   def delete(self): ...
#   def unsafe_buffer_pointer(self) -> Any: ...
#   def clone(self) -> ArrayImpl: ...
#   def _copy_single_device_array_to_host_async(self): ...
#   def _single_device_array_to_np_array_did_copy(self) -> tuple[np.ndarray, bool]: ...
#   def on_device_size_in_bytes(self) -> int: ...
#   def _fully_replicated_shard(self) -> ArrayImpl: ...
#   __cuda_array_interface__: Dict[str, Any]
#   dtype: np.dtype
#   shape: Tuple[int, ...]
#   _arrays: Any
#   _npy_value: Any
#   traceback: Traceback
#   _HAS_DYNAMIC_ATTRIBUTES: bool = ...

def batched_copy_array_to_devices_with_sharding(
    arrays: Sequence[ArrayImpl],
    devices: Sequence[list[Device]],
    sharding: Sequence[Any],
    array_copy_semantics: Sequence[ArrayCopySemantics],
) -> Sequence[ArrayImpl]: ...
def batched_block_until_ready(x: Sequence[ArrayImpl]) -> None: ...
def batched_device_put(
    aval: Any,
    sharding: Any,
    shards: Sequence[Any],
    devices: list[Device],
    committed: bool = ...,
    force_copy: bool = ...,
    host_buffer_semantics: Any = ...,
) -> ArrayImpl: ...
def reorder_shards(
    x: ArrayImpl,
    dst_sharding: Any,
    array_copy_semantics: ArrayCopySemantics,
) -> ArrayImpl: ...
def check_and_canonicalize_memory_kind(
    memory_kind: str | None, device_list: DeviceList
) -> str | None: ...
def array_result_handler(
    aval: Any, sharding: Any, committed: bool, _skip_checks: bool = ...
) -> Callable: ...

class Token:
  def block_until_ready(self): ...

class ShardedToken:
  def block_until_ready(self): ...
  def get_token(self, device_id: int): ...

class ExecuteResults:
  def __len__(self) -> int: ...
  def disassemble_into_single_device_arrays(self) -> list[list[ArrayImpl]]: ...
  def disassemble_prefix_into_single_device_arrays(
      self, n: int
  ) -> list[list[ArrayImpl]]: ...
  def consume_with_handlers(self, handlers: list[Callable]) -> list[Any]: ...
  def consume_token(self) -> ShardedToken: ...

def get_execution_stream_id() -> int: ...

def set_execution_stream_id(new_id: int): ...

class LoadedExecutable:
  client: Client
  def local_devices(self) -> list[Device]: ...
  def size_of_generated_code_in_bytes(self) -> int: ...
  def execute(self, arguments: Sequence[ArrayImpl]) -> list[ArrayImpl]: ...
  def execute_with_token(
      self, arguments: Sequence[ArrayImpl]
  ) -> tuple[list[ArrayImpl], Token]: ...
  def execute_sharded(
      self, arguments: Sequence[list[ArrayImpl]], with_tokens: bool = ...
  ) -> ExecuteResults: ...
  def hlo_modules(self) -> list[HloModule]: ...
  def get_output_memory_kinds(self) -> list[list[str]]: ...
  def get_compiled_memory_stats(self) -> CompiledMemoryStats: ...
  def get_output_shardings(self) -> list[OpSharding] | None: ...
  def get_parameter_shardings(self) -> list[OpSharding] | None: ...
  def get_parameter_layouts(self) -> list[Layout]: ...
  def get_output_layouts(self) -> list[Layout]: ...
  def keep_alive(self) -> None: ...
  def cost_analysis(self) -> dict[str, Any]: ...
  traceback: Traceback
  fingerprint: bytes | None

class Executable:
  def hlo_modules(self) -> list[HloModule]: ...
  def get_output_memory_kinds(self) -> list[list[str]]: ...
  def get_output_shardings(self) -> list[OpSharding] | None: ...
  def get_parameter_shardings(self) -> list[OpSharding] | None: ...
  def get_parameter_layouts(self) -> list[Layout]: ...
  def get_output_layouts(self) -> list[Layout]: ...
  def get_compiled_memory_stats(self) -> CompiledMemoryStats: ...
  def serialize(self) -> str: ...
  def cost_analysis(self) -> dict[str, Any]: ...

class DeviceTopology:
  platform: str
  platform_version: str
  def _make_compile_only_devices(self) -> list[Device]: ...
  def serialize(self) -> bytes: ...
  def __getattr__(self, name: str) -> Any: ...

def buffer_to_dlpack_managed_tensor(
    buffer: ArrayImpl, stream: int | None = None
) -> Any: ...
@overload
def dlpack_managed_tensor_to_buffer(
    tensor: Any, device: Device, stream: int | None
) -> ArrayImpl: ...
@overload
def dlpack_managed_tensor_to_buffer(  # Legacy overload
    tensor: Any,
    cpu_backend: Client | None = ...,
    gpu_backend: Client | None = ...,
) -> ArrayImpl: ...
def cuda_array_interface_to_buffer(
    cai: dict[
        str,
        (
            str
            | int
            | None
            | tuple[int, ...]
            | tuple[int, bool]
            | list[tuple[str, str]]
            | list[tuple[str, str, tuple[int, ...]]]
        ),
    ],
    gpu_backend: Client | None = ...,
    device_id: int | None = None,
) -> ArrayImpl: ...

# === BEGIN py_traceback.cc

class Frame:
  file_name: str
  function_name: str
  function_line_start: int
  line_num: int
  def __init__(
      self,
      file_name: str,
      function_name: str,
      function_line_start: int,
      line_num: int,
  ): ...
  def __repr__(self) -> str: ...

class Traceback:
  enabled: ClassVar[bool]
  @staticmethod
  def get_traceback() -> Traceback: ...
  @staticmethod
  def traceback_from_frames(frames: Sequence[Frame]) -> Any: ...
  frames: Sequence[Frame]
  def __str__(self) -> str: ...
  def as_python_traceback(self) -> Any: ...
  def raw_frames(self) -> tuple[list[types.CodeType], list[int]]: ...
  @staticmethod
  def code_addr2line(code: types.CodeType, lasti: int) -> int: ...
  @staticmethod
  def code_addr2location(
      code: types.CodeType, lasti: int
  ) -> tuple[int, int, int, int]: ...

def tracebacks_enabled() -> bool: ...
def set_tracebacks_enabled(enabled: bool) -> None: ...

# === END py_traceback.cc

class DistributedRuntimeService:
  def shutdown(self) -> None: ...

class DistributedRuntimeClient:
  def connect(self) -> _Status: ...
  def shutdown(self) -> _Status: ...
  def blocking_key_value_get(self, key: str, timeout_in_ms: int) -> _Status: ...
  def blocking_key_value_get_bytes(
      self, key: str, timeout_in_ms: int
  ) -> _Status: ...
  def key_value_try_get(self, key: str) -> _Status: ...
  def key_value_try_get_bytes(self, key: str) -> _Status: ...
  def key_value_dir_get(self, key: str) -> _Status: ...
  def key_value_dir_get_bytes(self, key: str) -> _Status: ...
  def key_value_set(
      self, key: str, value: str, allow_overwrite: bool = False
  ) -> _Status: ...
  def key_value_set_bytes(
      self, key: str, value: bytes, allow_overwrite: bool = False
  ) -> _Status: ...
  def key_value_delete(self, key: str) -> _Status: ...
  def wait_at_barrier(
      self,
      barrier_id: str,
      timeout_in_ms: int,
      process_ids: list[int] | None = None,
  ) -> _Status: ...
  def get_live_nodes(self, process_ids: list[int]) -> _Status: ...

def get_distributed_runtime_service(
    address: str,
    num_nodes: int,
    heartbeat_interval: int | None = ...,
    max_missing_heartbeats: int | None = ...,
    cluster_register_timeout: int | None = ...,
    shutdown_timeout: int | None = ...,
) -> DistributedRuntimeService: ...
def get_distributed_runtime_client(
    address: str,
    node_id: int,
    rpc_timeout: int | None = ...,
    init_timeout: int | None = ...,
    shutdown_timeout: int | None = ...,
    heartbeat_interval: int | None = ...,
    max_missing_heartbeats: int | None = ...,
    missed_heartbeat_callback: Any | None = ...,
    shutdown_on_destruction: bool | None = ...,
    use_compression: bool | None = ...,
) -> DistributedRuntimeClient: ...

class PreemptionSyncManager:
  def initialize(self, client: DistributedRuntimeClient) -> _Status: ...
  def reached_sync_point(self, step_counter: int) -> bool: ...
  def shutdown(self) -> None: ...

def create_preemption_sync_manager() -> PreemptionSyncManager: ...
def collect_garbage() -> None: ...
def is_optimized_build() -> bool: ...
def json_to_pprof_profile(json: str) -> bytes: ...
def pprof_profile_to_json(proto: bytes) -> str: ...

class PmapFunction:
  def __call__(self, *args, **kwargs) -> Any: ...
  def __getstate__(self) -> Any: ...
  def __setstate__(self, Any): ...
  __signature__: inspect.Signature
  def _cache_size(self) -> int: ...
  def _cache_clear(self) -> None: ...

class DeviceList:
  def __init__(self, device_assignment: tuple[Device, ...]): ...
  def __hash__(self) -> int: ...
  def __eq__(self, other: Any) -> bool: ...
  def __ne__(self, other: Any) -> bool: ...
  def __len__(self) -> int: ...
  def __getitem__(self, index: Any) -> Any: ...
  def __iter__(self) -> Iterator[Device]: ...
  def __str__(self) -> str: ...
  def __repr__(self) -> str: ...
  def __getstate__(self) -> Any: ...
  def __setstate__(self, state: Any): ...
  @property
  def is_fully_addressable(self) -> bool: ...
  @property
  def addressable_device_list(self) -> DeviceList: ...
  @property
  def process_indices(self) -> set[int]: ...
  @property
  def default_memory_kind(self) -> str | None: ...
  @property
  def memory_kinds(self) -> tuple[str, ...]: ...
  @property
  def device_kind(self) -> str: ...

class Sharding: ...

class NamedSharding(Sharding):
  def __init__(
      self,
      mesh: Any,
      spec: Any,
      *,
      memory_kind: str | None = None,
      _logical_device_ids: tuple[int, ...] | None = None,
  ): ...
  mesh: Any
  spec: Any
  _memory_kind: str | None
  _internal_device_list: DeviceList
  _logical_device_ids: tuple[int, ...] | None

class SingleDeviceSharding(Sharding):
  def __init__(self, device: Device, *, memory_kind: str | None = None): ...
  _device: Device
  _memory_kind: str | None
  _internal_device_list: DeviceList

class PmapSharding(Sharding):
  def __init__(
      self, devices: Sequence[Any], sharding_spec: pmap_lib.ShardingSpec
  ): ...
  devices: list[Any]
  sharding_spec: pmap_lib.ShardingSpec
  _internal_device_list: DeviceList

class GSPMDSharding(Sharding):
  def __init__(
      self,
      devices: Sequence[Device],
      op_sharding: OpSharding | HloSharding,
      *,
      memory_kind: str | None = None,
      _device_list: DeviceList | None = None,
  ): ...
  _devices: tuple[Device, ...]
  _hlo_sharding: HloSharding
  _memory_kind: str | None
  _internal_device_list: DeviceList

class PjitFunction:
  def __call__(self, *args, **kwargs) -> Any: ...

class PjitFunctionCache:
  def __init__(self, capacity: int = ...): ...
  def __getstate__(self) -> Any: ...
  def __setstate__(self, Any): ...
  def size(self) -> int: ...
  def capacity(self) -> int: ...
  def clear(self): ...
  @staticmethod
  def clear_all(): ...

def pjit(
    function_name: str,
    fun: Callable | None,
    cache_miss: Callable,
    static_argnums: Sequence[int],
    static_argnames: Sequence[str],
    global_cache_key: Any,
    pytree_registry: pytree.PyTreeRegistry,
    shard_arg_fallback: Callable,
    cache: PjitFunctionCache | None = ...,
) -> PjitFunction: ...

class WeakrefLRUCacheInfo:
  @property
  def hits(self) -> int: ...
  @property
  def misses(self) -> int: ...
  @property
  def maxsize(self) -> int: ...
  @property
  def currsize(self) -> int: ...

class WeakrefLRUCache:
  def __call__(self, weakref_key: Any, *args, **kwargs) -> Any: ...
  def cache_keys(self) -> list[Any]: ...
  def cache_info(self) -> WeakrefLRUCacheInfo: ...
  def cache_clear(self): ...

def is_asan() -> bool: ...
def is_msan() -> bool: ...
def is_tsan() -> bool: ...
def is_sanitized() -> bool: ...

class TransferConnection:
  def address(self) -> str: ...
  def _pull_flat(self, uuid, backend, avals_flat) -> list[Any]: ...

class TransferServer:
  def _await_pull_flat(self, uuid, args: list[ArrayImpl]): ...
  def connect(self, address: str) -> TransferConnection: ...

def start_transfer_server(
    client: Client,
    address: str = "",
    transport_addresses: list[str] = [],
    max_num_parallel_copies: int = 0,
    transfer_size: int = 0,
) -> TransferServer: ...
def approx_top_k_reduction_output_size(
    input_size: int,
    rank: int,
    top_k: int,
    recall_target: float,
    aggregate_to_topk: bool | None = ...,
    input_size_override: int | None = ...,
) -> tuple[int, int]: ...
def get_internal_device_put_info() -> dict[str, int]: ...

class UnconstrainedSingleton:
  def __repr__(self) -> str: ...
  def __reduce__(self) -> Any: ...

UNCONSTRAINED_PARTITION: UnconstrainedSingleton

class PartitionSpec:
  def __init__(self, *partitions, unreduced: Set[Any] | None = None): ...
  def __hash__(self): ...
  def __eq__(self, other): ...
  _HAS_DYNAMIC_ATTRIBUTES: bool = ...

def canonicalize_partition(partition: Any) -> Any: ...
